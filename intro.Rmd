---
output: html_document
editor_options: 
  chunk_output_type: console
---

# Introduction

## Brief history of interactive data visualization

### Early interactive data visualization: By statisticians for statisticians

Static data visualization has a rich and intricate history, and a full treatment is beyond the scope of the present thesis [but see e.g. @dix1998; @friendly2006; @friendly2021; @young2011]. Briefly, prior to the mid-20th century, data visualization was often considered as at best secondary to "serious" statistical analysis [although there were also some counter-examples, see e.g. @friendly2006; @young2011]. However, beginning in the late 1950's, a series of developments took place which lead to a rise in the prominence and accessibility of data visualization. Firstly, at the theoretical level, the work of Tukey [-@tukey1962; -@tukey1977] and Bertin [-@bertin1967] established data visualization as valuable discipline in its own right. Secondly, at the practical level, the development of personal computers [see e.g. @abbate1999] and high-level programming languages such as FORTRAN in 1954 [@backus1978], made the process of rendering production-grade figures near-effortless in comparison to the earlier hand-drawn techniques. Combined, these developments lead to a surge in the use and dissemination of data visualization.  

With static data visualization on the rise in the 1950's, interactive data visualization would not be left far behind. The very early interactive data visualization systems tended to be designed for niche, specialized tasks. For example, @fowlkes1969 designed a system which allowed the users to view probability plots under different configurations of parameters and transformations, whereas @kruskal1964 created a tool for visualizing multidimensional scaling. 

However, soon, researchers began exploring interactive data visualization as a general-purpose tool for data exploration. The first such general-purpose system was PRIM-9 [@fisherkeller1974]. PRIM-9 allowed for exploration of multivariate data via interactive features such as projection, rotation, masking, and filtering. Subsequent systems strove to provide an even wider range of features. For example, MacSpin [@donoho1988] and XGobi [@swayne1998] implemented features such as interactive scaling, rotation, linked selection (or "brushing"), and interactive plotting of smooth fits in scatterplots, as well as interactive parallel coordinate plots and grand tours (excellent video-documentaries of some of these early interactive data visualization systems are available at [ASA Statistical Graphics Video Library](https://community.amstat.org/jointscsg-section/media/videos)).

Later, with the proliferation of open-source, general-purpose statistical computing software such as S and R, . The successor system to XGobi, GGobi [@swayne2003], expanded on XGobi and made it directly embeddable in the R runtime. Mondrian [@theus2002] allowed for sophisticated linked interaction between many different types of plots including scatteplots, histograms, barplots, scatterplot, mosaic plots, parallel coordinates plots, and maps. Finally, iPlots [@urbanek2003] implemented a general framework for interactive plotting that was not only embedded in R but could be directly programmatically manipulated, and was later further expanded and made performant for big data in iPlots eXtreme [@urbanek2011].      

What all of these interactive data visualization systems had in common is that they were designed by statisticians and with interesting, often ambitious interactive features in mind. High-level analytic features such as linked selection/brushing, rotation and projection, and interactive manipulation of model parameters made frequent appearances. While being a clear strength, the more complex nature of the systems may have also slowed their adoption, as they often demanded an expert user to take advantage of most fully.

### Interactive data visualization and the Web: World-wide interactivity

The tail end of the millennium would mark the arrival of a whole new class of technologies that had a significant impact on interactive data visualization, just as it had on almost every other field of human endeavor. The arrival of the internet in the mid 1990's, and the development of JavaScript in 1995 as a high-level programming language for the Web [for a thorough description of the history, see e.g. @wirfs-brock2020], saw the rise of interactive applications that could be accessed by anyone, from anywhere. This was aided by the dissemination of robust and standardized Web browsers. Soon, interactive data visualization became just one of many interactive technologies highly sought after within the fledgling Web ecosystem. 

Early interactive data visualization systems for the Web, such as Prefuse [@heer2005] and Flare [developed around 2008, @flare2020] tended to relied on external plugins (Java and Adobe Flash Player, respectively). However, as browsers got faster at interpreting JavaScript, thanks to advances in compiler technologies, specifically Just In Time (JIT) compilation, it became possible to write entire data visualization libraries in JavaScript. In the late 2000's and early 2010's, several true Web-native interactive data visualization systems emerged. 

Currently, the most prominent framework for data visualization on the Web, interactive or otherwise, is D3.js [@bostock2022]. D3 is a fairly general and low-level JavaScript framework for visualizing data, and consists of a suite of specialized modules designed for various aspects of the data visualization workflow, including parsing data, transformation, defining scales, interfacing with the DOM and handling interaction events, and even physics simulation and animation. Importantly, while D3 does provide methods for handling interactive events, it does not provide a system for dispatching and coordinating these events - it instead delegates this responsibility to the user and encourages the use of reactive Web frameworks such as React [@react2024], Vue [@vue2024], or Svelte [@svelte2024]. Finally, D3.js visualizations are rendered as Scalable Vector Graphics (SVG) by default, meaning that they can be scaled without loss of quality. However, a price to pay for this robustness is in performance, as visualizations can become slow to render at high data volumes. The modular nature of D3 means that the visualizations can be rendered by other, more performant devices, such as the HTML 5 Canvas element or WebGL, however, as of this date, the implementation of such an alternative rendering framework is left to the user and there are no official modules.  

Building upon the fairly low level framework provided by D3, many packages have been developed to provide a more high-level and opinionated interfaces. Prominent two among these are plotly.js [@plotly2022] and Highcharts [@highcharts2024]. While D3 provides low-level utilities such as data transformations, scales, and geometric objects, these packages provide more high-level utilities such as functions for rendering complete plots and registering reactive events, which are under the hood automatically handled via systems based on the native DOM Event Target interface [@mdn2024a]. Like D3, both plotly.js and Highcharts also render the graphics in SVG by default, however, unlike D3, they both also provide alternative rendering engines based on WebGL [@highschartsboost2022; @plotly2024b]. 

A somewhat different approach is taken by another popular visualization package built partially on D3 - Vega [@satyanarayan2015; @vega2024a]. Vega provides a declarative framework for defining (interactive) data visualizations using a static [JSON] schema. Compared to plotly.js or Highcharts, Vega is significantly more expressive, allowing for fine-grained customization of graphics and interactive behavior, standing essentially just one level above D3. However, as a consequence, it is also significantly more verbose. For example, a full specification of a scatterplot matrix with linked brushing takes over 300 lines of code [JSON, not including the data and using default formatting such as would be created by calling `JSON.stringify(schema, null, "\t")`, @vega2024b].          

While these contemporary Web-based interactive data visualization systems offer great deal of flexibility and customizability, I argue that this comes at the cost of making them practical for applied researchers and data scientists. Most importantly, it seems that there is some ambiguity about what counts as interactive features. For example, in the [R Graph Gallery entry on Interactive Charts](https://r-graph-gallery.com/interactive-charts.html) [@holtz2022], which features several examples of interactive visualization derived from the above-mentioned JavaScript interactive data visualization libraries, the visualizations feature interactions such zooming, panning, hovering, 3D rotation, and repositioning a node within a network graph. However, in all of these examples, the user only manipulates surface-level graphical attributes of a single plot. In contrast, the [Plotly Dash documentation page on Interactive Visualizations](https://dash.plotly.com/interactive-graphing) [@plotly2022] does feature two examples of linked hovering and cross-filtering, i.e. examples of linked interactivity. However, it should be noted that vast majority of visualizations in the [Plotly R Open Source Graphing Library documentation page](https://plotly.com/r/) [@plotly2022] allow for only surface-level interactions. Similarly, [VegaLite Gallery pages on Interactive Charts](https://vega.github.io/vega-lite/examples/#interactive-charts) and [Interactive Multiview Displays](https://vega.github.io/vega-lite/examples/#interactive-multi-view-displays)  [@vegalite2022] feature many examples, however, only a few show limited examples of linked or parametric interactivity.  Finally, the [Highcharter Showcase Page ](https://jkunst.com/highcharter/articles/showcase.html) [@kunst2022] does not feature any examples of linking or parametric interactivity.

What all of the packages listed above have in common is that most featured interaction is typically surface-level and takes place within a single plot, and the few examples that feature interesting types of interactivity (linked or parametric) often require a complicated setup. The main reason for this is most likely that all of these packages have been designed to be very general-purpose and flexible, and the price to pay for this flexibility is that complex types of interactivity require complex code. Another reason is that these packages have been built for static visualizations first, and interactivity second. Further, since all of these packages are native to JavaScript, the expectation may be that if more interesting types of interactivity are desired, the interactive "back-end" may be written separately, outside of the package. Finally, the typical use case for these packages seems to be presentation, not EDA.  

Be it as it may, there is a fairly high barrier for entry for creating interesting types of interactivity (i.e. linked or parametric) with these packages. This may not be an issue for large organizations which can afford to hire computer science specialists to work on complex interactive dashboards and visualizations full-time. However, to the average applied scientist or data scientist, the upfront cost of producing a useful interactive data visualization may be too high, especially if one is only interested in exploratory data analysis for one's own benefit. This may be the reason why interactive visualizations are nowadays mainly used for data communication, not data exploration [@batch2017]. On a higher level, the current options for interactive data visualization may reflect a broader cultural differences between Computer Science and Statistics, where Computer Science may be more oriented towards business and large-team collaboration, whereas Statistics may be more focused on applied research and individual/small-team workflow.    

## What even is interactive data visualization? {#what-is-interactive-visualization}

> If it looks like a duck, swims like a duck, and quacks like a duck, then it probably is a duck. 
>
> [...] The irony is that while the phrase is often cited as proof of abductive reasoning, it is not proof, as the mechanical duck is still not a living duck
>
> [Duck Test](https://en.wikipedia.org/wiki/Duck_test) entry, [@wikipedia2022] 

What is interactive data visualization? Surprisingly, despite the widespread popularity of interactive visualizations, if you ask researchers, you may get many different and at times even incongruent answers [see e.g. @dimara2019; @elmqvist2011; @pike2009]. Within the literature, the terms "interactive" and "interaction" are used in many different ways and across a wide variety of contexts, with an explicit definition being rarely given. 

The lack of a clear consensus about what "interactive data visualization" is makes the task of discussing existing work challenging. On one hand, ignoring the issue might leave the reader confused about the relevant concepts. On the other, a comprehensive account of the terminology surrounding interactive data visualization would almost surely become too dense; entire research papers have been dedicated to this topic [see e.g. @dimara2019; @elmqvist2011]. Therefore, in the following section, I have tried to strike a balance by providing a concise yet informative-enough account of how interactivity has been conceptualized within the existing literature. Ultimately, the goal is to provide the reader with context and define what the terms "interactive" and "interaction" should mean for the scope of the present thesis.

The following section is laid out as follows. I start with a brief overview of the history of the field, of data visualization more generally and interactive data visualization more specifically. Then I discuss the different ways the term "interactive data visualization" has been used throughout the literature, and finally I arrive at a working definition of interactivity for the scope of the present thesis.

### Interactive vs. interacting with

First, when we say "interactive data visualization", are we referring to a concrete figure or chart, or are we referring to the process of interacting with such a figure? In other words, are we using the term "visualization" as a noun or a verb? Here already we can see a significant overloading of the term [@dimara2019; @pike2009; see also @yi2007]. The split between these two meanings is quite noticeable within the interactive data visualization literature. On one hand, there are some papers which focus on the mathematical and computational aspects of interactive data visualization, discussing specific systems and implementations [see e.g. @buja1996; @kelleher2015; @leman2013; @wills2008]. On the other hand, there are papers which approach the topic from a more cognitive or human-computer interaction (HCI) point of view: exploring what impact different kinds of visualization and interaction styles have on the user's ability to derive insights from the data [see e.g. @dimara2019; @dix1998; @pike2009; @quadri2021; @yi2007]. 

There is of course a significant overlap between these two uses of the term "interactive data visualization": most papers discuss both concrete implementations of interactive data visualization systems and the user's actions and experiences while using those systems. Nevertheless, the fact that the term is used to refer to both the user's actions and experiences as well as the object of these actions and experiences can make can make literature search complicated - whenever searching for any subtopic within one of the two fields, one will inevitably find hits from the other. It also highlights an important fact about interactive data visualization as a research area: rather than being a single field, it is actually an intersection of several different fields, including statistics, computer science, applied mathematics, business analytics, human-computer interaction, and cognitive psychology [@dimara2019]. 

While I do plan to discuss some elementary features of the psychology of *interacting* with visualizations, when used throughout this thesis, the term *"interactive data visualization"* will refer to concrete charts or figures, typically displayed on a computer screen. When referring to the *practice* of interactive data visualization, I will attempt to use more active phrasing such as *"interacting with a visualization"* or *"user's interaction with a visualization"*, to indicate that what is being referred to is the activity or process of visualization, rather than any concrete object or implementation. 

### What counts as "interactive enough"?

But even when we use the term "interactive data visualization" to refer to concrete charts or figures, the meaning still remains ambiguous. What is the bar for calling a figure "interactive"? What features should an interactive figure have? Among data visualization researchers, there are considerable differences of opinion, such that the same figure may be considered interactive by some but not by others. And these differences are important - they are not just a matter of opinion or aesthetic taste. When building interactive data visualization systems, what we consider "interactive" has a profound impact on the implementation details and requirements of the system.  

Consider a scatterplot with a color palette widget that can be used to select the color of the points. Does such a feature justify the scatterplot being called an "interactive data visualization"? There are some researchers who will answer affirmatively - for them, interactivity is something fundamental, and if the user is able to manipulate some visual aspect of the figure, that's enough to call the visualization interactive. To some, almost any user manipulation qualifies [@brodbeck2009]. Other researchers emphasize speed of the computer's responses to user interaction, with faster updates translating to greater interactivity [@becker1987; @buja1996]. Complicating matters further, some of these researchers also make the distinction between "interactive" and "dynamic" manipulation, where interactive manipulation involves discrete actions such as pressing a button or selecting an item from a drop-down menu, whereas dynamic manipulation involves continuous actions, like moving a slider or clicking-and-dragging to highlight a rectangular area [@rheingans2002; @jankun2007; see also @dimara2019]. 

Yet, for other researchers, simple features such as changing the color of points in a scatterplot are far too low of a bar. For many, true interactivity hinges on high-level analytic features which allow the practitioner to derive insights from the data which would be much harder or time-intensive to derive from static visualizations. These features include the ability to generate different views of the data (by e.g. zooming, panning, sorting, and filtering), and the reactive propagation of changes between connected or "linked" parts of a figure [@kehrer2012; @buja1996; @keim2002; @unwin1999]. Similarly, in visual analytics research, a distinction is made between "surface-level" (or "low-level") and "parametric" (or "high-level") interactions, where surface-level interactions manipulate attributes of the visual domain only (e.g. zooming and panning), whereas parametric interactions manipulate attributes of mathematical models or algorithms underlying the visualization [@leman2013; @pike2009]. 

Table \@ref(tab:definitions) summarizes the several ways of defining interactivity as discussed above. Note that the list is not supposed to be exhaustive; more complete taxonomies of interactive visualization systems and features have been described before [see e.g. @dimara2019; @yi2007]. Instead, I want to use the list to broadly summarize the ways researchers have thought about interactivity, and to have a place to refer the reader to when discussing these ideas later on in the text.

```{r definitions}
#| echo: false

library(kableExtra)

tab <- data.frame(
  type = c("User interaction", 
           "Real-time updates", 
           "Plot- and data-space manipulation",
           "Linked views",
           "Parametric updates"),

  details = c("The user can interactively manipulate the visualization in some way",
              "The user's interactions propagate into the visualization with little to no lag",
              'The user can interactively explore different parts of the data set by doing actions which effectively amount to "subsetting" rows of the data (e.g. zooming, panning, and filtering)',
              r"(The visualization consists of connected or "linked" parts and the user's interactions with one part propagate to the other parts (e.g. linked highlighting))",
              "The user can manipulate the parameters of some underlying mathematical model or algorithm (e.g. histogram bins, grand tour projections, etc...)")
)

colnames(tab) <- c("Feature", "Details")

knitr::kable(tab, caption = "Definitions of Interactivity") |> kable_styling(full_width = FALSE)
# odd_rows <- (1:nrow(tab))[1:nrow(tab) %% 2 == 1]
# 
# flextable(tab) |>
#   theme_booktabs(bold_header = TRUE) |>
#   set_table_properties(width = 1, layout = "autofit") |>
#   bg(i = odd_rows, bg = "grey95") |>
#   border_inner_h(border = fp_border_default(color = "grey80"))

```

### Complexity of interactive features

The different definitions of interactivity are not just differences of opinion or taste - they also have a significant impact on implementation requirements. To start with a perhaps slightly over-exaggerated example, many programming languages come equipped with a read-evaluate-print loop (REPL) which can be used to interactively execute code from the command line. The user writes code, presses ENTER, and the language interpreter evaluates the code, returns any output, and waits for more input from the user. Now, if the language in question supports plotting, then, under the permissive "user interaction" definition, it could be argued that even the act of running code from a command line to produce new plots could be considered an "interactive data visualization system", since the user's interaction with the REPL produces changes to the visual output. And, hypothetically, if the user could type fast enough, they would see the updates appear almost instantly, satisfying the "real-time update" definition.

Does this mean that every programming language which has a REPL and supports plotting automatically ships with an interactive data visualization system? I would argue that no: most people nowadays probably do not consider the command line to be an interactive data visualization system. But perhaps it has not always been this way. Several decades ago, the command line played a much bigger role as an interactive user interface [see e.g. @foley1990; @howard1995]. Compared to waiting seconds or minutes for code to compile, a REPL is indeed a much more interactive experience. However, with the rise in processor speed and the proliferation of highly interactive graphical user interfaces (GUIs), users have come to expect visualizations that can be interacted with *directly* [@dimara2019]. As such, our perceptions of what is "interactive" are not constant but change over time; as technologies improve, we come to expect more direct and responsive user interfaces.

Now, let's set the somewhat exaggerated example of the REPL aside, and focus on what today would be considered more "typical" examples interactive data visualization systems. That is, systems in which the user can interact with the visualizations directly, by pressing keys or mouse buttons. Then, there still are considerable differences in what different features imply for implementation requirements.  

There are features which manipulate visual attributes of the plot only, independent of the data. These include, for example, changing the size, color, or opacity of points in a scatterplot. Features like this are usually fairly simple to implement because they do not affect the underlying data representation: a point displays the same data (as indicated by its xy-coordinates) no matter whether it is green or orange. Also, these graphical-only features typically do not require specialized data structures, and have low time- and space-complexity: for example, when interactively changing the opacity of points in a scatterplot, we only need to update one scalar value - the points' opacity - and as such most of the user-experienced time will be spent re-rendering, rather than on any computation.     

In contrast, some interactive features require specialized data structures and complex algorithms, above and beyond those that are required for static plots. For instance, each time the user engages in interactive features such as filtering, linked highlighting, or parametric interaction, new summaries of the underlying data may need to be computed. When a user selects several points in a linked scatterplot, we first have to find the ids of all the corresponding cases, recompute the statistics underlying all other linked plots (such as counts/sums in barplots or histograms), train all of the relevant scales, and only then can we re-render the plot. Likewise, if we interactively manipulate a histogram's binwidth, we need to recompute the number of cases in each bin each time the binwidth changes. To maintain the illusion of smooth, "continuous" interaction [@dimara2019], these computations need to happen fast, and as such, computational efficiency becomes imperative.

### Working definition

Clearly, when building an interactive data visualization system, what we choose to call "interactive" has profound implications on our implementation. So how do we decide what we should consider "interactive"? 

There are essentially two modes of visualizing data. 

In statistics, the goal of data visualization is to facilitate rapid, accurate, and effective data exploration. 

However, it is not always the case that more complex visuals necessarily translate to better statistical insights. In static visualization, it is a well-established fact that plots can include sophisticated-looking and seemingly appealing features which do not promote the acquisition of statistical insights in any way [@cairo2014; @cairo2019; @gelman2013; @tufte2001]. Similarly, adding interactivity to a visualization does not always improve its statistical legibility [see e.g. @abukhodair2013; @franconeri2021]. 

I propose to approach interactive features the same way we treat visual features in static visualization. Specifically, I propose the following working definition:

> To justify being called an "interactive data visualization", the interactive features in a visualization should promote statistical understanding.

If we accept this proposition, then there are several important consequences that follow. First, we must favour high-level, data-dependent, parametric interactions over the purely graphical ones.

That is not to say that purely graphical interactive features are not useful. For example, in the presence of overplotting, manipulating size or alpha of objects can help us features (areas of high density) that would otherwise remain hidden. Likewise, zooming and panning, while often being counted among the more high-level features, require manipulation of existing axis limits only and can be done without reference to the original data. Still, I argue that the ability to see new summaries of the data is what makes some interactive data visualizations systems ultimately more powerful (and also more challenging to implement). The interactive features that enable this, such as filtering, linked highlighting, and parameter manipulation, go beyond aesthetics, and empower the users to explore the data dynamically, uncovering hidden patterns and relationships that may otherwise remain hidden. 

### List of common interactive features

This section describes several common types of interactive features that facilitate interactive data exploration. 

## The highlighting problem

As was discussed in the previous section, linked selection is one of the most highly ranked interactive features in interactive data visualization. It allows us to mark specific cases in one plot, and see the corresponding summary statistics in all the other plots. In this way, it allows us to quickly explore different dynamically-generated subsets of our data, within the context of the whole data set.

### Linked selection and stacking

When we engage in linked selection, we want to highlight parts of objects corresponding to the selected cases. Thus, fundamentally, we need to break each object into parts. We then need some way of representing those parts, in a way that visually preserves the hierarchical nature of the relationship (it needs to be clear that the parts are still parts of the whole object).

In data visualization, there are three common methods for dealing with objects that have been split into parts: stacking, dodging, and layering. Let's briefly illustrate these on the example of the barplot. We start with the whole bars and then break each bar into multiple segments based on the levels of another variable (such as selection status). Now, what we do with these segments will differ based on the technique we use. When stacking, we plot the bar segments vertically on top of each other. When dodging, we plot the bars side-by-side, as "clusters". Finally, when layering, we plot bar segments in separate graphical layers and use partial transparency to mitigate overplotting.

```{r}
#| echo: false
#| fig-cap: "Examples of the three methods for dealing with objects that 
#| have been split into parts: stacking, dodging, and layering."
#| fig-height: 3
library(ggplot2)
library(patchwork)

mtcars$cyl <- factor(mtcars$cyl)
mtcars$am <- factor(mtcars$am)

p0 <- ggplot(mtcars, aes(cyl, fill = am, group = am)) +
  scale_fill_manual(values = pal_dark_3) +
  guides(fill = "none") +
  labs(x = NULL, y = NULL) +
  clean_theme

p1 <- p0 + geom_bar() + labs(title = "Stacking")
p2 <- p0 + geom_bar(position = position_dodge()) + labs(title = "Dodging")
p3 <- p0 + 
  geom_bar(position = position_identity(), alpha = 0.75) + 
  labs(title = "Layering")

p1 + p2 + p3
```

Much has been written about the relative merits of stacking, dodging, and layering. For example, layering is only useful with few categories, as blending many colors can make it difficult to tell the categories apart [@franconeri2021; @wilke2019]. Further, in a landmark study, @cleveland1984 showed that people tend to be less accurate when reading information from stacked bar charts as opposed to dodged bar charts. Specifically, since the lower y-axis coordinate of a stacked segment is pushed up by the cumulative height of the segments below, it becomes difficult to accurately compare segments' length, both within and across bars [@cleveland1984]. Subsequent research has independently validated these findings and expanded upon them [see e.g. @heer2010; @thudt2016; @quadri2021]. Due to this suboptimal statistical legibility, many data visualization researchers have urged caution about stacking [see e.g. @byron2008; @cairo2014; @franconeri2021], and some have even discouraged its use altogether [@kosara2016; @wilke2019]. 

However, much of this research has been done with on static visualizations. I argue that, in interactive data visualization, there are additional constraints that affect the calculus of which method is the best.

First, if we want to support the ability to interactively manipulate the alpha channel and do linked selection with multiple (3+) selection groups, we can effectively eliminate layering as an option. Having two different sources of alpha channel (layering and user input) would make the interaction confusing and limit its range (e.g. if we use 75% alpha to do layering, then the effective range of values the user can manipulate is 0-75%). Further, with multiple selection groups, it would become hard to tell the selection status apart. It is much better to keep the two interactive features (selection and alpha manipulation) orthogonal and use a separate visual attribute (color and transparency) for each.

Thus, the choice is between stacking and dodging. While, as was discussed above, dodging is the preferred choice in static data visualization, I propose that 

First, I propose that stacking is the superior method for *displaying* selection. Thanks to the four Gestalt principles of proximity, similarity, closure, and common region, in a stacked barplot, each bar presents itself as a single visual entity. In other words, since the stacked segments are placed right on top of each other, and share width and a closed border, they are perceived as part of a unified whole (the bar). This remains true throughout selection - no matter how the heights of the highlighted segments change, the height of the whole stacked bar remains constant, and so does the overall outline of the plot.

To address the highlighting problem, we need to first discuss some key mathematical concepts.

## Mathematical theory

This section starts with some foundational mathematical concepts, such as functions, relations, and orders, and slowly builds up to more advanced topics such as categories, monoids, and functors. Readers familiar these concepts may feel free to skip ahead. However, even the fundamental concepts will be used throughout the thesis, so a refresher might be beneficial. 

The material covers topics from category theory and abstract algebra and follows primarily from @fong2019, @lawvere2009, @baez2023, @pinter2010, and @milewski2018. For an accessible introduction to the topic, interested readers are encouraged to consult these references, particularly @fong2019 and @lawvere2009.

#### Note on past applications of category theory to data visualization

There have been some applications of category theory to data visualization in the past. Specifically, several authors have used category theory to describe the more abstract aspects of data visualization, with some trying to lay down the theoretical foundations for what it means to "visualize" [@beckmann1995] or for which visualizations can be considered well-formed perceptual representations of the data [@kindlmann2014]. Handful of authors have also jointly discussed category theory and data visualization in the context of functional programming, introducing new functional programming libraries and domain-specific languages for data visualization [see e.g. @yorgey2012; @petricek2021; @smeltzer2014; @smeltzer2018]. 

The present thesis seeks to do neither. Instead, the goal is to use category theory to pin down the algebraic properties of the statistics underlying our visualizations, to determine which combinations of graphics, statistical summaries, and interactive features will work well together. Thus, the target is somewhere in the middle: provide a theoretical framework which will guide application.

### Relations

A relation is one of the simplest mathematical structures. Given two sets $X$ and $Y$, a relation $R$ between $X$ and $Y$ is a subset of the Cartesian product of the two sets, $R \subseteq X \times Y$. In other words, a relation can be thought of as the subset of pairs $(x, y) \in X \times Y$ for which the condition "$x$ and $y$ relate" holds. 

There are many different types of relations. One of the most fundamental relations is equality; in this case, "$x$ and $y$ relate" means that, for our purposes, $x$ and $y$ are the same, i.e. $x = y$. Other examples of relations include the usual order relations $<$, $\leq$, $>$, $\geq$, or the divides operator $\mid$ ($x \mid y$ means "$x$ divides $y$ without remainder"). Finally, note that $X$ and $Y$ can be the same set, such that $R \subseteq X \times X$.

Since a relation is a subset of the product set $X \times Y$, we can visualize it as a matrix, with values of $X$ as rows, values of $Y$ as columns, and the related pairs $(x, y)$ marked out in some specific way. For example, here's how we can display the order relation $\leq$ on the set $X = \{ 1, 2, 3 \}$:

```{r}
#| echo: false
#| dpi: 300
#| fig-cap: "A relation is a subset of the Cartesian product of two sets.
#| The diagram shows the usual order relation $\\leq$.
#| We can see that 1 is less than or equal to every other element, 
#| 2 is less than or equal to 2 and 3, and 3 is less than or equal to 3 only.
#| Note the symmetry between rows and columns - this is due to the fact that
#| the same set ($X$) is display on both dimensions." 

library(ggplot2)

df_relation <- expand.grid(x = 1:3, y = 1:3)
df_relation$rel <- df_relation$x <= df_relation$y

ggplot(df_relation, aes(x, y, col = factor(rel))) +
  geom_point(size = 25) +
  scale_x_continuous(breaks = 1:3, limits = c(0.5, 3.5)) +
  scale_y_continuous(breaks = 1:3, limits = c(0.5, 3.5)) +
  scale_color_manual(values = c("white", "indianred")) +
  labs(x = "X", y = "X") +
  guides(col = "none") +
  clean_theme +
  theme(panel.background = element_rect(fill = "antiquewhite"),
        panel.border = element_blank(),
        axis.title.y = element_text(angle = 0, vjust = 0.5),
        axis.title = element_text(colour = "grey60"))
```

As the examples above show, some relations $R$ are marked with an infix symbol (such as $\star$), such that, if $x$ and $y$ relate, i.e. $(x, y) \in R$, then we write $x \star_R y$ or $x \star y$ ($R$ implicit), for example, $x = y$, $x \leq y$, and so on. In less common types of relations, $R$ is also sometimes used as the infix symbol, e.g. $x R y$. If two elements do not relate, i.e. $(x, y) \not \in R$, we typically do not write this out explicitly - the lack of relation is indicated by its absence.

Relations can have properties. For example, some types of relations are *reflexive*, meaning that $x \star x$ for all $x \in X$ (every element relates to itself). This is the case for equivalence relations. In fact, we can define equivalence relations using just three properties:

::: {.definition name="Equivalence relation"}
A relation $\sim$ on $X$ is called an equivalence relation if it is:

1. *Reflexive*: $x \sim x$ for all $x \in X$
2. *Symmetric*: $x \sim y$ if and only if $y \sim x$ for all $x, y \in X$
3. *Transitive*: if $x \sim y$ and $y \sim z$, then $x \sim z$
:::

Equivalence relations encode the notion that two things are the same, *for our purposes*. We can further use them to assign objects in $X$ to *equivalence classes*, which divide $X$ into groups of equivalent objects. That is, for some element $a \in X$, its corresponding equivalence class is:

$$[a] = \{ x \in X : x \sim a \}$$

While relations might seem simple, they are very versatile. The next few sections will discuss three important examples of relations: functions, partitions, and preorders. 

### Functions

A function is a special kind of relation, one that encodes a mapping between two sets. More specifically, let $S$ be the set of sources (also called the *domain*) and $T$ be the set of possible targets (also called the *codomain*). Then, we can think of a function as a relation $F \subseteq S \times T$ of valid source-target pairs $(s, t)$, such that for every $s \in S$ in there exists a unique $t \in T$ with $(s, t) \in F$ (see Figure \@ref(fig:function-subset)). In other words, every source relates to exactly one target:

```{r function-subset}
#| echo: false
#| dpi: 300
#| fig-cap: "A function is a type of relation.
#|  Specifically, it is a subset of the Cartesian product of its domain ($S$) and codomain ($T$),
#|  such that element in the domain marks out exactly one element in the codomain (shown in red). 
#| The depicted function has the following characteristics:
#|  $F: \\{ 1, 2, 3 \\} \\to \\{ 1, 2, 3 \\}$, such that $F(1) = 1$, $F(2) = 1$, and $F(3) = 2$.
#|  One possible example of this function might be $f(x) = \\lfloor x / 2 \\rceil$ 
#|  (divide $x$ by two and round to the nearest whole number). 
#|  Note that, for any function, there must be exactly one red dot in each column 
#|  (each source maps to one and only one target), 
#|  however, there may be zero or many red dots in any row 
#|  (some targets may not be reached from any source, and others
#|  may be reachable from many sources)."

library(ggplot2)

df_relation <- expand.grid(x = 1:3, y = 1:3)
df_relation$rel <- with(df_relation, round(x / 2 + 0.1) == y)

ggplot(df_relation, aes(x, y, col = factor(rel))) +
  geom_point(size = 25) +
  scale_x_continuous(breaks = 1:3, limits = c(0.5, 3.5)) +
  scale_y_continuous(breaks = 1:3, limits = c(0.5, 3.5)) +
  scale_color_manual(values = c("white", "indianred")) +
  labs(x = "S", y = "T") +
  guides(col = "none") +
  clean_theme +
  theme(panel.background = element_rect(fill = "antiquewhite"),
        panel.border = element_blank(),
        axis.title.y = element_text(angle = 0, vjust = 0.5),
        axis.title = element_text(colour = "grey60"))
```

We can classify functions based on how their domains and codomains relate to each other (see Figure \@ref(fig:function-types)). If every target in the function's codomain has a path leading to it from some source, such that no target remains unreachable, then we call it a *surjective* or *onto* function. More formally:


::: {.definition name="Surjectivity"}
A function $f$ is surjective if, for all $t \in T$, there exists a $s \in S$ such that $f(s) = t$. 
:::


Alternatively, if each source in the function's domain leads to a unique target, such that no two sources map to the same target, then we call such a function *injective* or *one-to-one*. That is: 

::: {.definition name="Injectivity"}
A function is injective if for all $s_1, s_2 \in S$, if $f(s_1) = t$ and $f(s_2) = t$, then $s_1 = s_2$. 
:::

Finally, if a function is both surjective and injective, meaning that every target can be reached from, and only from, a unique source, then we call such a function *bijective* or a *bijection*.

::: {.definition name="Bijectivity"}
A function is a bijection and only if it is both surjective and injective, which is also the case if and only if it is invertible. 
:::

```{r, function-types}
#| echo: false
#| dpi: 300
#| fig-cap: "Types of functions. 
#| Left: in a *surjective* function, each target can be reached from some source. 
#| Middle: in an *injective* function, there is a unique source for each target.
#| Right: in a *bijection*, each target can be reached from, and only from, a unique source."
 
library(ggplot2)
library(patchwork)

normalize <- function(x) (x - min(x)) / (max(x) - min(x))

plot_function <- function(mapping, codomain_size = NULL) {
  
  codomain_size <- ifelse(is.null(codomain_size), max(mapping), codomain_size)
  
  domain <- 1:length(mapping)
  codomain <- 1:codomain_size
  
  # Center on the same value
  diff <- mean(domain) - mean(codomain)
  domain <- domain - diff / 2
  codomain <- codomain + diff / 2
  
  domain <- normalize(domain)
  codomain <- normalize(codomain)
  
  point_data <- data.frame(x = c(domain, codomain), 
                           y = rep(c(1, 0), sapply(list(domain, codomain), length)))
  arrow_data <- data.frame(x = domain, xend = codomain[mapping], 
                           y = 1, yend = 0)
  
  h <- 0.25
  
  ggplot(point_data, aes(x, y)) +
    geom_rect(aes(xmin = -0.2, xmax = 1.2, ymin = 0 - h / 2, ymax = 0 + h / 2),
              fill = "antiquewhite") +
    geom_rect(aes(xmin = -0.2, xmax = 1.2, ymin = 1 - h / 2, ymax = 1 + h / 2),
              fill = "antiquewhite") +
    geom_text(x = -0.1, y = 1.08, label = "S", size = 4, col = "grey60") +
    geom_text(x = -0.1, y = -0.08, label = "T", size = 4, col = "grey60") +
    geom_point(size = 5, col = "indianred") +
    geom_segment(data = arrow_data, aes(x = x, xend = xend, y = y, yend = yend),
                 arrow = arrow(angle = 30, length = unit(0.025, "npc")),
                 col = "steelblue") +
    scale_x_continuous(limits = c(-0.2, 1.2)) +
    scale_y_continuous(limits = c(-0.2, 1.2)) +
    clean_theme +
    theme(panel.border = element_blank(),
          axis.text = element_blank(),
          axis.title = element_blank(),
          plot.title = element_text(vjust = -5, hjust = 0.5))
}

p1 <- plot_function(c(1, 2, 1, 3, 4, 4)) + labs(title = "Surjective")
p2 <- plot_function(c(3, 4, 2, 5), 6) + labs(title = "Injective")
p3 <- plot_function(c(4, 2, 6, 3, 1, 5)) + labs(title = "Bijective")
 
p1 + p2 + p3

```

Bijections are special since they encode the idea of reversible transformations. Any bijective function $f$ has an associated inverse function $f^{-1}$ such that $f^{-1}(f(x)) = x$ and $f(f^{-1}(y)) = y$ for all $x$ and $y$ in the function's domain and codomain, respectively. In other words, we can keep translating the value from the domain to codomain and back without losing any information. We will generalize this idea later when we discuss *isomorphisms*.

As an example, suppose I have a group of friends $f \in F$ that each went to one city $c \in C$ in Europe during the holiday. I can construct a function $f: F \to C$ that sends each friend to his or her holiday destination. If every city in $C$ was visited by at least one friend, then the function is surjective. If each friend went to a different destination, then the function is injective. If both are true - that is, if every city on our list was visited by exactly one friend - then the function is bijective. In that case, we could just as well use the names of cities $c \in C$ when we speak of friends $f \in F$ - instead of "Sam", we could say "the person who went to Rome", and it will be clear who are we talking about.

An important property of functions is that they can be composed. Specifically, if the domain of one function matches the codomain of another, the functions can be composed by piping the output of the first function is used as the input of the second. Then we end up with a new, composite function: 

::: {.definition name="Function composition"}
If we have two functions $f: X \to Y$ and $g: Y \to Z$, we can form a new function $h: X \to Z$ such that:

$$h(x) = g(f(x))$$
In terms of notation, we can omit the explicit reference to the variable $x$ and write the composition in several different ways:

1. $h = g \circ f$ (read: "apply $g$ after $f$")
2. $h = gf$ (same as above)
3. $h = f ⨾ g$ (read "apply $f$ then $g$")
:::

I will use the bracket notation ($h(x) = g(f(x))$) when explicitly referring to the variable, and the postfix/fat semicolon notation ($h = f ⨾ g$) otherwise.   

There are other things we can do with functions. For example, given a subset of sources, we can ask about the *image* - the set of targets we can reach from those sources:

::: {.definition name="Image"}
For some subset $S_i \subseteq S$, its image under $f$ is defined as $f_!(S_i) = \{ f(s) \in T \lvert s \in S_i \}$. 
:::

Likewise, given a subset of targets, we can ask about the *pre-image* - the set of sources that could have produced those targets. That is:

::: {.definition name="Pre-image"}
For some subset $T_i \subseteq T$, its pre-image under $f$ is defined as $f^*(T_i) = \{ s \in S \lvert f(s) \in T_i \}$. 
:::

An important fact to note is that, although the pre-image $f^*$ is also sometimes called "inverse image", it is *not* the inverse of the image $f_!$ for most functions (unless they are bijections). That is, by applying the pre-image after image or vice versa, we cannot expect to come up with the same set as we started with. Specifically, if we have a non-injective function and apply the pre-image after the image, we may come up with *more* sources that we started with, $S_i \subseteq f^*(f_!(S_i))$ (equality if injective), and similarly, if we have a non-surjective function and apply the image after the pre-image, we might end up with *fewer* targets than we started with, $f_!(f^*(T_i)) \subseteq T_i$ (again, equality if surjective). 

As an example, suppose again I have the function $f$ which maps each friend to a holiday destination. The image of that function, $f_!$, maps a set of friends to the set of all cities that at least one of them went to, and similarly, the pre-image, $f^*$, maps a set of cities to the set of friends that went to them.  
Now, suppose that Sam and Dominic went to Rome, and I ask:

> *"who went to [the city that Sam went to]?"*

I will get both Sam and Dominic back, since:

$$f^*(f_!(\{ Sam \})) = f^*(\{ Rome \}) = \{ Sam, Dominic \}$$

That is, I will get back Sam and Dominic *even though I had initially only asked about Sam*. Similarly, if no friends had visited Paris and I ask:

> *"what are the cities that [people who went to Paris or Rome] went to?"*

then I will get Rome only, since 

$$f_!(f^*(\{Paris, Rome \})) = f_!(\{ Sam, Dominic \}) = \{ Rome \}$$

This weird relationship between the the image and the pre-image is due to the fact that the image is actually something called *left adjoint* [@baez2023; @fong2019]. Adjoints can be thought of as the "best approximate answer to a problem that has no solution" [no inverse, @baez2023], and they come in pairs - a left and a right adjoint - with the left adjoint being more permissive or "liberal" and the right adjoint being more strict or "conservative" [@baez2023].

Proper treatment of adjoint is beyond the scope of this thesis, however.

### Partitions

One useful thing we can construct with functions (or equivalently, relations) are partitions. Partitions encode the idea of splitting elements of some some into distinct groups.  

::: {.definition name="Function definition of a partition"}
Given some set $X$, a set of part labels $P$, and a surjective function $f: X \to P$, we can partition $A$ by assigning every element $x \in X$ a part label $p \in P$, by simply applying the function: $f(x) = p$.
:::

Above we used a function to define a partition, however, we can achieve the same with a relations, specifically equivalence classes. By taking any part label $p \in P$, we can recover the corresponding subset of $X$ by pulling out its pre-image: $f^*(\{p\}) = X_p \subseteq X$. We can then define a partition without reference to $f$:

::: {.definition name="Equivalence class definition of a partition"}
A partition of $A$ consists of a set of part labels $P$, such that, for all $p \in P$, there is a non-empty subset $A_p \subseteq A$ which forms an equivalence class on $A$ and:

$$X = \bigcup_{p \in P} X_p \qquad \text{and} \qquad \text{if } p \neq q, \text{ then } X_p \cap X_q = \varnothing$$
I.e. the parts $X_p$ jointly cover the entirety of $X$ and parts cannot share any elements.
:::

We can rank partitions by their coarseness. That is, for any set $X$, the coarsest partition is one with only one part label $P = \{ 1 \}$, such that each element of $X$ gets assigned $1$ as label. Conversely, the finest partition is one where each element gets assigned its own unique part label, such that $\lvert X \lvert = \lvert P \lvert$. 

Given two partitions, we can form a finer (or at least as fine) partition by taking their intersection, i.e. by taking the set of all unique pairs of labels that co-occur for any $x \in X$ as the new part labels. For example, suppose $X = \{ 1, 2, 3 \}$ and partition 1 assigns part labels:

$$p_1(x) = \begin{cases} 
a & \text{if } x = 1 \text{ or } x = 2 \\
b & \text{if } x = 3
\end{cases}$$

and partition 2 assigns part labels the following way:

$$
p_2(a) = \begin{cases}
s & \text{if } x = 1 \\
t & \text{if } x = 2 \text{ or } x = 3
\end{cases}
$$

Then the intersection partition will have the following part labels $P_3 = \{ (a, s), (a, t), (b, t) \}$ such that:

$$
p_3(a) = \begin{cases}
(a, s) & \text{if } x = 1 \\
(b, s) & \text{if } x = 2 \\ 
(b, t) & \text{if } x = 3
\end{cases}
$$

### Preorders

::: {.definition name="Preorder"}
A preorder is a set $X$ equipped with a binary relation $\leq$ that conforms to two simple properties:

1. *Reflexivity*: $x \leq x$ for all $x \in X$
2. *Transitivity*: if $x \leq y$ and $y \leq z$, then $x \leq z$, for all $x, y, z \in X$
:::

Simply speaking, this means that between any two elements in $X$, there either is a relation and the elements relate (one element is somehow "less than or equal" to the other), or the two elements do not relate. 

An example of a preorder is the family tree, with the underlying set being the set of family members: $X = \{  \textbf{daughter}, \textbf{son}, \textbf{mother}, \textbf{father}, \textbf{grandmother}, ... \}$ and the binary relation being ancestry or familial relation. Thus, for example, $\textbf{daughter} \leq \textbf{father}$, since the daughter is related to the father, and $\textbf{father} \leq \textbf{father}$, since a person is related to themselves. However, there is no relation ($\leq$) between $\textbf{father}$ and $\textbf{mother}$ since they are not related. Finally, since $\textbf{daughter} \leq \textbf{father}$ and $\textbf{father} \leq \textbf{grandmother}$, then, by reflexivity, $\textbf{daughter} \leq \textbf{grandmother}$.

We can further restrict preorders by imposing additional properties, such as:

3. If $x \leq y$ and $y \leq x$, then $x = y$ (anti-symmetry)
4. Either $x \leq y$ or $y \leq x$ (comparability)

If a preorder conforms to 3., we speak of a partially ordered set or *poset*. If it conforms to both 3. and 4., then it is a *total order*. 

### Monoids

A monoid is a tuple $(M, e, \otimes)$ consisting of:

a. A set of objects $M$
b. A neutral element $e$ called the *monoidal unit*
c. A binary function $\otimes: M \times M \to M$ called the *monoidal product*

Such that:

1. $m \otimes e = e \otimes m = m$ for all $m \in M$ (unitality)
2. $m_1 \otimes (m_2 \otimes m_3) = (m_1 \otimes m_2) \otimes m_3 = m_1 \otimes m_2 \otimes m_3$ for all $m_1, m_2, m_3 \in M$ (associativity)

In simple terms, monoids encapsulate the idea that *the whole is exactly the "sum" of its parts* (where "sum" can be replaced by the monoidal product). Specifically, we have some elements and a way to combine them, and when we combine the same elements, no matter where we put the brackets we always get the same result (i.e. something like "the order does not matter", although that is not precisely right, more on that later). Finally, we have some neutral element that when combined with an element yields back the same element.  

For example, take summation on natural numbers, $(\mathbb{N}, 0, +)$:

$$1 + 0 = 0 + 1 = 1 \qquad \text{(unitality)}$$
$$1 + (2 + 3) = (1 + 2) + 3 = 1 + 2 + 3 \qquad \text{(associativity)}$$

Likewise, products of real numbers $(\mathbb{R}, 1, \times)$ are also a monoid, and so is multiplication of $n \times n$ square matrices $(\mathbf{M}_{n \in \mathbb{Z}}, \mathbf{I}, \cdot)$, where $\mathbf{I}$ is the identity matrix and $\cdot$ stands for an infix operator that is usually omitted. As a counterexample, exponentiation does not meet the definition of a monoid, since it is not associative: $x^{(y^z)} \neq (x^y)^z$. 

There are more exotic forms of monoids as well. For example, the operation of appending a value to a vector and taking the Euclidean norm can also be recast as a monoid:

$$||(||(x, y)||_2, z)||_2 = \sqrt{\bigg(\sqrt{(x^2 + y^2)}\bigg)^2 + z^2} = \sqrt{(x^2 + y^2) + z^2} = ||(x, y, z)||_2$$

We may want to impose further restrictions on monoids, for example:

3. $m_1 \otimes m_2 = m_2 \otimes m_1$ for all $m_1, m_2 \in M$ (commutativity)

Both commutativity and associativity can both be viewed as a kind of "order does not matter" rule, however, they are fundamentally different. Let's imagine our set of objects consists of three wires of different colours $\{ \textbf{red}, \textbf{green}, \textbf{blue} \}$ and the monoidal product consists of connecting wires. Let's also imagine that the $\textbf{red}$ wire is connected to a power source and the $\textbf{blue}$ wire is connected to a lightbulb, and the blue wire amplifies the current from the power source such that it is enough to power the light bulb. To turn on the lightbulb, we need to connect $\textbf{red} \to \textbf{green}$ and $\textbf{green} \to \textbf{blue}$. The time order in which we connect the three wires does not matter: we can connect $\textbf{green} \to \textbf{blue}$ first and $\textbf{red} \to \textbf{green}$ second or vice versa, either way we get the same result (lightbulb turns on). However, the spatial order in which we connect the wires *does* matter: if we connect $\textbf{red} \to \textbf{blue}$, then the current will not be enough to power the lightbulb. Hence, the operation is associative (temporal order does not matter) but not commutative (spatial order does matter).      

If $M$ is a preorder, another restriction we may want to impose is that the monoidal product is strictly increasing:

4. $m_1 \leq m_1 \otimes m_2$ and $m_2 \leq m_1 \otimes m_2$ for all $m_1, m_2 \in M$ (monotonicity)

This means that when we combine two things, we get back something that's at least as big as the bigger of the two things. Summation of natural numbers $(\mathbb{N}, 0, +)$ again works, but for example summation of integers $(\mathbb{Z}, 0, +)$ or multiplication of reals $(\mathbb{R}, 1, \times)$ does not. 

## Theory of data visualization systems

### Scales

Every data visualization system needs some way of translating abstract data values into concrete graphical attributes such as position, size, or colour. Given the ubiquitous need for scales, one might expect them to be a "solved issue", within the relevant literature. However, this is far from the truth. Specifically, the issues of scales and measurement are still being grappled with in the areas of mathematics and philosophy of science to this day [for a gentle yet thorough introduction, see @tal2015].

Scales are another area of data visualization in which there has been considerable debate, with many terms being overloaded and relating to concepts from different fields. This is due to the fact that the issue of how to compare, rank, and translate values has a long and complicated history. In particular, the issue of measurement has been hotly debated in the field of psychometrics and mathematical psychology, leading to the development of the *theory of measurement* (which has some overlap with, but is not the same as, *measurement theory* in mathematics). 

One paper that has been key to the debate around scales and measurement has been the seminal work of @stevens1946. In this paper, Stevens defined a *scale* as a method of assigning numbers to values, allowing for various kinds of comparisons. Further, by considering transformations which preserve the comparisons, Stevens identified 4 types of scales: *nominal*, *ordinal*, *interval*, and *ratio* [see also @michell1986; @velleman1993].

```{r scales}
#| echo: false

tab <- data.frame(
  v1 = c("Nominal", "Ordinal", "Interval", "Ratio"),
  v2 = c("Equivalence relation",
         "Total order",
         "Lebesque measure",
         ""),
  v3 = c("Are $x$ and $y$ the same?", 
         "Is $x$ is greater than $y$?", 
         "How far is $x$ from $y$?",
         "How many times is $x$ greater than $y$?"),
  v4 = c("$x' = f(x)$, where $f$ is a bijection", 
         "$x' = f(x)$, where $f$ is a monotonic bijection", 
         "$x' = ax + b$, for $a, b$ real", 
         "$x' = ax$, for $a$ real")
)

col_names <- c("Scale", 
               "Structure",
               "Comparison", 
               "Valid transformations")
knitr::kable(tab, col.names = col_names, 
             caption = "Types of scales identified by Stevens (1946)")

```

Table \@ref(tab:scales) shows a loose reproduction of Table 1 from @stevens1946. Note that the family of valid transformations gets smaller in each row, meaning that the scales carry more information [@velleman1993]. Let's discuss the scales individually.    

##### Nominal scales

*Nominal* scales correspond to equivalence relations. An equivalence relation is a binary relation $\sim$ on some set $X$ which, for all $x, y, z \in X$, has the following properties:

- *Reflexivity*: $x \sim x$
- *Symmetry*: $x \sim y \text{ if and only if } y \sim x$
- *Transitivity*: $x \sim y \text{ and } y \sim z \text{ then } x \sim z$

Intuitively, we can think of the numbers on a nominal scale as "labels". Thus, the only question which we can ask a nominal scale is whether two labels are the same or different. Examples of variables with nominal scale include variables of which we typically think of as categorical, such as color, species, or political party. It does not make sense to say "blue is *more* than green" or "cat is *more* than dog" without specifying some other axis of comparison. It does make sense, however, to say "Daisy and Molly are the same species of animal (cat)" or "these two glasses are of different colors".

For nominal scales, any permutation is a valid transformation [@stevens1946]. For example, if we use the numbers $\{ 1, 2, 3 \}$ to represent the species $S = \{ \text{cat}, \text{dog}, \text{hamster} \}$, respectively, we can re-assign the numbers in any order we want and the properties of the scale are preserved.   

It is arguable whether any nominal quantities exist in and of themselves or whether they only ever exists as abstract social constructions over underlying continuous reality. Color is a discretization of the visible light spectrum (frequency of electromagnetic radiation), and the pre-Darwinian concept of a species is likewise an abstraction over continuously varying distribution of genes [although there have been some attempts to ground the definition of a discrete species in the theory of genetics, e.g. as a population of individuals which can produce viable offsprings, see @mayr1999]. Further, even many subjective concepts which are typically described as discrete such as emotions may be abstractions over underlying continuous phenomena [@barrett2013]. 

However, even if nominal quantities are entirely socially constructed, this does not mean they are arbitrary or useless. SEARLE     

##### Ordinal scales

*Ordinal* scales correspond to total orders. A total order is a relation $\leq$ on $X$ which, for all $x, y, z \in X$, has the following properties:

1) *Reflexivity*: $x \leq x$
2) *Antisymmetry*: $\text{if } x \leq y \leq x \text{ then } x = y$
3) *Transitivity*: $x \leq y \text{ and } y \leq z \text{ then } x \leq z$
4) *Totality* or *comparability* or *strong connectedness*: $\text{for all } x, y, \text{ either } x \leq y \text{ or } y \leq x$

Examples of total orders include the usual ordering $\leq$ on natural numbers $\mathbb{N}$: $1 \leq 2 \leq 3 \leq \ldots$ or the alphabetical order on letters: $A \leq B \leq C \ldots \leq Z$.  

As total orders, ordinal scales allow us to rank quantities. A good example of an ordinal variable is placement in a race or competition. If Emma and Charlotte ran a marathon, and Emma placed 2nd and Charlotte 3rd, we can say that Charlotte ran finished the race earlier than Emma. However, we do not know whether she crossed the finish line 15 minutes or 2 hours earlier, or whether or not her average pace was less than half of that of Emma. 

Some authors have related ordinal scales to *weak orders* [see @michell1986]. Weak orders [also known as *total preorders*, see @nlab2024d] generalize total orders by allowing for ties (properties 2. and 4. above do not need hold). While this seems like a useful property, I opted to relate ordinal scales to total orders here instead, since there is currently ambuiguity in the way the term *weak order* is used in the literature [see e.g. @nlab2024a; @nlab2024b; @stackexchange2024], and, for practical data analysis, the distinction is fairly inconsequential. For instance, in the marathon example above, if Emma and Charlotte both placed second, after Lucy and before Lily, we could frame the outcome of the race as the following weak order on the set $M$ of marathoners: $\text{Lucy} \leq \text{Emma, Charlotte} \leq \text{Lily} \leq ...$. However, the underlying set of ranks $R \subset \mathbb{N}$ still retains a total ordering: $1 \leq 2 \leq 3 \leq \ldots$ and we can specify a surjective monotonically increasing function $r: M \to R$ which maps each marathoner to her rank. Clearly, we can map any weak order to a total order by applying a functor which enforces anti-symmetry in this way [@fong2019; @nlab2024d].

The only transformations which are permissible for ordinal scales are those which preserve order, that is, monotonic increasing transformations [@stevens1946; @michell1986]. For example, transforming our set of ranks $R$ by taking the log or square root of each rank leaves the order relations between them unchanged.  

##### Interval scales

*Interval* scales. 

Interval scales allow us to calculate a distance between two points. However, they do not have a natural "zero point" or intercept. As such we cannot use them to determine the ratio between two quantities. Examples of interval scales include the calendar date and geographical position. It does not make sense to say that the year 1000 CE is "twice as much" as 500 CE, since the birth of Jesus Christ is (religious beliefs aside) an arbitrary zero point: we could set 0 CE at any other point in time, such as the founding of Athens or the release of Taylor Swift's first album, and all of the properties we care about when tracking historical time would be preserved. Likewise, it does not make sense to say that 90° longitude is "three times" that of 30° longitude: the location of the prime meridian is also the product of arbitrary historical cirumstances.  

###### Ratio scales

Unlike interval scales, ratio scale have a well-defined natural zero point. For example, 

##### Criticism of On the Theory of Scales of Measurement

In the original paper, Stevens had also made the claim that the type of scale determined which statistical tests and summaries were "permissible" for the data. For example, according to Stevens, while mean is an appropriate summary of an interval scale (since expectation is linear), it would not be a permissible summary of ordinal data. This claim was later disputed by researchers      

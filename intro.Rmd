---
output: html_document
editor_options: 
  chunk_output_type: console
---

# Introduction

## Defining Interactivity

> If it looks like a duck, swims like a duck, and quacks like a duck, then it probably is a duck.
> 
> [...] The irony is that while the phrase is often cited as proof of abductive reasoning, it is not proof, as the mechanical duck is still not a living duck
>
> [Duck Test](https://en.wikipedia.org/wiki/Duck_test) entry, [@wikipedia2022] 

What is an interactive data visualization? Surprisingly, despite the widespread popularity of interactive visualizations, if you ask data visualization researchers, you may get many different and even incompatible answers. This lack of a clear consensus makes it challenging to discuss existing work without some clarification. Let's first survey the available literature and then try to define what the "interactive" and "interactivity" mean within the scope of the present thesis.

### Interactive Visualization vs. Interacting with a Visualization

First, when we say "interactive data visualization", are we using the term as a noun, referring to a concrete figure or chart, or are we using the term as a verb, referring to the practice of visualization? This "overloading" of the term, as described by @pike2009, creates confusion in the field, reflected in the available literature [see also @yi2007]. One can find papers which discuss concrete implementations of data visualization systems and the relevant mathematical theory, with little to no mention of the underlying psychology of the user. Equally likely, one can find papers which delve deep into the cognitive and human-computer interaction aspects of visualization, without referring to any concrete implementations. This points to an important fact about interactive data visualization: rather than being a singular field, it is in fact an intersection of several different fields, including statistics, computer science, cognitive psychology, and human-computer interaction. The interdisciplinary nature of the topic highlights the need to use clear and specific definitions.     

While I do plan to discuss some fundamental aspects of the psychology of interacting with visualizations, when used in this thesis, the term *"interactive data visualization"* will refer to concrete charts or figures, typically displayed on a computer screen. When referring to the practice of interactive data visualization, I will try to use more active terms such as *"interacting with a visualization"*, to indicate that the term is meant to be interpreted as a verb and not a noun.

### Features of Interactivity

But even when we focus on interactive data visualizations as charts or figures, there are still considerable differences in what different researchers consider the bar for "interactivity". To some, almost any user manipulation qualifies [@brodbeck2009]. Others emphasise the speed of response, with faster updates translating to greater interactivity [@becker1987; @buja1996]. Complicating matters further, some researchers make the distinction between "interactive" and "dynamic" manipulation. According to these researchers, interactive manipulation involves discrete actions such as pressing a button or selecting an item from a drop-down menu, whereas dynamic manipulation involves continuous actions, like moving a slider or clicking-and-dragging to highlight a rectangular area [@rheingans2002; @jankun2007]. 

Yet, for other researchers, the two definitions outlined above (the user can do something and changes propagate fast enough) are far too broad. For many, true interactivity hinges on high-level features, such as the ability to query different parts of the dataset (by e.g. zooming, panning, and filtering), and reactive propagation of changes between connected or "linked" parts of a figure [@kehrer2012; @buja1996; @keim2002; @unwin1999]. Similarly, in Visual Analytics (VA) research, a distinction is made between "surface-level" (or "low-level") and "parametric" (or "high-level") interactions, where surface-level interactions manipulate attributes of the visual domain only (e.g. zooming and panning), whereas parametric interactions manipulate attributes of mathematical models or algorithms underlying the visualization [@leman2013; @pike2009]. 

In Table \@ref(tab:definitions), I have summarized the several important definitions of interactivity, as discussed above. This list is in no way supposed to be exhaustive [more complete taxonomies of interactive visualizations have been described before, see @yi2007toward]. Instead, it serves as a jumping off point for further discussion.

```{r definitions}
#| echo: false

library(kableExtra)

tab <- data.frame(
  type = c("User Interaction", 
           "Real-Time Updates", 
           "Filtering/Subsetting",
           "Linked Views",
           "Parametric Updates"),
  # short_definition = c("Change happens", 
  #                      "Change happens in real time", 
  #                      "Change affects what is seen", 
  #                      "Change propagates",
  #                      "Change affects parameters"),
  details = c("The user can interact with the visualization in some way",
              "The user's interactions propagate to the visualization with little to no lag",
              "The user can interactively explore different parts of the data set by effectively 'subsetting' rows (e.g. zooming, panning, and filtering)",
              "Parts of the figure are 'linked' such that the user's interactions with one part propagate into the other parts (e.g. linked brushing)",
              "The user can manipulate the parameters of some underlying mathematical model or algorithm")
)

colnames(tab) <- c("Name", "Details")

knitr::kable(tab, caption = "Definitions of Interactivity") |> kable_styling(full_width = FALSE)
# odd_rows <- (1:nrow(tab))[1:nrow(tab) %% 2 == 1]
# 
# flextable(tab) |>
#   theme_booktabs(bold_header = TRUE) |>
#   set_table_properties(width = 1, layout = "autofit") |>
#   bg(i = odd_rows, bg = "grey95") |>
#   border_inner_h(border = fp_border_default(color = "grey80"))

```

The different definitions of interactivity imply radically different implementation requirements. For example, under the broad "user interaction" definition, it could be argued that even running code from the command line could be considered "interactive visualization". Further, these interactions may be purely visual and require little to no data wrangling: for example, changing color or size of points in a scatterplot can be done without reference to the original data. In contrast, features such as filtering, linking, or parametric interaction, cannot be implemented without specialized data structures that handle updates. These interactions extend beyond the purely visual realm and require the system to be able to refer back to the original data in order to recompute new summaries.  

## Mathematical Theory

I argue that applications of category theory to visualization can be classified in one of two ways: in a more "abstract" way, with authors trying to lay down theoretical foundations for what it means to "visualize" [@beckmann1995] or which visualizations can be considered well-formed perceptual representations of the data [@kindlmann2014], and a more "applied" way, with authors introducing libraries and domain-specific languages which allow for constructing plots and figures in a compositional, functional-programming style [see e.g. @yorgey2012; @petricek2021; @smeltzer2014; @smeltzer2018]. The present thesis attempts to do neither. Instead, the goal is to focus on a much more narrow area of data visualization: the computation of statistics on data.

Category theory is an area of mathematics concerned with the ideas of structure, relation, and composition. It may seem at first that such an abstract area of mathematics would be far removed from the applied field of interactive data visualization. However, this thesis is aims to demonstrate that category theory is in fact deeply relevant to the production of meaningful interactive graphics. 

Moreover, the idea of using category theory in an applied discipline such as graphics is not unprecedented. In recent decades, category theory has been making inroads into other applied disciplines, including computer science, physics, biology, and epidemiology [@baez2023; @fong2019], under the guise of "applied category theory". 

The following chapter gives an overview of some concepts from applied category theory which may be relevant to the production of interactive graphics. It follows mainly from @fong2019, @lawvere2009, @baez2023, and @milewski2018.

### Relations

Relations are some of the simplest mathematical structures. Given two sets $X$ and $Y$, a relation $R$ between $X$ and $Y$ is a subset of the Cartesian product of the two sets, $R \subseteq X \times Y$. In other words, a relation can be thought of as the set of $(x, y)$ pairs $\in X \times Y$ for which the condition that "$x$ and $y$ relate" is true. Note that $X$ and $Y$ can also be the same set, such that $R \subseteq X \times X$.

Since a relation is a subset of the product set $X \times Y$, we can display it as a matrix with values of $X$ as rows and values of $Y$ as columns and the pairs $(x, y)$ which relate marked in some specific way. For example, here's how we can display the usual order relation $\leq$ on the set $X = \{ 1, 2, 3 \}$:

```{r relations-subset}
#| echo: false
#| dpi: 300
#| fig-cap: "Relations as a subset of the Cartesian product on two sets.
#| From the diagram, we see that 1 is less than or equal to every element of y, 
#| 2 is less than or equal to 2 and 3, and 3 is less than or equal to 3 only.
#| Note the symmetry between rows and columns - this is due to the fact 
#| that we have the same set ($X$) on both dimensions. 
#| " 

library(grid)
library(gridExtra)

at <- seq(0.25, 0.75, length = 3)
is_less <- outer(1:3, 1:3, `<=`)

grid.newpage()
grid.roundrect(width = 0.75, height = 0.75, r = unit(0.05, "npc"),
               gp = gpar(fill = "antiquewhite", col = "antiquewhite"))
grid.text("X", y = 0.05, gp = gpar(fontsize = 16, col = "burlywood4"))
grid.text("X", x = 0.075, gp = gpar(fontsize = 16, col = "burlywood4"))
grid.text(c(1, 2, 3), x = at, y = 0.1, gp = gpar(col = "grey70"))
grid.text(c(1, 2, 3), x = 0.11, y = at, gp = gpar(col = "grey70"))

grid.circle(x = rep(at, 3), y = rep(at, each = 3), r = 0.075, gp = gpar(col = FALSE))
grid.circle(x = rep(at, each = 3)[t(is_less)], 
            y = 1 - rep(at, 3)[t(is_less)], 
            r = 0.075, gp = gpar(fill = "indianred", col = FALSE))
```

Under some relation $R$, if two elements $x, y \in X$ relate, i.e. if $(x, y) \in R$, we can write this using the infix notation using some infix symbol such as $\star$: then, if $x$ and $y$ relate, we write $x \star_R y$ or $x \star y$ ($R$ implicit). For example, for common relations such as $=$, $\leq$, we write $x = y$ or $x \leq y$. $R$ is also sometimes used, e.g. $x R y$. If the elements do not relate, i.e. $(x, y) \not \in R$, we typically do not write this out explicitly.   

Relations can have properties. For example, many types of relations are *reflexive*, meaning that $x \star x$ for all $x \in X$ (every element relates to itself). Of note, specifying three specific properties allow us to define equivalence relations:

::: {.definition name="Equivalence relation"}
A relation $\sim$ on $X$ is called an equivalence relation if it is:

1. *Reflexive*: $x \sim x$ for all $x \in X$
2. *Symmetric*: $x \sim y$ if and only if $y \sim x$ for all $x, y \in X$
3. *Transitive*: if $x \sim y$ and $y \sim z$, then $x \sim z$
:::

Equivalence relations encode the notion that two things are *same-ish*. We can further use them to assign objects in $X$ to *equivalence classes*, which group equivalent objects together. That is, for some element $a \in X$, its corresponding equivalence class is:

$$[a] = \{ x \in X : x \sim a \}$$
We can do a lot of things with relations. The next few sections will discuss three important examples: functions, partitions, and preorders. 

### Functions

A function is a special kind of relation that encodes a mapping between two sets. More specifically, let $S$ be the set of sources (also called the *domain*) and $T$ be the set of possible targets (also called the *codomain*). Then, we can think of a function as a relation $F \subseteq S \times T$ of valid source-target pairs $(s, t)$, such that for every $s \in S$ in there exists a unique $t \in T$ with $(s, t) \in F$ (see Figure \@ref(fig:function-subset)). In other words, every source relates to exactly one target, see Figure \@ref(fig:function-subset):

```{r function-subset}
#| echo: false
#| dpi: 300
#| fig-cap: "Function as a subset of the Cartesian product on domain ($S$) and codomain ($T$). 
#| The subset $F \\subseteq S \\times T$ is shown in red. 
#| In words, the function is $F: \\{ 1, 2, 3 \\} \\to \\{ 1, 2, 3 \\}$,
#| such that $F(1) = 1$, $F(2) = 1$, and $F(3) = 2$, e.g. $f(x) = \\lfloor x / 2 \\rceil$ (divide by two and round). Note that, for any function, there must be exactly one red dot in each column (each source maps to one and only one target), however, there may be zero or many red dots in any row (some target may not be reachable, or they may be reachable from multiple sources)."

library(grid)
library(gridExtra)

at <- seq(0.25, 0.75, length = 3)
positions <- c(1, 2, 6)

grid.newpage()
grid.roundrect(width = 0.75, height = 0.75, r = unit(0.05, "npc"),
               gp = gpar(fill = "antiquewhite", col = "antiquewhite"))
grid.text("S", y = 0.05, gp = gpar(fontsize = 16, col = "burlywood4"))
grid.text("T", x = 0.075, gp = gpar(fontsize = 16, col = "burlywood4"))
grid.text(c(1, 2, 3), x = at, y = 0.1, gp = gpar(col = "grey70"))
grid.text(c(1, 2, 3), x = 0.11, y = at, gp = gpar(col = "grey70"))

grid.circle(x = rep(at, 3), y = rep(at, each = 3), r = 0.075, gp = gpar(col = FALSE))
grid.circle(x = rep(at, 3)[positions], y = rep(at, each = 3)[positions], 
            r = 0.075, gp = gpar(fill = "indianred", col = FALSE))

```

We can classify functions based on how their domains and codomains map onto each other (see Figure \@ref(fig:function-types)). If every target in the function's codomain has a path leading to it from some source, such that no target remains unreachable, then we call it a *surjective* or *onto* function. More formally:


::: {.definition name="Surjectivity"}
A function $f$ is surjective if, for all $t \in T$, there exists a $s \in S$ such that $f(s) = t$. 
:::


Alternatively, if each source in the function's domain leads to a unique target, then we call such a function *injective* or *one-to-one*. That is: 

::: {.definition name="Injectivity"}
A function is injective if for all $s_1, s_2 \in S$, if $f(s_1) = t$ and $f(s_2) = t$, then $s_1 = s_2$. 
:::

Finally, if a function is both surjective and injective, meaning that every target can be reached from, and only from, a unique source, then we call such a function *bijective* or a *bijection*.

::: {.definition name="Bijectivity"}
A function is a bijection and only if it is both surjective and injective, and if and only if it is invertible. 
:::

```{r function-types}
#| echo: false
#| dpi: 300
#| fig-cap: "Types of functions. 
#| Left: in a *surjective* function, every target can be reached from some source. 
#| Middle: in an *injective* function, every target can only be reached from a unique source.
#| Right: in a *bijection*, every target can be reached from, and only from, a unique source."

library(grid)
library(gridExtra)

domain_pts1 <- seq(0.25, 0.75, length = 6)
codomain_pts1 <- seq(0.35, 0.65, length = 4)
mapping1 <- c(1, 2, 1, 3, 4, 4)

codomain_pts2 <- domain_pts1
domain_pts2 <- codomain_pts2[-c(1, length(codomain_pts2))]

grid.newpage()
pushViewport(viewport(x = 1/6, width = 1/3))
grid.text("Surjective", y = 0.9)
grid.roundrect(x = 0.5, y = 0.75,
               width = 0.75, height = 0.15, r = unit(0.1, "npc"),
               gp = gpar(fill = "antiquewhite", col = FALSE))
grid.roundrect(x = 0.5, y = 0.25,
               width = 0.75, height = 0.15, r = unit(0.1, "npc"),
               gp = gpar(fill = "antiquewhite", col = FALSE))

grid.circle(domain_pts1, 0.75, r = 0.035,
            gp = gpar(fill = "indianred", col = FALSE))
grid.circle(codomain_pts1, 0.25, r = 0.035,
            gp = gpar(fill = "indianred", col = FALSE))

grid.text(c("S", "T"), x = 0.175, y = c(0.75, 0.25) + 0.045,
          gp = gpar(col = "burlywood4"))

grid.segments(domain_pts1, 0.75, codomain_pts1[mapping1], 0.25,
              arrow = arrow(length = unit(0.025, "npc")),
              gp = gpar(col = "steelblue"))

popViewport()
pushViewport(viewport(x = 3/6, width = 1/3))
grid.text("Injective", y = 0.9)
grid.roundrect(x = 0.5, y = 0.75,
               width = 0.75, height = 0.15, r = unit(0.1, "npc"),
               gp = gpar(fill = "antiquewhite", col = FALSE))
grid.roundrect(x = 0.5, y = 0.25,
               width = 0.75, height = 0.15, r = unit(0.1, "npc"),
               gp = gpar(fill = "antiquewhite", col = FALSE))

grid.circle(domain_pts2, 0.75, r = 0.035,
            gp = gpar(fill = "indianred", col = FALSE))
grid.circle(codomain_pts2, 0.25, r = 0.035,
            gp = gpar(fill = "indianred", col = FALSE))

grid.text(c("S", "T"), x = 0.175, y = c(0.75, 0.25) + 0.045,
          gp = gpar(col = "burlywood4"))

grid.segments(domain_pts2, 0.75, domain_pts2[c(2, 3, 1, 4)], 0.25,
              arrow = arrow(length = unit(0.025, "npc")),
              gp = gpar(col = "steelblue"))

popViewport()
pushViewport(viewport(x = 5/6, width = 1/3))
grid.text("Bijective", y = 0.9)
grid.roundrect(x = 0.5, y = 0.75,
               width = 0.75, height = 0.15, r = unit(0.1, "npc"),
               gp = gpar(fill = "antiquewhite", col = FALSE))
grid.roundrect(x = 0.5, y = 0.25,
               width = 0.75, height = 0.15, r = unit(0.1, "npc"),
               gp = gpar(fill = "antiquewhite", col = FALSE))

grid.circle(domain_pts1, 0.75, r = 0.035,
            gp = gpar(fill = "indianred", col = FALSE))
grid.circle(domain_pts1, 0.25, r = 0.035,
            gp = gpar(fill = "indianred", col = FALSE))

grid.text(c("S", "T"), x = 0.175, y = c(0.75, 0.25) + 0.045,
          gp = gpar(col = "burlywood4"))

grid.segments(domain_pts1, 0.75, sample(domain_pts1), 0.25,
              arrow = arrow(length = unit(0.025, "npc")),
              gp = gpar(col = "steelblue"))

```


Bijections are special since they encode the idea of *reversibility* or *lossless transformation*. Any bijective function $f$ has an associated inverse function $f^{-1}$ such that $f^{-1}(f(x)) = x$ and $f(f^{-1}(a)) = a$ for all $x$ and $a$ in the function's domain and codomain, respectively, and we can keep switching back and forth from domain to codomain and back without losing any information. We'll generalize this idea later when we discuss *isomorphisms*.

As an example, suppose I have a group of friends $f \in F$ that each went to one city $c \in C$ in Europe during the holiday. I can construct a function $f: F \to C$ that sends each friend to his or her holiday destination. If every city in $C$ was visited by at least one friend, then the function is surjective. If each friend went to a different destination, then the function is injective. If both are true - that is, if every city on our list was visited by exactly one friend - then the function is bijective. In that case, we could just as well use the names of cities $c \in C$ when we speak of friends $f \in F$ - instead of "Sam", we could say "the person who went to Rome", and it would be clear who are we talking about.

An important property of functions is that they can be composed. Specifically, if the domain of one function matches the codomain of another, the functions can be composed by piping the output of one into another to form a new function: 

::: {.definition name="Function composition"}
If we have two functions $f: X \to Y$ and $g: Y \to Z$, we can form a new function $h: X \to Z$ such that:

$$h(x) = g(f(x))$$
We can omit the explicit reference to the variable $x$ and write the composition in several different ways:

1. $h = g \circ f$ (read: "apply $g$ after $f$")
2. $h = gf$ (same as above)
3. $h = f ⨾ g$ (read "apply $f$ then $g$")
:::

I will use the bracket notation ($h(x) = g(f(x))$) when explicitly referring to the variable, and the postfix/fat semicolon notation ($h = f ⨾ g$) otherwise.   

There are other things we can do with functions. For example, given a subset of sources, we can ask about the *image* - the set of targets we can reach from those sources:

::: {.definition name="Image"}
For some subset $S_i \subseteq S$, its image under $f$ is defined as $f_!(S_i) = \{ f(s) \in T \lvert s \in S_i \}$. 
:::

Likewise, given a subset of targets, we can ask about the *pre-image* - the set of sources that could have produced those targets. That is:

::: {.definition name="Pre-image"}
For some subset $T_i \subseteq T$, its pre-image under $f$ is defined as $f^*(T_i) = \{ s \in S \lvert f(s) \in T_i \}$. 
:::

An important fact to note is that, although the pre-image $f^*$ is also sometimes called "inverse image", it is *not* the inverse of the image $f_!$ for most functions (unless they are bijections). That is, by applying the pre-image after image or vice versa, we cannot expect to come up with the same set as we started with. Specifically, if we have a non-injective function and apply the pre-image after the image, we may come up with *more* sources that we started with, $S_i \subseteq f^*(f_!(S_i))$ (equality if injective), and similarly, if we have a non-surjective function and apply the image after the pre-image, we might end up with *fewer* targets than we started with, $f_!(f^*(T_i)) \subseteq T_i$ (again, equality if surjective). 

As an example, suppose again I have the function $f$ which maps each friend to a holiday destination. The image of that function, $f_!$, maps a set of friends to the set of all cities that at least one of them went to, and similarly, the pre-image, $f^*$, maps a set of cities to the set of friends that went to them.  
Now, suppose that Sam and Dominic went to Rome, and I ask:

> *"who went to [the city that Sam went to]?"*

I will get both Sam and Dominic back, since:

$$f^*(f_!(\{ Sam \})) = f^*(\{ Rome \}) = \{ Sam, Dominic \}$$

That is, I will get back Sam and Dominic *even though I had initially only asked about Sam*. Similarly, if no friends had visited Paris and I ask:

> *"what are the cities that [people who went to Paris or Rome] went to?"*

then I will get Rome only, since 

$$f_!(f^*(\{Paris, Rome \})) = f_!(\{ Sam, Dominic \}) = \{ Rome \}$$

This weird relationship between the the image and the pre-image is due to the fact that the image is actually something called *left adjoint* [@baez2023; @fong2019]. Adjoints can be thought of as the "best approximate answer to a problem that has no solution" [no inverse, @baez2023], and they come in pairs - a left and a right adjoint - with the left adjoint being more permissive or "liberal" and the right adjoint being more strict or "conservative" [@baez2023].

Proper treatment of adjoint is beyond the scope of this thesis, however.

### Partitions

One useful thing we can construct with functions (or equivalently, relations) are partitions. Partitions encode the idea of splitting elements of some some into distinct groups.  

::: {.definition name="Function definition of a partition"}
Given some set $X$, a set of part labels $P$, and a surjective function $f: X \to P$, we can partition $A$ by assigning every element $x \in X$ a part label $p \in P$, by simply applying the function: $f(x) = p$.
:::

Above we used a function to define a partition, however, we can achieve the same with a relations, specifically equivalence classes. By taking any part label $p \in P$, we can recover the corresponding subset of $X$ by pulling out its pre-image: $f^*(\{p\}) = X_p \subseteq X$. We can then define a partition without reference to $f$:

::: {.definition name="Equivalence class definition of a partition"}
A partition of $A$ consists of a set of part labels $P$, such that, for all $p \in P$, there is a non-empty subset $A_p \subseteq A$ which forms an equivalence class on $A$ and:

$$X = \bigcup_{p \in P} X_p \qquad \text{and} \qquad \text{if } p \neq q, \text{ then } X_p \cap X_q = \varnothing$$
I.e. the parts $X_p$ jointly cover the entirety of $X$ and parts cannot share any elements.
:::

We can rank partitions by their coarseness. That is, for any set $X$, the coarsest partition is one with only one part label $P = \{ 1 \}$, such that each element of $X$ gets assigned $1$ as label. Conversely, the finest partition is one where each element gets assigned its own unique part label, such that $\lvert X \lvert = \lvert P \lvert$. 

Given two partitions, we can form a finer (or at least as fine) partition by taking their intersection, i.e. by taking the set of all unique pairs of labels that co-occur for any $x \in X$ as the new part labels. For example, suppose $X = \{ 1, 2, 3 \}$ and partition 1 assigns part labels:

$$p_1(x) = \begin{cases} 
a & \text{if } x = 1 \text{ or } x = 2 \\
b & \text{if } x = 3
\end{cases}$$

and partition 2 assigns part labels the following way:

$$
p_2(a) = \begin{cases}
s & \text{if } x = 1 \\
t & \text{if } x = 2 \text{ or } x = 3
\end{cases}
$$

Then the intersection partition will have the following part labels $P_3 = \{ (a, s), (a, t), (b, t) \}$ such that:

$$
p_3(a) = \begin{cases}
(a, s) & \text{if } x = 1 \\
(b, s) & \text{if } x = 2 \\ 
(b, t) & \text{if } x = 3
\end{cases}
$$

### Preorders

::: {.definition name="Preorder"}
A preorder is a set $X$ equipped with a binary relation $\leq$ that conforms to two simple properties:

1. *Reflexivity*: $x \leq x$ for all $x \in X$
2. *Transitivity*: if $x \leq y$ and $y \leq z$, then $x \leq z$, for all $x, y, z \in X$
:::

Simply speaking, this means that between any two elements in $X$, there either is a relation and the elements relate (one element is somehow "less than or equal" to the other), or the two elements do not relate. 

An example of a preorder is the family tree, with the underlying set being the set of family members: $X = \{  \textbf{daughter}, \textbf{son}, \textbf{mother}, \textbf{father}, \textbf{grandmother}, ... \}$ and the binary relation being ancestry or familial relation. Thus, for example, $\textbf{daughter} \leq \textbf{father}$, since the daughter is related to the father, and $\textbf{father} \leq \textbf{father}$, since a person is related to themselves. However, there is no relation ($\leq$) between $\textbf{father}$ and $\textbf{mother}$ since they are not related. Finally, since $\textbf{daughter} \leq \textbf{father}$ and $\textbf{father} \leq \textbf{grandmother}$, then, by reflexivity, $\textbf{daughter} \leq \textbf{grandmother}$.

We can further restrict preorders by imposing additional properties, such as:

3. If $x \leq y$ and $y \leq x$, then $x = y$ (anti-symmetry)
4. Either $x \leq y$ or $y \leq x$ (comparability)

If a preorder conforms to 3., we speak of a partially ordered set or *poset*. If it conforms to both 3. and 4., then it is a *total order*. 

### Monoids

A monoid is a tuple $(M, e, \otimes)$ consisting of:

a. A set of objects $M$
b. A neutral element $e$ called the *monoidal unit*
c. A binary function $\otimes: M \times M \to M$ called the *monoidal product*

Such that:

1. $m \otimes e = e \otimes m = m$ for all $m \in M$ (unitality)
2. $m_1 \otimes (m_2 \otimes m_3) = (m_1 \otimes m_2) \otimes m_3 = m_1 \otimes m_2 \otimes m_3$ for all $m_1, m_2, m_3 \in M$ (associativity)

In simple terms, monoids encapsulate the idea that *the whole is exactly the "sum" of its parts* (where "sum" can be replaced by the monoidal product). Specifically, we have some elements and a way to combine them, and when we combine the same elements, no matter where we put the brackets we always get the same result (i.e. something like "the order does not matter", although that is not precisely right, more on that later). Finally, we have some neutral element that when combined with an element yields back the same element.  

For example, take summation on natural numbers, $(\mathbb{N}, 0, +)$:

$$1 + 0 = 0 + 1 = 1 \qquad \text{(unitality)}$$
$$1 + (2 + 3) = (1 + 2) + 3 = 1 + 2 + 3 \qquad \text{(associativity)}$$

Likewise, products of real numbers $(\mathbb{R}, 1, \times)$ are also a monoid, and so is multiplication of $n \times n$ square matrices $(\mathbf{M}_{n \in \mathbb{Z}}, \mathbf{I}, \cdot)$, where $\mathbf{I}$ is the identity matrix and $\cdot$ stands for an infix operator that is usually omitted. As a counterexample, exponentiation does not meet the definition of a monoid, since it is not associative: $x^{(y^z)} \neq (x^y)^z$. 

We may want to impose further restrictions on monoids, for example:

3. $m_1 \otimes m_2 = m_2 \otimes m_1$ for all $m_1, m_2 \in M$ (commutativity)

Both commutativity and associativity can both be viewed as a kind of "order does not matter" rule, however, they are fundamentally different. Let's imagine our set of objects consists of three wires of different colours $\{ \textbf{red}, \textbf{green}, \textbf{blue} \}$ and the monoidal product consists of connecting wires. Let's also imagine that the $\textbf{red}$ wire is connected to a power source and the $\textbf{blue}$ wire is connected to a lightbulb, and the blue wire amplifies the current from the power source such that it is enough to power the light bulb. To turn on the lightbulb, we need to connect $\textbf{red} \to \textbf{green}$ and $\textbf{green} \to \textbf{blue}$. The time order in which we connect the three wires does not matter: we can connect $\textbf{green} \to \textbf{blue}$ first and $\textbf{red} \to \textbf{green}$ second or vice versa, either way we get the same result (lightbulb turns on). However, the spatial order in which we connect the wires *does* matter: if we connect $\textbf{red} \to \textbf{blue}$, then the current will not be enough to power the lightbulb. Hence, the operation is associative (temporal order does not matter) but not commutative (spatial order does matter).      

If $M$ is a preorder, another restriction we may want to impose is that the monoidal product is strictly increasing:

4. $m_1 \leq m_1 \otimes m_2$ and $m_2 \leq m_1 \otimes m_2$ for all $m_1, m_2 \in M$ (monotonicity)

This means that when we combine two things, we get back something that's at least as big as the bigger of the two things. Summation of natural numbers $(\mathbb{N}, 0, +)$ again works, but for example summation of integers $(\mathbb{Z}, 0, +)$ or multiplication of reals $(\mathbb{R}, 1, \times)$ does not. 

## Components of a Data Visualization System

### Scales

Every data visualization system needs some way of translating abstract data values into concrete graphical attributes such as position, size, or colour. Given the ubiquitous need for scales, one might expect them to be a "solved issue", within the relevant literature. However, this is far from the truth. Specifically, the issues of scales and measurement present are still being grappled with the areas of mathematics and philosophy of science to this day [for a gentle yet thorough introduction, see @tal2015].

Scales are another area of data visualization in which there has been considerable debate, with many terms being overloaded and relating to concepts from different fields. This is due to the fact that the issue of how to compare, rank, and translate values has a long and complicated history. In particular, the issue of measurement has been hotly debated in the field of psychometrics and mathematical psychology, leading to the development of the *theory of measurement* (which has some overlap with, but is not the same as, *measurement theory* in mathematics). 

One paper that has been key to the debate around scales and measurement has been the seminal work of @stevens1946. In this paper, Stevens defined a *scale* as a method of assigning numbers to values, allowing for various kinds of comparisons. Further, by considering transformations which preserve the comparisons, Stevens identified 4 types of scales: *nominal*, *ordinal*, *interval*, and *ratio* [see also @michell1986; @velleman1993].

```{r scales}
#| echo: false

tab <- data.frame(
  v1 = c("Nominal", "Ordinal", "Interval", "Ratio"),
  v2 = c("Equivalence relation",
         "Total order",
         "Lebesque measure",
         ""),
  v3 = c("Are $x$ and $y$ the same?", 
         "Is $x$ is greater than $y$?", 
         "Is the distance from $x$ to $y$ the same as from $a$ to $b$?",
         "How many times is $x$ greater than $y$?"),
  v4 = c("$x' = f(x)$ where $f$ is a bijection", 
         "$x' = f(x)$ where $f$ is a monotonically increasing bijection", 
         "$x' = ax + b$", 
         "$x' = ax$")
)

col_names <- c("Scale", 
               "Structure",
               "Comparison", 
               "Valid transformations")
knitr::kable(tab, col.names = col_names, 
             caption = "Types of scales identified by Stevens (1946)")

```

Table \@ref(tab:scales) shows a loose reproduction of Table 1 from @stevens1946. Note that the family of valid transformations gets smaller in each row, meaning that the scales carry more information [@velleman1993]. Let's discuss the scales individually.    

##### Nominal scales

*Nominal* scales correspond to equivalence relations. An equivalence relation is a binary relation $\sim$ on some set $X$ which, for all $x, y, z \in X$, has the following properties:

- *Reflexivity*: $x \sim x$
- *Symmetry*: $x \sim y \text{ if and only if } y \sim x$
- *Transitivity*: $x \sim y \text{ and } y \sim z \text{ then } x \sim z$

Intuitively, we can think of the numbers on a nominal scale as "labels", and the only question which we can ask a nominal scale is whether two labels are the same or different. A such, examples of variables with nominal scale include variables of which we typically think of as categorical, such as color, species, or political party. It does not make sense to say "blue is *more* than green" or "cat is *more* than dog" without specifying some other axis along which we compare. It does make sense, however, to say "Daisy and Molly are the same species of animal (cat)" or "these two glasses are of different colors".

The only transformations which are permissible for nominal scales are permutations [@stevens1946]. For example, if we use the numbers $\{ 1, 2, 3 \}$ to represent the species $S = \{ \text{cat}, \text{dog}, \text{hamster} \}$, respectively, we can assign the numbers in any order we want and the properties of the scale are preserved.   

It is arguable whether any nominal quantities exist in and of themselves or whether they only ever exists as abstract social constructions over underlying continuous reality. Color is a discretization of the visible light spectrum (frequency of electromagnetic radiation), and the pre-Darwinian concept of a species is likewise an abstraction over continuously varying distribution of genes [although there have been some attempts to ground the definition of a discrete species in the theory of genetics, e.g. as a population of individuals which can produce viable offsprings, see @mayr1999]. Further, even many subjective concepts which are typically described as discrete such as emotions may be abstractions over underlying continuous phenomena [@barrett2013]. 

However, even if nominal quantities are entirely socially constructed, this does not mean they are arbitrary or useless. SEARLE     

##### Ordinal scales

*Ordinal* scales correspond to total orders. A total order is a relation $\leq$ on $X$ which, for all $x, y, z \in X$, has the following properties:

1) *Reflexivity*: $x \leq x$
2) *Antisymmetry*: $\text{if } x \leq y \leq x \text{ then } x = y$
3) *Transitivity*: $x \leq y \text{ and } y \leq z \text{ then } x \leq z$
4) *Totality* or *comparability* or *strong connectedness*: $\text{for all } x, y, \text{ either } x \leq y \text{ or } y \leq x$

Examples of total orders include the usual ordering $\leq$ on natural numbers $\mathbb{N}$: $1 \leq 2 \leq 3 \leq \ldots$ or the alphabetical order on letters: $A \leq B \leq C \ldots \leq Z$.  

As total orders, ordinal scales allow us to rank quantities. A good example of an ordinal variable is placement in a race or competition. If Emma and Charlotte ran a marathon, and Emma placed 2nd and Charlotte 3rd, we can say that Charlotte ran finished the race earlier than Emma. However, we do not know whether she crossed the finish line 15 minutes or 2 hours earlier, or whether or not her average pace was less than half of that of Emma. 

Some authors have related ordinal scales to *weak orders* [see @michell1986]. Weak orders [also known as *total preorders*, see @nlab2024d] generalize total orders by allowing for ties (properties 2. and 4. above do not need hold). While this could seem desirable, I opted to relate ordinal scales to total orders here instead, since there is currently some ambiguity in the literature in the way the term *weak order* is being used [see e.g. @nlab2024a; @nlab2024b; @stackexchange2024], and, for practical data analysis, the distinction is fairly inconsequential. For instance, in the marathon example above, if Emma and Charlotte both placed second, after Lucy and before Lily, we could frame the outcome of the race as the following weak order on the set $M$ of marathoners: $\text{Lucy} \leq \text{Emma, Charlotte} \leq \text{Lily} \leq ...$. However, the underlying set of ranks $R \subset \mathbb{N}$ still retains a total ordering: $1 \leq 2 \leq 3 \leq \ldots$ and we can specify a surjective monotonically increasing function $r: M \to R$ which maps each marathoner to her rank. Clearly, we can map any weak order to a total order by applying a functor which enforces anti-symmetry in this way [@fong2019; @nlab2024d].

The only transformations which are permissible for ordinal scales are those which preserve order, that is, monotonic increasing transformations [@stevens1946; @michell1986]. For example, transforming our set of ranks $R$ by taking the log or square root of each rank leaves the order relations between them unchanged.  

##### Interval scales

*Interval* scales. 

Interval scales allow us to identify a distance between two points. However, they do not have a natural "zero point" or intercept. As such we cannot use them to determine the ratio between two quantities. Examples of interval scales include the calendar date and geographical position. It does not make sense to say that the year 1000 CE is "twice" that of 500 CE, since the birth of Jesus Christ is (one's personal religious beliefs aside) an arbitrary zero point: we could set the point 0 CE to any other event, such as the founding of Athens or the release of Taylor Swift's first album, and the ratios would be altered. Likewise, it does not make sense to say that 90° longitude is "three times" that of 30° longitude: the location of the prime meridian is also the product of arbitrary historical cirumstances.  

###### Ratio scales

Unlike interval scales, ratio scale have a well-defined natural zero point. For example, 

##### Criticism of On the Theory of Scales of Measurement

In the original paper, Stevens had also made the claim that the type of scale determined which statistical tests and summaries were "permissible" for the data. For example, according to Stevens, while mean is an appropriate summary of an interval scale (since expectation is linear), it would not be a permissible summary of ordinal data. This claim was later disputed by researchers      

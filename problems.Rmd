---
output: html_document
editor_options: 
  chunk_output_type: console
---

# Problem Set

Designing an interactive data visualization system presents a unique set of challenges which need to be addressed. Some of these have been already mentioned in the [Introduction](#Introduction). This section discusses these inherent challenges in greater depth, and begins exploring avenues for possible solutions.

## Data representation

There is no data visualization without data. However, all data is not equal. Data can come to use in various shapes and sizes, and this can affect various aspects of the system, including design, memory, and performance.

In most data analytic languages, the default data model is that of two-dimensional table or dataframe. Examples include the S3 `data.frame` class in base R [@r2024], the `tbl_df` S3 class in the `tibble` package [@muller2023], the `DataFrame` class in the Python `pandas` package [@pandas2024], the `DataFrame` class in the `polars` library [@polars2024], or the `DataFrame` type in the Julia `DataFrame.jl` package [@bouchet-valat2023]. In this model, data is organized in columns, which are homogeneous arrays each storing values of the same type. Unlike in a matrix, the columns can be of different types (such as floats, integers, or strings). The dataframe object is then just a dictionary of columns, with some optional metadata, such as row names, column labels, or grouping structure [@r2024; @bouchet-valat2023].

However, this column-based organization of data is not universal. For example, in the popular JavaScript data visualization and transformation library D3 [@bostock2022] ascribes to a row-based data model, such that data is organized as an array of rows, with each row being its own separate dictionary. 

In a more general programming context, the column-based and row-based data layouts are also known as the struct of arrays (SoA) and array of structs (AoS) data structures, respectively. These data layouts have generally different performance characteristics, and hence why they are also studied in database design [see e.g. @abadi2013]. The SoA layout has (typically) smaller memory footprint and better performance in tight loops that operate on individual columns, thanks to cache locality [@abadi2013; @acton2014; @kelley2023]. The AoS layout can have arguably better developer ergonomics and can perform better when retrieving individual records/rows [hence why it is more common in traditional Online Transaction Processing databases, @abadi2013].    

## Data transformation

When visualizing data, it is rarely the case that we can just draw the raw data as is. Often, the quantities underlying a specific plot type are instead summaries or aggregates of some kind. Take for example a typical barplot. To draw a barplot, we first need to divide the data into disjoint parts, each corresponding to one bar, and then summarize each part by some metric, usually either the number of cases (count) or the sum of some continuous variable. Similarly, in a histogram, we first need to divide the data into bins and then summarize them (typically by count). Thus, there are two fundamental operations involved in every visualization: splitting the data into parts and computing the summaries on these parts. 

### Partitioning the data

#### Leave no data out

A common-sense guideline that many data visualization experts provide is that faithful visualizations should show the full data and leave nothing out. For instance, @cleveland1985 argues that axis limits should generally be expanded so that data points at or near these limits are not arbitrarily obscured. Take, for example, the following two scatterplots:

```{r}
#| echo: false
#| fig-cap: "Without expanding axes, data points near the limits can become obscured. Left: axis limits match the data limits exactly, and so points at or near the axis limits (top-left and bottom-right corner of the plot) are represented by smaller area and become less salient. Right: by expanding axis limits, we can ensure that all data is represented faithfully."

knitr::include_graphics("./figures/expand-scatterplot.png")
```

In the left plot, the axis limits match the data limits exactly, whereas in the right plot, they are expanded by a small fraction [5%, `ggplot2` default, @wickham2016]. The problem with the left plot is that the data points near the axis limits (top-left and bottom-right corner) are represented only by a fraction of the area: for example, the point in the bottom-right corner lies simultaneously at the limits of the x- and y-axis, and is thus represented only by 1/4 of the area of the points in the center of the plot.

The example above shows how data can be obscured visually, even after all of the data points have been included in the plot (rendering all rows of the data set). Clearly then, when complete data is available, leaving information out by arbitrarily dropping rows is even less acceptable. This issue becomes more complicated in the presence of missing or incomplete data, however, there exist ways of dealing with that as well, see e.g. @unwin1996, @tierney2023. Thus, ideally, the visualization should represent a surjective mapping from the space of the geometric objects to the underlying data set, such that, by default, there are no cases (rows of the data) which are marginalized or left out of the figure entirely [@ziemkiewicz2009].

#### Distinctness, disjointness, and comparison

> "To be truthful and revealing, data graphics must bear on the question at the heart of quantitative thinking: 'compared to what'?" [@tufte2001, pp. 74].

> "Graphics are for comparison - comparison of one kind or another - not for access to individual amounts." [@tukey1993]

In data visualization, a practice so ubiquitous that it is often overlooked is that, in most types of plots, each geometric object represents one disjoint part of the data. That is, each point, bar, line, or polygon typically represents a unique set of cases (rows of the data), with no overlap with the cases represented by the other objects. 

Why is this the case? This unconscious "law" might be rooted in the fundamental purpose of data visualization: comparison [@tufte2001; @tukey1993]. When we visualize, we draw our graphics with the ultimate goal of being able to compare our data along a set of visual channels, such as position, length, size, or colour [@bertin1983; @wilkinson2012; @franconeri2021; @wilke2019]. This mirrors the comparisons we make in the real world, where we compare physical objects along their respective dimensions or attributes.

With disjoint parts, there is a natural correspondence or bijection between the subsets of the data and their visual representation, see figure \@ref(fig:geoms-bijection). Specifically, if we imagine the act of taking an object and picking the corresponding set of cases as a function, then, with disjoint parts, this function is invertible: we can pick an object, identify the corresponding set of cases, and then use that set to get back the original object. In plots where the objects do not represent disjoint subsets of the data, this correspondence is broken: if we identify cases corresponding to a single object, there is no way to go back from these cases to the original object.

```{r geoms-bijection}
#| echo: false
#| fig-cap: "Disjoint subsets provide a one-to-one correspondence (bijection) between geometric objects and the data. Suppose we mark out the cases corresponding to one object (the left most bar). Top row: if each geometric object (bar) represents unique set of cases, we can easily go back and forth between the object and its underlying data (middle panel). Thus, the function of picking a set of cases corresponding to an object is bijective. Bottom row: when there is an overlap between the cases represented by each object, once we pick the set of cases corresponding to that object, there is no simple way to use that set to get the corresponding object back."

knitr::include_graphics("./figures/geoms-bijection.png")
```

To illustrate this idea on concrete data, take the following barplot representing vote share among the top three parties in the 2023 New Zealand general election [@election2023]:

```{r barplot-bijection}
#| echo: false
#| fig-cap: "Geometric objects typically represent disjoint subsets of the data. The plot shows the vote share of the top three parties in the 2023 New Zealand general election, with each bar representing a unique subset of voters."

knitr::include_graphics("./figures/barplot-bijection.png")
```

Each bar represents a unique set of voters and thus the subsets of the data represented by the bars are disjoint. Technically, there is no hard and fast rule about this. For example, we could transform the first bar by taking a union of the votes of National and Labour parties, and represent the same underlying data the following way:

```{r union-geoms}
#| echo: false
#| fig-cap: "Hypothetically, there is nothing preventing us from encoding the same information in multiple objects, and showing non-disjoint parts of our data. The plot shows the same underlying data as \\@ref(fig:geoms-bijection), with the leftmost bar representing a union of National and Labour voters. The two leftmost bars thus do not represent disjoint subsets of the data. For a more realistic example, see Figure \\@ref(fig:union-geoms2)."

knitr::include_graphics("./figures/barplot-notbijection.png")
```

However, this way of representing the data has several problems. First, there is the issue of its suitability towards the main goal of the visualization. Specifically, when visualizing election data such as this one, we are typically interested in judging the relative number of votes each party received. The second barplot makes this comparison difficult. Specifically, in the second barplot, since the National bar represents a subset of the National OR Labour bar, we have to perform additional mental calculation if we want to find out how many votes Labour received and compare the absolute counts directly [@cleveland1985]. Second, we have metadata knowledge [see e.g. @wilkinson2012; @velleman1993] about the data being disjoint - we know that, in the New Zealand parliament electoral system, each voter can only cast one vote for a single party. Finally, there is the issue of duplicating information: in the second barplot, the number of votes the National party received is counted twice, once in the leftmost bar and again in the second-left bar. This goes against the general principle of representing our data in the most parsimonious way [@tufte2001].

Even when our goal is not to compare absolute counts, there are usually better disjoint data visualization techniques available. For example, if we were interested in visualizing the proportion of votes that each party received, we could draw the following plot: 

```{r stacked-proportion}
#| echo: false
#| fig-cap: "Even when proportions are of interest, there are usually disjoint data visualization techniques available. The plot shows proportion of vote share of the top three parties in the 2023 New Zealand general election, with each bar segment again representing a unique subset of voters."

knitr::include_graphics("./figures/barplot-bijection-proportions.png")
```

By stacking the bar segments on top of each other, we can easily compare proportion of the total number of votes while retaining a parsimonious representation of our data. Each bar segments now again represents a unique subset of voters.

The example above was fairly clear case of where a non-disjoint representation of the data would be the wrong choice, however, there are also more ambiguous situations. One such situation is when there are multiple attributes of the data which can be simultaneously present or absent for each case. For example, in 2020, a joint referendum was held in New Zealand on the question of legalization of euthanasia and cannabis. The two issues were simultaneously included on the same ballot. The legalization of euthanasia was accepted by the voters, with 65.1% of votes supporting the decision, whereas the legalization of cannabis was rejected, with 50.7% of voters rejecting the decision [@referendum2020].

The referendum data can be visualized in the following way:

```{r union-geoms2}
#| echo: false
#| fig-height: 3
#| fig-cap: "Realistic example of a non-disjoint data representation. The plot shows the vote share in the combined 2020 New Zealand referendum on euthanasia and cannabis, where the two issues were simultaneousy presented on the same ballot. The two bars each show (mostly) the same subset of ballot, with each ballot contributing to the height of one segment in each bar."

knitr::include_graphics("./figures/referendum-notbijection.png")
```

By definition, both bars include votes which were cast by the same voter [ignoring the votes where no preference was given for either issue, @referendum2020]. Thus, the sets of voters that the two bars and the four bar segments represent overlap.

In general, there is nothing inherently wrong with the plot above. Non-disjoint representations of the data can work well for certain data types such as set-typed data [see e.g. @alsallakh2014]. In the context of static data visualization, plots like these can serve useful roles. However, even here, there is a simple way of representing the same data in a disjoint way - draw two separate plots:  

```{r}
#| echo: false
#| fig-height: 3
#| fig-cap: "Non-disjoint data representations can often be recast into disjoint ones. The figure again shows the vote share in the combined 2020 New Zealand referendum on euthanasia and cannabis, however, now each issue is plotted in a separate plot, and thus each geometric object in each plot represents a unique subset of ballots/voters."

knitr::include_graphics("./figures/referendum-bijection.png")
```

Why should we care about whether our representations of the data are disjoint or not? I argue that, for many types of plots, disjointness presents a better mental model: one geometric object for one unique set of observational units or "things". Conversely, if the objects in our plots do not represent disjoint subsets, then we need to keep track of *how* these objects are related. 

This issue may be particularly true for interactive visualization. The natural correspondence between geometric objects and disjoint subsets of the data makes certain interactions more intuitive. Conversely, overlapping subsets introduce complications. For instance, when a user clicks on a bar in a linked barplot, they might be surprised if parts of the other bars within the same plot get highlighted as well: they intended to highlight *that* bar, not the others. Likewise, when querying, if our objects do not represent disjoint subsets of the data, we have to think about what querying means: are we querying the objects or the cases corresponding to the objects? Disjoint partitions simplify our mental model, and this may be the reason why some authors discuss interactive features in the context of partitions [see e.g. @buja1996; @keim2002].    

SQL aggregation queries (`GROUP BY`) are based on partitions [@hellerstein1999].

#### Plots as partitions

In the two preceding sections, I have argued that our interactive data visualization system should have fulfill have two fundamental properties:

- Plots should show the full data (surjective mapping)
- Geometric objects within these plots should represent distinct subsets (disjoint parts)

These two properties suggest a fundamental model for plots: that of a [partitions](#Partitions). 

While I have not been able to find explicit references linking geometric objects to partitions, some authors have used the language of partitions. For example, Wilkinson [-@wilkinson2012, pp 210] and @keim2002 have linked stacked plots to (hierarchical) partitions.

#### Partitions and products

In a typical interactive plot, the data will be partitioned across multiple dimensions. To give a concrete example, suppose we want to draw the following barplot:

```{r}
#| echo: false
#| message: false

df <- data.frame(group = factor(c("A", "A", "A", "B", "B", "C", "C", "C")),
                 selection = factor(c(1, 1, 2, 1, 2, 1, 2, 2)),
                 value = c(12, 21, 10, 9, 15, 15, 12, 13))
df2 <- aggregate(value ~ group + selection, data = df, sum)

knitr::include_graphics("./figures/barplot-partitions-products.png")
```


We start with the following data, which includes a categorical variable (`group`) that we will plot along the x-axis, a variable representing selection status (`selection`) that we will color the bar segments with, and a continuous variable that we want to summarize (`value`):

```{r}
#| echo: false

library(kableExtra)

render_table <- function(x, ...) {
  knitr::kable(x, ...) |> kable_styling(full_width = FALSE)
} 

render_tables <- function(x) {
  n <- length(x)
  n_pairs <- floor(n / 2)

  for (i in seq_len(n_pairs)) {
    cat(knitr::kables(lapply(x[(2 * i - 1):(2 * i)], render_table)))
  }
  
  if (n %% 2 == 1) render_table(x[[length(x)]])
}

render_table(df, row.names = TRUE)
```



To draw the individual bar segments, we need to sum the `value` variable across the cases corresponding to each segment. To do this, we first need to split our data into multiple small disjoint subsets according to the product of `group` and `selection` variables:

```{r}
#| results: "asis"
# Using paste0() here to simulate a product of two factors
product_factor <- paste0(df$group, df$selection)
split_dfs <- split(df, product_factor)
render_tables(split_dfs)
```

We could then summarize each small data set by summing `value`:

```{r}
#| results: "asis"
summarized_dfs <- lapply(split_dfs, function(x) {
  aggregate(value ~ ., data = x, sum)
})

render_tables(summarized_dfs)
```

Finally, to "stack" the segments on top of each other, we need to combine the summaries back together, according to the levels of `group` variable, and take the cumulative sum:

```{r}
#| results: "asis"
grouped_dfs <- split(summarized_dfs, sapply(summarized_dfs, function(x) x$group))
stacked_dfs <- lapply(grouped_dfs, function(x) {
  x <- do.call(rbind, x)
  x$value <- cumsum(x$value)
  rownames(x) <- NULL
  x
})

render_tables(stacked_dfs)
```

Now, we can combine the tables into one data set and render:

```{r}
#| eval: false
combined_df <- do.call(rbind, stacked_dfs)
combined_df$selection <- factor(combined_df$selection, levels = c(2, 1))
# Need to reverse data order for ggplot2 to layer segments appropriately
combined_df <- combined_df[6:1, ] 

ggplot(combined_df, aes(x = group, y = value, fill = selection)) +
  geom_col(position = position_identity(), col = "white")
```

```{r}
#| echo: false
#| message: false

knitr::include_graphics("./figures/barplot-partitions-products.png")
```


Now, we have shown how we can compute summary statistics for a stacked barplot using a simple split-apply-combine pipeline [@wickham2011]. This is in fact what happens implicitly in most `ggplot2` plots:

```{r}
#| eval: false
ggplot(data, aes(x, y, fill = fill)) +
  geom_bar()
```

In the call above, we partition the data set by the Cartesian product of the `x`, `y`, and `fill` variables. That is, we break the data into parts based on the unique combinations of these variables, and then compute whatever statistical transformation we need. See the following comment from the [`ggplot2` documentation](https://github.com/tidyverse/ggplot2/blob/f46805349d6ca8ca7a99f8966cfa0f29279c2f6c/R/grouping.R#L7) [@wickham2016]:

```{r}
# If the `group` variable is not present, then a new group
# variable is generated from the interaction of all discrete (factor or
# character) vectors, excluding `label`.
```

#### Limits of simple product partitions

For many types of plots, the simple strategy of taking products of factors to form a single "flat" partition of the data works reasonably well. However, for other types of plots, this flat model is not enough. 

To give a concrete example, let's turn back to the barplot from the section \@ref(plots-as-partitions). To draw the barplot, we first split our data into smaller tables, summarized each table by summing up the values, stacked the summaries by taking their cumulative sum, and finally used these to draw the bar segments. This gave us a good visualization for comparing absolute counts across the categories. However, what if we wanted to compare proportions?

It turns out there is another type of visualization, called spineplot, which can be used to represent the same underlying data that barplot can, however, is much more useful for comparing proportions:  

```{r barplot-spineplot}
#| echo: false
#| fig-cap: "The same underlying data represented as a barplot (left) and a spineplot (right)."

knitr::include_graphics("./figures/barplot-spineplot.png")
```

A spineplot represents the same underlying statistic as a barplot (usually sums of counts). However, unlike in barplot, where the underlying statistic gets mapped to the y-axis position/bar height, in spineplot, the underlying summary statistic gets mapped to two aesthetics: the y-axis position and the bar width. Further, the y-axis position gets normalized, such that the heights of the different segments add up to one. The result is a visualization that makes it easy to compare relative proportions across the categories.

The fact that the spineplot makes it easy to see relative proportions makes it a very useful visualization. Notice how, in Figure \@ref(fig:barplot-spineplot), the spineplot makes it much easier to see that the proportions of the red cases are same across the B and C groups. Thus, spineplot is a definitely desirable type of representation for categorical data, especially if we can easily switch between it and the barplot.

However, despite the fact that both the barplot and the spineplot represent the same underlying summaries, turning one into the other is not a always trivial exercise. For example, in `ggplot2`, while it is easy to create a barplot using the simple declarative syntax, there is no such simple recipe for spineplots. To create the spineplot in Figure \@ref(fig:barplot-spineplot) took over 10 lines of external data wrangling code (using standard `dplyr` syntax).

Both aesthetics are stacked: width horizontally, *across bars*, y-axis position vertically, *across bar segments*.  

The important thing to notice that, 

The reason for this is because we have ignored or side-stepped certain issues. For example, how do we know if we are stacking the summaries in the right order? What if we need to transform the bar segments in some way, such as divide by the height of the parent bar, to show proportions rather than absolute counts? Once we start dealing with plots like spineplots or spinograms, it becomes clear that the flat partition structure is not enough.    

#### Partitions and hierarchy

@keim2002 stacked plots are suited for hierarchically-partitioned data. 

#### Partition data structures

### Computing summaries

After we have partitioned our data, we need a way to summarize each part by a set of summary statistics. Moreover, since the partitions of our data (may) form a hierarchy, we also need a way of referring to parts across the levels of the hierarchy.

#### Reducers

Suppose we are summarizing a part consisting of $n$ data points.

## Scaling

Information needs to be encoded in visual attributes [@cleveland1985]

Transformation is a critical tool for visualization or for any other mode of data analysis
because it can substantially simplify the structure of a set of data [@cleveland1993, pp. 48]

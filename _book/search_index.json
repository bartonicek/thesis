[["design.html", "3 Design 3.1 Implementation 3.2 Programming paradigm 3.3 System components 3.4 ", " 3 Design 3.0.1 User profile The profile of average user can also differ significantly, across systems and research areas. For example, some areas of interactive data visualization and HCI make no assumptions about the user’s level of experience or motivation, whereas others assume a highly motivated “expert” user with a sufficient level of domain knowledge (Dimara and Perin 2019). While designing my system, I have attempted to 3.1 Implementation The example code chunks in this section are written in both R and TypeScript. While I would prefer to use R for all code examples, due to its tight integration with RMarkdown, some of the concepts are much easier to explain in a language with static typing like TypeScript (particularly, type annotations and interfaces). However, since some examples also use graphical output/plots, I also wanted to use R. So, where graphical output is important, the code examples are written in R, and, where the code itself is the main focus, they are written in TypeScript. I hope this bilingualism is not too confusing and have tried to use only the basic features of each language to make the examples clear. 3.2 Programming paradigm “apex predator of grug is complexity complexity bad say again: complexity very bad you say now: complexity very, very bad given choice between complexity or one on one against t-rex, grug take t-rex: at least grug see t-rex” The Grug Brained Developer, Gross (2024) It is often the case that the same programming task can be tackled in many different ways. Programs which solve the same problem can differ in how they decompose the problem, how they represent and manipulate data, how they handle state, how explicit they are in handling low-level details versus relying on abstractions, and so on. However, all of these concerns share one central topic: complexity (Booch et al. 2008). Without constant effort and careful thought, software has the tendency to grow out of scope and become unmanageable. Preventing this, and coming up with a formal set of high-level principles for safeguarding against complexity is the job of programming paradigms (Chambers 2014; Jordan et al. 2015; Van Roy et al. 2009). All programming paradigms offer techniques for decomposing code into smaller, more manageable parts, and delineating boundaries between those parts. However, the way they do this can differ considerably. Most programming languages are geared towards one specific programming paradigm, and typically support only one or two to a reasonable capacity (Van Roy et al. 2009). Fortunately, this is not the case for either JavaScript/TypeScript or R, since both are multiparadigm programming languages (Chambers 2014; MDN 2024d). Both languages support object-oriented programming, via prototype inheritance in the case of JavaScript (MDN 2024a) and the S3, S4, and R6 systems in the case of R (Wickham 2019), and treat functions as first class citizens, allowing for functional programming style (Chambers 2014; MDN 2024c). Further, as C based languages, both also support classical imperative/procedural programming style, and also provide some utilities for reflective metaprogramming. The flexibility of JavaScript and R had allowed me to experiment with different programming paradigms while developing my interactive data visualization package. I have rewritten the JavaScript side of the package multiple times from scratch, testing out several different programming paradigms and styles in the process. Below, I provide a rough sketch of the paradigms I have sampled, as well as an account of my experience of using each paradigm and some thoughts on its suitability for designing interactive data visualization systems. 3.2.1 Imperative programming Imperative programming is one of the oldest and most classical programming paradigms. It conceptualizes the program as a sequence of discrete steps that manipulate some mutable state (Frame and Coffey 2014). In this way, it closely resembles the way computer programs get executed on the underlying hardware (barring some advanced techniques such as branch prediction and speculative execution, the CPU executes instructions sequentially, see e.g. Parihar 2015; Raghavan, Shachnai, and Yaniv 1998). 3.2.2 Functional programming 3.2.3 Object oriented programming 3.2.3.1 Encapsulation Encapsulation or information hiding refers to the idea that certain properties of a class should not be accessible from the outside. Encapsulation is generally seen as one of the most important concepts in OOP, since it allows for continuity: by encapsulating data/functionality, the developer of an OOP system may modify these private properties without affecting the public interface (Meyer 1997). Note that the encapsulation does not mean that the hidden data or implementation has to be invisible to the user, merely that the user should not be able to access it and rely on it as part of their program (Meyer 1997). 3.2.3.2 Small interfaces A concept which is closely related to encapsulation is the idea of small interfaces: if two modules or classes communicate, they should exchange as little information as possible (Meyer 1997). In practice, this idea is often not closely adhered to in OOP. It is common for class methods to receive pointers to other objects as arguments, sending all of the information contained in the object and effectively defeating the principle of small interfaces (Will 2016). 3.2.3.3 Open-closed principle Modules should be open to extension but close to modification. 3.2.3.4 Polymorphism In other words, only the behavior of the object should matter (Black 2013). 3.2.3.5 Domain-driven design One concept that is not unique to OOP but has a strong tradition in it is domain-drive design: the idea that classes and object should model things in the business domain or the real world (Hadar 2013; Meyer 1997). The stated benefit of this strategy is that this makes it easier to discover objects/classes and their relationships, by exploiting the mind’s capacity for thinking about the natural world (Hadar 2013; Meyer 1997). 3.2.4 Data oriented programming Data oriented programming (DOP) and data oriented design (DOD) are two newer programming paradigms that have been slowly gaining a foothold in the recent years. While the terms are sometimes used interchangeably, there are some important differences: DOP focuses more on high-level principles such as structuring and organizing code (Sharvit 2022; Nicolai Parlog 2024), whereas DOD, having originated in the world of video-game development, tends to be more concerned with low-level optimization details such as memory layout and CPU access patterns (Acton 2014; Bayliss 2022; Kelley 2023; Nikolov 2018; Fabian 2018). However, there is a considerable overlap the conclusions and high-level guidelines that the two programming paradigms provide. For this reason, I will discuss both DOP and DOD here, within the same section, and refer to both as “DOP” unless explicitly stated otherwise. The core idea of DOP is a data-first perspective. In the DOP view, programs should be viewed as transformations of data, nothing less, nothing more (Acton 2014; Fabian 2018; Sharvit 2022). As a result, every program is composed of two sets of components: stateful data and stateless functions that act on and transform this data (Fabian 2018). In other words, data and code should always be kept separate, such that both can be reasoned about in isolation.Code should live inside modules composed of pure (stateless) functions (Fabian 2018; Sharvit 2022). This strict segregation brings several benefits. The data becomes easy to pass around and serialize, and, since the code is composed of pure functions, it becomes easy to test and mock (Sharvit 2022). Data should be represented by plain data structures. These can formed by combining generic components, such as primitives, arrays, and maps/dictionaries/structs (for example JSON; Sharvit 2022). It may even be desirable that the data adheres to the relational model (Codd 1970; Moseley and Marks 2006; Fabian 2018). This does not mean that the data has to actually live inside a relational database, just that its shape should be close to that of normalized relational database tables, with columns represented by generic arrays (Fabian 2018). Data represents just itself - data - and there is no obligation for it to model the real world or any kind of abstract entity. As a result, and we are free to represent data in any way we want, and this can in fact bring significant performance benefits (Acton 2014; Fabian 2018). A typical example of this in DOD is the Structure of Arrays (SoA; aka “parallel arrays” or “columnar format”) data structure (Acton 2014, 2019; Kelley 2023), in which a list of records is stored as a single object, with the records’ properties being stored as homogeneous arrays (this is also how R’s data.frame class is implemented). This type of representation reduces memory footprint and can improve cache line utilization, resulting in faster runtime (Acton 2014; Fabian 2018; Kelley 2023). Another example of idiosyncratic data representation that can lead to improved performance are Entity Component Systems in videogames (Härkönen 2019). Either way, even if performance is not the nubmer one concern, another benefit data representing itself is that it allows us introduce abstraction only gradually, since we can get far by relying on generic data manipulation functions (Fabian 2018; Sharvit 2022). It may seem that many of the DOP principles directly contradict many popular OOP principles, specifically encapsulation, inheritance, polymorphism, and domain driven design. However, many of these principles can either be reconciled with DOP, or DOP in fact provides better alternatives. Below, I go over these principles and provide code examples that further illustrate how DOP works. 3.2.4.0.1 Encapsulation When it comes to encapsulation in DOP, we have to differentiate between encapsulating data and encapsulating code. Encapsulating code is easy in DOP - we can simply not export certain functions from the code modules. We are then free to modify the signature of these functions without affecting the public interface (Fabian 2018). Encapsulating data may require a bit more work. Depending on the language, generic data structures may not have property access modifiers (although there does seem to be a trend in recent languages to support property access modifiers more generically, see e.g. Rust Foundation 2024; Zig Software Foundation 2024). For instance, in JavaScript, private properties can only be declared as part of a class declaration (MDN 2024c). However, in most languages, it is still possible to use other language features and metaprogramming to achieve data encapsulation - for example, in JavaScript, we can use the Proxy class to emulate private property access (see Appendix). Thus, encapsulation of data is certainly possible in DOP. However, a question still remains whether it is a good idea. While in OOP, encapsulation is generally seen as a net positive, in DOP it is thought to come with trade-offs. It does provide an additional layer of security, however, it also makes systems more complex and harder to debug (Fabian 2018; Sharvit 2022). And, even with full encapsulation, users may still come to rely on hidden features of the system (Fabian 2018). Ultimately, it is necessary to weigh the pros and cons of encapsulating data within the context of the specific use-case. Some languages also have features which allow for a weak form encapsulation which is compatible with DOP. In JavaScript, this can be implemented by using symbol keys for object properties (MDN 2024b). Symbols are builtin primitive in JavaScript and are guaranteed to be unique. If we assign a property to an object using an unexported symbol as the key, the user will still be able to inspect the object and see the property, however, they will not be able to access it without using reflection. This is actually in line with the data hiding concept as laid out by Meyer (1997). I actually found this form a weak encapsulation a good fit for plotscape. For example, here is how I implemented the Meta mixin which allows use to store arbitrary metadata on objects: // Meta.ts const METADATA = Symbol(&quot;metadata&quot;); type METADATA = typeof METADATA; export interface Meta&lt;T extends Record&lt;string, any&gt; = any&gt; { [METADATA]: T; } export namespace Meta { export function of&lt;T extends Object&gt;(object: T) { return { ...object, [METADATA]: {} }; } export function get&lt;T extends Meta&gt;(object: T, key: keyof T[METADATA]) { return object[METADATA][key]; } export function set&lt;T extends Meta, K extends keyof T[METADATA]&gt;( object: T, key: K, value: T[METADATA][K] ) { object[METADATA][key] = value; } } Now we can import the module and use it to add secret metadata to arbitrary data objects: import { Meta } from &quot;./Meta.ts&quot; interface User extends Meta&lt;{ id: number }&gt; { name: string; } const user: User = Meta.of({ name: &quot;Adam&quot; }); Meta.set(user, `id`, 1337); console.log(user) console.log(Meta.get(user, `id`)); ## { ## name: &quot;Adam&quot;, ## [Symbol(metadata)]: { ## id: 1337, ## }, ## } ## 1337 We can now use the Meta module internally, and, if we do not export the module or the METADATA symbol, then the user will not be able to access those properties easily. The user can still inspect the object and see the associated metadata, however, they cannot access it without the symbol. They may still be able to retrieve the symbol using the via the Reflect.ownKeys and Object.getOwnPropertySymbols functions, and so this is not a strong form of encapsulation (MDN 2024b). However, arguably, if the user goes this far as to retrive the symbol, and, assuming we are not storing any sensitive data (which is not the case in plotscape), it may be reasonable to allow them to access these “private” properties and suffer the consequences. 3.2.4.0.2 Inheritance In OOP, the primary mechanism for code reuse is inheritance. In DOP, since data is generic and separate from behavior, we can call functions from any module as long as the data we provide it as arguments adheres to the module’s interface. This makes code reuse trivial. For example, here’s a simplified version of the Reactive interface (Observer pattern) from plotscape: const LISTENERS = Symbol(`listeners`); // A unique symbol, to avoid namespace clashes type Dict = Record&lt;string, any&gt;; // Generic dictionary type type Callback = (data: Dict) =&gt; void; // Generic callback function type interface Reactive { [LISTENERS]: Record&lt;string, Callback[]&gt;; } namespace Reactive { export function of&lt;T extends Object&gt;(object: T): T &amp; Reactive { return { ...object, [LISTENERS]: {} }; } export function listen(object: Reactive, event: string, cb: Callback) { if (!object[LISTENERS][event]) object[LISTENERS][event] = []; object[LISTENERS][event].push(cb); } export function dispatch(object: Reactive, event: string, data: Dict) { for (const cb of object[LISTENERS][event] ?? []) cb(data); } } interface Dog extends Reactive { name: string } namespace Dog { export function of(name: string) { return Reactive.of({ name }) } } const dog = Dog.of(`Terry`) Reactive.listen(dog, `car goes by`, () =&gt; console.log(`Woof!`)) Reactive.dispatch(dog, `car goes by`) ## Woof! 3.3 System components 3.3.1 Factors Factors provide a way to partition the data into multiple disjoint parts. 3.3.1.1 Product factors We can combine a factor with \\(j\\) levels and another factor with \\(k\\) levels into a product factor with up to \\(j \\cdot k\\) levels. I independently discovered a formula similar to (Wickham 2013): \\[i_{\\text{product}} = i_1 + i_2 \\cdot \\max(j, k)\\] 3.3.2 Reducers (Gray et al. 1997) came up with OLAP data cube 3.3.3 Scales To visualize data, we need to be able to translate values from the space of the data to the space of the graphical device (computer screen). In most data visualization systems, this is done by specialized components called scales or coordinate systems. By serving as a bridge between what we have (data) and what we see (visual attributes), scales act as a fundamental building block of all data visualization systems. There exists is a fair research on the theoretical properties of scales and how they relate to the mechanisms of visual perception (see e.g. Krzywinski 2013; Michell 1986; Wilkinson 2012; Stevens 1946). However, when it comes to applying this knowledge and implementing scales in concrete data visualization systems, only scant information is available. And, even when such information is available, it is it is often quite abstract or high-level (for some rare counter-examples, see e.g. Murrell 2005; Ziemkiewicz and Kosara 2009). Therefore, I thought it could be quite instructive to go into the details of how scales are implemented in my own data visualization system. The following section is based largely on how scales have been implemented in existing data visualization codebases, such as the ggplot2 R package (Wickham 2016) or d3-scale module of D3 (Observable 2024; also used by Vega Satyanarayan et al. 2015), as well as on personal insights gained while implementing the package. 3.3.3.1 Overview From a high-level perspective, a scale is just a function \\(s: D \\to V\\) which translates values of the data \\(d \\in D\\) to values of some visual attribute \\(v \\in V\\), such as the x- and y-position, length, area, radius, or color (Wilkinson 2012). This function may or may not be invertible, such that, at times, each value of the visual attribute may be identified with a unique data value (but this is not always the case). One of the most common and typical cases is a scale where both \\(D\\) and \\(V\\) are subsets of the real numbers: \\[s: [d_{min}, d_{max}] \\to [v_{min}, v_{max}] \\qquad d_{min}, d_{max}, v_{min}, v_{max} \\in \\mathbb{R}\\] For example, suppose our data takes values in the range from 1 to 10 and we want to plot it along the x-axis, within a 800 pixels wide plotting region. Then, our scale is simply: \\[s_x: [1, 10] \\to [0, 800]\\] Now, there is an infinite number of functions that fit this signature. However, one particularly nice and simple candidate is the following function: Definition 3.1 (Simple linear mapping) \\[s(d) = v_{max} + \\frac{d - d_{min}}{d_{max} - d_{min}} \\cdot (v_{max} - v_{min})\\] if we substitute the concrete values into the formula, this becomes: \\[s_x(d) = 0 + \\frac{d - 1}{10 - 1} \\cdot (800 - 0) = [(d - 1) / 9] \\cdot 800\\] The function acts on the data in the following way: \\(s_x(1) = (1 - 1) / 9 \\cdot 800 = 0\\) \\(s_x(10) = (10 - 1) / 9 \\cdot 800 = 800\\) \\(s_x(d) \\in (0, 800)\\) for any \\(d \\in (1, 10)\\) That is, the function maps the data value 1 to pixel 0 (left border of the plotting region), value 10 to to pixel 800 (right border of the plotting region), and any value in between 1 and 10 inside the interval 0 to 800, proportionally to where in the data range it is located. It is relatively simple to translate the formula in 3.1 to code: // simpleScale.ts export function simpleScale( d: number, dmin: number, dmax: number, vmin: number, vmax: number, ): number { return vmin + ((d - dmin) / (dmax - dmin)) * (vmax - vmin); } And indeed, this function works the way we would expect: import { simpleScale } from &quot;./simpleScale.ts&quot; console.log(simpleScale(1, 1, 10, 0, 800)) console.log(simpleScale(5.5, 1, 10, 0, 800)) console.log(simpleScale(10, 1, 10, 0, 800)) ## 0 ## 400 ## 800 3.3.3.2 Limits of modeling scales as simple functions Simple scale functions like the one above can work fine for basic data visualization systems. However, once we begin adding more features, this design becomes prohibitive. Consider, for example, what happens if we want to: Expand the scale limits Scale discrete data Apply non-linear transformations Pan, zoom, reverse, reorder, or otherwise modify the scales interactively Let’s take the first point as a motivating example. Consider what happens to data points at the limits of the data range under the simple linear mapping: x &lt;- 1:10 y &lt;- rnorm(10, 0, 5) col &lt;- ifelse(1:10 %in% c(1, 10), &quot;indianred&quot;, &quot;grey80&quot;) plot(x, y, col = col, cex = 3, xaxs = &quot;i&quot;) The plot above shows values scaled using the simple linear mapping along the x-axis, i.e. \\(s: [1, 10] \\to [0, 800]\\). Notice that, since the position of the points representing the values 1 and 10 gets mapped to pixel values 0 and 800 (the left and right border of the plot), only half of each point is visible. To address this, most data visualization systems automatically expand the range of the domain by some pre-specified percentage: # By default, the plot() automatically expands the x- and y-axis # limits by approximately 4% on each end, see `xaxs` in ?graphics::par plot(x, y, col = col, cex = 3) We could achieve similar effect by modifying the simple linear mapping and adding an additional argument: // simpleScale2.ts export function simpleScale2( d: number, dmin: number, dmax: number, vmin: number, vmax: number, exp: number, // Extra argument ): number { return ( vmin + (exp / 2 + ((d - dmin) / (dmax - dmin)) * (1 - exp)) * (vmax - vmin) ); } Now, if we set the exp argument to some positive value, the scaled values get mapped closer to the center of the plotting region. For example, setting exp to 0.2 moves each of the data limits 10% closer to the center of the plotting region: import { simpleScale2 } from &quot;./simpleScale2.ts&quot; console.log(simpleScale2(1, 1, 10, 0, 800, 0.2)); console.log(simpleScale2(5.5, 1, 10, 0, 800, 0.2)); console.log(simpleScale2(10, 1, 10, 0, 800, 0.2)); ## 80 ## 400 ## 720 However, notice that this argument is applied symmetrically. At times, we may want to apply a different margin to each end of the scale. We could solve this by adding two arguments instead of one, e.g. expLeft and expRight, however, at this point, the function signature starts to become unwieldy. Also, note that the logic inside the function’s body becomes more complicated. If we have to call the function in multiple places, it may become difficult to remember what the each individual argument represents. Further, we may want to persist or modify some of the arguments during runtime (such as when panning or zooming). Therefore, a more structured approach is required. 3.3.3.3 Solution: Two-component scales The linear mapping formula in 3.1 can guide us in decomposing the scaling function into smaller, more manageable parts. Let’s look at the formula again: \\[s(d) = v_{min} + \\frac{d - d_{min}}{d_{max} - d_{min}} \\cdot (v_{max} - v_{min})\\] If we look closely, we may be able to see that the linear mapping is composed of two parts: \\[s(d) = \\color{steelblue}{v_{min} +} \\color{indianred}{\\frac{\\color{black}{d} - d_{min}}{d_{max} - d_{min}}} \\color{steelblue}{\\cdot (v_{max} - v_{min})}\\] That is, the linear mapping is composed of two simpler functions: \\(\\color{indianred}{n(d) = (d - d_{min}) / (d_{max} - d_{min})}\\) takes a data value \\(d \\in D\\) and maps it to the interval \\([0, 1]\\) \\(\\color{steelblue}{u(p) = v_{min} + p \\cdot (v_{max} - v_{min})}\\) takes a value in \\([0, 1]\\) and maps it to a visual attribute value \\(v \\in V\\) This leads us to the following definition of a scale: Definition 3.2 (Scale as composition of two functions) A scale \\(s\\) can be created by composing: A normalize function \\(n: D \\to [0, 1]\\), mapping data to the interval \\([0, 1]\\) An unnormalize function \\(u: [0, 1] \\to V\\), mapping value in \\([0, 1]\\) to the visual attribute codomain Such that: \\[s(d) = u(n(d))\\] For the case of the linear mapping, we could rewrite this in code as follows: // LinearMap.ts export namespace LinearMap { export function normalize(d: number, dmin: number, dmax: number) { return (d - dmin) / (dmax - dmin); } export function unnormalize(p: number, vmin: number, vmax: number) { return vmin + p * (vmax - vmin); } } import { LinearMap } from &quot;./LinearMap.ts&quot; console.log(LinearMap.normalize(5.5, 1, 10)) console.log(LinearMap.unnormalize(0.5, 0, 800)) console.log(LinearMap.unnormalize(LinearMap.normalize(5.5, 1, 10), 0, 800)) ## 0.5 ## 400 ## 400 This two component system allows for a clean separation of concerns. Specifically, the normalize function only needs to know how to map the data values to \\([0, 1]\\). It does not need to be aware of where these normalized data values will be mapped to. Conversely, the unnormalize function only needs to understand how to translate values from \\([0, 1]\\) to the space of the visual attribute (such as x-axis position). 3.3.3.3.1 Beyond linear maps One big advantage of the two-component scale system is that the functions \\(n\\) and \\(u\\) do not need to be a simple linear maps anymore. For example, suppose that our data \\(D\\) takes form of a set of discrete labels, such as \\(D = \\{ Prague, Vienna, Munich, Salzburg \\}\\). We can then replace \\(n\\) with a surjective function \\(n: D \\to [0, 1]\\) such that: \\[n(d) = \\begin{cases} 0.2 &amp; \\text{if } d = Munich \\\\ 0.4 &amp; \\text{if } d = Prague \\\\ 0.6 &amp; \\text{if } d = Salzburg \\\\ 0.8 &amp; \\text{if } d = Vienna \\end{cases}\\] In other words, \\(n\\) will place values of \\(D\\) at equidistant points along \\([0, 1]\\), ordered alphabetically. We can implement this function in code as follows: // PointMap.ts export namespace PointMap { export function normalize(d: string, dlabels: string[]) { return (dlabels.indexOf(d) + 1) / (dlabels.length + 1) } } Since the codomain of \\(n\\) is still \\([0, 1]\\), we can compose it with a simple linear mapping \\(u\\) just as easily as before: import { LinearMap } from &quot;./LinearMap.ts&quot; import { PointMap } from &quot;./PointMap.ts&quot; const labels = [&quot;Munich&quot;, &quot;Prague&quot;, &quot;Salzburg&quot;, &quot;Vienna&quot;]; console.log(PointMap.normalize(&quot;Munich&quot;, labels)); console.log(LinearMap.unnormalize(PointMap.normalize(&quot;Munich&quot;, labels), 0, 800)); console.log(LinearMap.unnormalize(PointMap.normalize(&quot;Prague&quot;, labels), 0, 800)); ## 0.2 ## 160 ## 320 3.3.3.3.2 Inverses Another benefit of two-component scale system is that, if both \\(n\\) and \\(u\\) are invertible, then so is \\(s\\): we can easily obtain the inverse scale function by inverting the definition from 3.2: Definition 3.3 (Scale inverse) If a scale \\(s\\) is composed of invertible functions \\(n\\) and \\(u\\), then \\(s\\) is invertible: \\[s^{-1}(v) = n^{-1}(u^{-1}(v))\\] This is the case for the simple linear map: the normalize and unnormalize functions are actually inverses of each other: import { LinearMap } from &quot;./LinearMap.ts&quot; console.log(LinearMap.unnormalize(LinearMap.normalize(300, 0, 500), 0, 500)) ## 300 However, the inverse may not always exist. In practice, this is often the case when the domain of the data \\(D\\) is smaller than the codomain \\([0, 1]\\). Take, for example, the discrete point mapping. Since \\(D\\) is finite but \\([0, 1]\\) has infinitely many values, there will always be some values in \\([0, 1]\\) that no \\(d \\in D\\) maps to. For example, if \\(D = \\{ Munich, Prague, Salzburg, Vienna \\}\\) and \\(Munich\\) maps to 0.2, \\(Prague\\) maps to \\(0.4\\), and \\(Salzburg\\) maps to \\(0.8\\), then there are no cities which map to 0.9, 0.444, or 0.123456789. Conversely, if we get given those numeric values, then there is no obvious way to map them back to the cities. One thing we can do is to replace the inverse/unnormalize function with a weaker form of inverse, called retraction (Lawvere and Schanuel 2009). Specifically, if we have a normalize function \\(n: D \\to [0, 1]\\), then an unnormalize retraction \\(u^*\\) will have the property that: \\[u^*(n(d)) = d \\qquad \\forall d \\in D\\] However, the converse doesn’t necessarily hold: \\[\\neg \\big[ n(u^*(v)) = v \\qquad \\forall v \\in V \\big]\\] For example, for the discrete point mapping, a retraction may map a value in \\([0, 1]\\) to the closest data value \\(d \\in D\\): // PointMap.ts export namespace PointMap { export function normalize(d: string, dlabels: string[]) { return (dlabels.indexOf(d) + 1) / (dlabels.length + 1) } // Retraction - find the closest label export function unnormalize(p: number, dlabels: string[]) { const k = Math.round(p * (dlabels.length + 1) - 1) return dlabels[k] } } const labels = [&quot;Munich&quot;, &quot;Prague&quot;, &quot;Salzburg&quot;, &quot;Vienna&quot;]; const [prague, munich] = [&quot;Prague&quot;, &quot;Munich&quot;].map(x =&gt; PointMap.normalize(x, labels)) const midpoint = (prague + munich) / 2 // Helper function for stripping away floating point error const strip = (x: number) =&gt; parseFloat(x.toPrecision(12)) console.log(`Midpoint between Munich and Prague: `, strip(midpoint)) console.log(`unnormalize(0.2999): `, PointMap.unnormalize(0.2999, labels)) console.log(`unnormalize(3): `, PointMap.unnormalize(0.3, labels)) ## Midpoint between Munich and Prague: 0.3 ## unnormalize(0.2999): Munich ## unnormalize(3): Prague While inverses are always unique (Lawvere and Schanuel 2009; Fong and Spivak 2019), we may be able to come up with many different retractions for any given function. For example, with the discrete point map above, we could use the floor function instead of rounding and assign label to a value in \\([0, 1]\\) if it is less than the value of the normalized label (but more than the preceding labels). The non-uniqueness of retractions presents a bit of a dilemma. How do we decide which retraction to use? And, if a certain retractive implementation of unnormalize returns a value, how do we decide if it is the “correct one”? However, in practice, this is not much of a problem. While developing the package, I found that I’ve only ever had to use the unnormalize function with continuous data (LinearMap), and so the inverse was always well-defined. This is probably also why packages like ggplot2 and D3 can get by without this functionality. However, I still find it helpful to include the unnormalize function as a first class citizen (instead of it being relegated to some special case), both in terms of the mental model and also for debugging. 3.3.3.3.3 Some other remarks about the two-component scale system It is worth noting that there is nothing inherently special about the interval \\([0, 1]\\) as the intermediate domain: any finite subset of \\(\\mathbb{R}\\) would do. However, the interval \\([0, 1]\\) is convenient, both in terms of interpretation as well as for implementation, as we will see later. Finally, so far I have discussed scales as functions: the scale function, the normalize function, and unnormalize function. Framing scales as composition of functions leads to a nice correspondence between the mathematical definition and the code. However, in practice, it may be more convenient to implement the domain and codomain as objects or classes, as we will also see in the following section. The important point is that, no matter how the two components are represented, each is responsible for translating values from/to its domain and the interval \\([0, 1]\\). 3.3.3.4 Past implementations of scales Two-component scale systems are fairly standard across data visualization packages. For example, the D3 library (Bostock, Ogievetsky, and Heer 2011) implements scales in a functional style, with the values representing the data domain and the visual (co)domain being provided as tuples or arrays of values, either during initialization or at some later point. For illustration, here is an example from the oficial documentation (Observable 2024): const x = d3.scaleLinear([10, 130], [0, 960]); x(20); // 80 const color = d3.scaleLinear([10, 100], [&quot;brown&quot;, &quot;steelblue&quot;]); color(20); // &quot;rgb(154, 52, 57)&quot; // The domain and codomain can also be specified separately const y = d3.scaleLinear().domain([10, 130]); Internally, D3 uses specialized functions to translate from the domain to the codomain (such as the normalize() and scale() functions for continuous and discrete/ordinal domains, respectively, and various interpolate() functions for codomains). Similarly, in ggplot2 (Wickham 2016), scales are built upon the Scale class, with each subtype implementing limits and palette properties. Similar to D3, the limits property is a vector which corresponds to the data domain and the palette property is a function which corresponds roughly to the visual codomain (the x- and y-position behave slightly differently, due to being passed through coordinate systems). Internally, the package uses the rescale function from the scales package (Wickham, Pedersen, and Seidel 2023) to map data values to \\([0, 1]\\) and then the palette function is responsible for mapping these normalized values to the visual attribute. For illustration, here’s the full definition of the map method on the ScaleContinuous class (I’ve added comments for clarity): map = function(self, x, limits = self$get_limits()) { # Limits are just a tuple, rescale maps x to [0, 1] x &lt;- self$rescale(self$oob(x, range = limits), limits) uniq &lt;- unique0(x) # Palette is a function which returns a vector of attribute values pal &lt;- self$palette(uniq) scaled &lt;- pal[match(x, uniq)] ifelse(!is.na(scaled), scaled, self$na.value) } 3.3.3.5 Proposed model of scales One feature that the models of scales that D3 and ggplot2 rely on is that they both treat the data domain and the visual attribute codomain as different types. In D3, fundamentally different functions are used to translate from \\(D \\to [0, 1]\\) and from \\([0, 1] \\to V\\), and in ggplot2, limits is a simple vector/tuple whereas palette is a function. While these approaches may have some benefits, such as perhaps offering greater flexibility, they also add additional complexity. Specifically, we have to use two different mental models: one when considering the domain and another when considering the codomain. Further, these models of scales only work in one direction: mapping values \\(D \\to V\\). For going the the other way, i.e. mapping \\(V \\to D\\), other specialized functions have to be used. I propose a model of scales which implements both the domain and the codomain as components of the same type: Expanse. Fundamentally, this makes it so that the only difference between the data domain and the visual attribute codomain is which property of the scale they are assigned to. Here is a (slightly) simplified version of the Scale interface: interface Scale&lt;D extends Expanse, V extends Expanse&gt; { domain: D codomain: V } D and V represent the data domain and the visual attribute codomain, respectively. The two fundamental functions connected to Scale are: function pushforward&lt;D, V&gt;(scale: Scale&lt;D, V&gt;, value: ValueOf&lt;D&gt;): ValueOf&lt;V&gt; function pullback&lt;D, V&gt;(scale: Scale&lt;D, V&gt;, value: ValueOf&lt;V&gt;): ValueOf&lt;D&gt; The pushforward function pushes values forward through the scale, first through its domain and then its codomain, and the pullback function pulls values back, first through its codomain and then through its domain. The ValueOf type helper just identifies the type associated with the expanse’s data (e.g. number for a continuous Expanse, string for a discrete Expanse, etc…). I’ve omitted the generic type parameter constraint (&lt;D extends Expanse, V extends Expanse&gt;) for brevity. Here is a simplified implementation of the two functions: namespace Scale { function pushforward&lt;D, V&gt;(scale: Scale&lt;D, V&gt;, value: ValueOf&lt;D&gt;): ValueOf&lt;V&gt; { const { domain, codomain } = scale; return Expanse.unnormalize(codomain, Expanse.normalize(domain, value)); } function pullback&lt;D, V&gt;(scale: Scale&lt;D, V&gt;, value: ValueOf&lt;V&gt;): ValueOf&lt;D&gt; { const { domain, codomain } = scale; return Expanse.unnormalize(domain, Expanse.normalize(codomain, value)) } } We can see that most of the work is done by the two Expanse components: we use domain to translates \\(D \\to [0, 1]\\) and codomain to translate \\([0, 1] \\to V\\). Scale only serves as plumbing, connecting the two together. I argue that this model provides several benefits. First of all, it makes the code easier to reason about. Since both the domain and codomain are of the same type, we only need to keep a single mental model of Expanse in mind. Second, if domain and codomain provide inverse functions (unnormalize), we get the inverse scale function \\(V \\to D\\) for free (this is just the pullback function). Finally, there is also some functionalities we can implement on Scale directly, and thus we can keep the interface for Expanse relatively simple. These common functionalities of Scale will be the subject of the next few sections, before we dive into Expanse proper. 3.3.3.5.1 Zero and one Recall that in Section 3.3.3.2, we ran into the problem of expanding axis limits. We solved the problem by adding an additional argument to the simpleScale function. However, is expanding the axis limits something that we would want to do for continuous scales only? Clearly, we may want to add margins to axes which do not represent continuous data too, such the the x-axis of a barplot. So, we do not want this functionality to be tied to the scale’s domain (which represents the data). We could add it to the scale’s codomain, however, then we would be breaking with our intention of representing both domain and codomain with the same generic Expanse type. So what can we do? We can put the functionality for expanding axis limits directly onto Scale. Specifically, notice that any values passing through scale will be first converted to the interval \\([0, 1]\\) and then back to the space of either the domain or codomain: \\[D \\to [0, 1] \\to V\\] If we re-normalize these normalized values in \\([0, 1]\\), we can achieve the result of expanding or shrinking axis limits. As an analogy, if Scale represents the plumbing between the domain and codomain, we can imagine squeezing or stretching the pipe to allow more or less water to flow through. To actually do this, we can add two additional parameters to Scale, zero and one: interface Scale&lt;D extends Expanse, V extends Expanse&gt; { domain: D codomain: V props: { // A dictionary of properties zero: number one: number } } Now, we can implement a new version of the pushforward function: function pushforward&lt;D, V&gt;(scale: Scale&lt;D, V&gt;, value: D): V { const { domain, codomain, props } = scale; const { zero, one } = props let normalized = Expanse.normalize(domain, value) normalized = zero + normalized * (one - zero) // Re-normalize return Expanse.unnormalize(codomain, normalized) } The function’s body is bit longer than before, however, the only real change is in the line with the comment. We push the normalized value up by the zero property and scale it by (zero - one) range. In other words, we can interpret zero as the proportion of the codomain range that the minimum data value will get mapped to, and one as the proportion of the codomain range that the maximum data value will get mapped to. Now we can use zero and one to implement margins. For example, by setting zero to 0.1 and one to 0.9, we can implement 10% margins on either side of the scale. That is, we get the same behavior that we did with the exp argument from Section 3.3.3.2. However, we will get this behavior generically, no matter what subtype of Expanse domain and codomain are. Also, we now have the freedom to manipulate each parameter separately (for example, we could specify just the “left” margin by setting zero to 0.1 and one to 1). However, there is much more we can do with the zero and one than just margins. Firstly, despite the names being a bit suggestive, zero and one can both take values less than zero and more than one. For example, suppose we increment both zero and one by the same amount, e.g. we set zero to 0.1 and one to 1.1. Then, the minimum data value will get mapped to the 10% of the codomain range, and the maximum data value will get mapped to 110% of the codomain range (which may lie outside the space representable by the graphic device). If the codomain represents the x-axis position, then we have shifted all of the geometric objects 10% to the right. We have effectively implemented panning: function move(scale: Scale, amount: number) { scale.props.zero += amount; scale.props.one += amount; } That’s it. We have implemented a functionality for panning which will work no matter if domain is transforms numbers, strings, or some more complicated data. We can also stretch or shrink zero and one in opposite directions. For example, by setting zero to -0.5 and one to 1.5, then the minimum and maximum data values will get mapped 50% below and 50% above the limits of the codomain range, respectively (hence, they may both become unrepresentable). Further, the 25 and 75 data percentiles will get mapped to the minimum and maximum of the codomain range. If we apply this to the x- or y-axes, we’ve just implemented zooming To be perfectly honest, there’s a bit more ceremony involved with zooming. Specifically, if we don’t start from zero = 0 and one = 1 (e.g. if our plot already has margins or if we’re zooming in multiple levels deep), then we need to re-normalize within these values. This took me a bit of time to nail down, however, it’s just (highschool) algebra: function rangeInverse(min: number, max: number) { return 1 / (max - min); } function invertRange(min: number, max: number) { const ri = rangeInverse(min, max); return [-min * ri, ri - min * ri]; } namespace Scale { export function expand( scale: { props: { zero: number; one: number } }, zero: number, one: number ) { const { zero: currentZero, one: currentOne } = scale.props; const currentRange = currentOne - currentZero; // Re-normalize within current values zero = (zero - currentZero) / currentRange; one = (one - currentZero) / currentRange; // Invert [zero, one] = invertRange(zero, one); scale.props.zero = zero; scale.props.one = one; } } const scale1 = { props: { zero: 0, one: 1 } }; // Mock of default scale const scale2 = { props: { zero: 0.1, one: 0.9 } }; // Mock of scale with margins // Zoom into the middle 50% of either scale Scale.expand(scale1, 0.25, 0.75); Scale.expand(scale2, 0.25, 0.75); console.log(`Zoomed in scale with no margins`, scale1.props); console.log(`Zoomed in scale with 10% margins`, scale2.props); ## Zoomed in scale with no margins { ## zero: -0.5, ## one: 1.5, ## } ## Zoomed in scale with 10% margins { ## zero: -0.3, ## one: 1.3, ## } As you can see, zooming into the middle 50% of a scale that already includes margins has a smaller effect on zero and one, since the margins have effectively expanded the space we’re zooming into (i.e., a scale with margins is already zoomed out, in a way). 3.3.3.5.2 Direction In the same way we can think about expanding/shrinking axis limits in a way that is not coupled to any particular data representation or visual attribute, it may also be helpful to make direction a property of Scale rather than either of the Expanse components. We could do this by manipulating the zero and one properties. For example, by setting zero to 1 and one to 0, we could effectively reverse the direction of the scale. However, in practice, this would complicate our logic and make it harder for someone to interpret the Scale properties. It is a better idea to add an explicit direction parameter instead: interface Scale&lt;D extends Expanse, V extends Expanse&gt; { domain: D codomain: V props: { zero: number one: number direction: 1 | -1 // Extra parameter } } Like with zero and one, direction acts on the normalized values in \\([0, 1]\\). This means that we need to apply it in any transformations that use these values. For example, here’s an updated version of the move function: export function move(scale: Scale, amount: number) { let { direction, zero, one } = scale.props; zero += direction * amount; one += direction * amount; } Likewise, the pushforward, pullback, and expand functions also need to take direction into account. Either way, with this functionality in place, it becomes trivial to flip or reverse a scale: export function flip(scale: Scale) { scale.props.direction -= 1; } 3.3.3.5.3 Multipliers Finally, it may also be helpful to have the ability to shrink/expand the normalized values by some constant without having to modify properties of either the domain or codomain. Again, this could be done by using the zero and one properties, however, it’s better to define explicit properties instead. Specifically, we can add two additional parameters: interface Scale&lt;D extends Expanse, V extends Expanse&gt; { domain: D codomain: V props: { zero: number one: number direction: 1 | -1 scale: number // Extra parameter mult: number // And another one } } The reason why we want two multiplier parameters instead of just a single one is that there are different reasons for why we may want to multiply values by some constant. Firstly, we may want to multiply the values by some constant value that remains static throughout the lifetime of the program/visualization. That is the job of the scale parameter. Conversely, we may want to also dynamically manipulate the constant by which the values are multiplied. That is what mult is for. Having two multipliers makes it easier to reason about the scale’s behavior, as well as to apply changes such as restoring to defaults. A good example of this is the barplot. In a typical barplot, all bars have the same width, and this bar width is some fraction of the width of the entire plotting region. Clearly, this fraction needs to depend on the number of bars in the plot, such that, with \\(k\\) categories/bars, the bar width will be proportional to \\(k\\). However, we may also want to be able to make the bars wider/narrower interactively, e.g. by pressing the +\\- keys. Thus, the width of the bars is proportional to \\(c \\cdot k\\) where \\(k\\) is the static part of the constant (scale) and \\(c\\) is the dynamic part of the constant (mult). We apply the constant to the normalized value each time we push/pull a value through a scale: // This will be included in the body of pushforward(); see below for full example let normalized = Expanse.normalize(domain, value) normalized = normalized * scale * mult Finally, we could hypothetically extend this idea of multipliers further and have an entire array of different multipliers, that would be reduced into a single constant each time we pushed a value through a scale. However, I found that having two, scale and mult, was enough to solve all of my scaling problems. Additionally, having an array of multipliers might make the scaling functions slightly less performant, if we have to reduce the array each time we pushforward/pullback, or it might make keeping track of the state of the Scale object slightly more complicated, if we roll these multipliers into one constant each time we update the array. We also lose the semantic distinction that we have with scale and mult. This might be a perfectly fine trade-off if our scales require more multipliers, however, I did not find this to be the case in my implementation. 3.3.4 Expanses As was hinted at in the previous section, expanses map values between their domain and the interval \\([0, 1]\\). To do this, they are equipped with two familiar functions: A normalize function \\(\\color{indianred}{n: X \\to [0, 1]}\\) An unnormalize function \\(\\color{steelblue}{u: [0, 1] \\to X}\\) foo Or, in code: function normalize&lt;X&gt;(expanse: Expanse&lt;X&gt;, value: X): number function unnnormalize&lt;X&gt;(expanse: Expanse&lt;X&gt;, value: number): X Notice that, whereas before we had considered normalize as mapping between \\(D \\to [0, 1]\\) and unnormalize as mapping between \\([0, 1] \\to V\\), we now consider both as mapping between \\([0, 1]\\) and an arbitrary domain \\(X\\). This domain can represent the data or the visual attribute - the expanse is agnostic about this. Also, while before we have discussed domains as subsets of \\(\\mathbb{R}\\), we can now start thinking about them as arbitrary sets. While subsets of \\(\\mathbb{R}\\) and sets of strings are really the only types of domains used in plotscaper, the model should readily extends to other sets, as long as a mapping to and from \\([0, 1]\\) can be provided. How normalize and unnormalize are implemented will depend largely on the subtype of Expanse, as well as on the desired behavior. For example, as will be discussed later, in plotscaper, the normalize and unnormalize methods implemented for ExpanseContinuous (subtype of Expanse&lt;number&gt;) work largely the same way as in section [SECTION]. However, while ExpansePoint and ExpanseBand are both subtypes of Expanse&lt;string&gt;, they behave differently - ExpansePoint maps strings (factor levels) to equidistant points along \\([0, 1]\\), whereas ExpanseBand maps the strings into the middle of “buckets” along \\([0, 1]\\). However, there is also some behavior that we may want to apply the same way across the different expanse subtypes. For example, it seems reasonable that the user should be able to zoom, pan, or reverse axes, regardless of whether a plot shows discrete or continuous data. As such, there may be some properties and functions common to the Expanse type. I will discuss these first. 3.3.4.1 Zero and one The maps may also take in and return values outside of \\(D^*\\) and \\([0, 1]\\), if adjustments have been made. For instance, in most data visualization packages, x- and y-axis limits are by default expanded some percentage beyond the range of the observed data to avoid the maximum and minimum datapoints from overlapping with the limits. For example, in base R: set.seed(12345) x &lt;- rnorm(5) y &lt;- rnorm(5) par(mfrow = c(1, 2)) plot(x, y) plot(x, y, xaxs = &#39;i&#39;, yaxs = &#39;i&#39;) Figure 3.1: Expanding axes. By default, axes in base R plot() function are expanded 4% beyond the range of the data (left). Otherwise, datapoints on the limits of their respective scales end up overlapping with the plot borders (right). Thus, upon normalizing the minimum and maximum data values, the expanse should return values other than \\(\\{0, 1\\}\\). Likewise, to support user interactions such as zooming and panning, the expanses may accept and return values outside of \\(D^*\\) and \\([0, 1]\\). Zooming and panning should be orthogonal to the underlying data type, such that user can interact with the plots the same way1, no matter whether their axes are continuous, discrete, or some combination of the two. To this end, I introduce two parameters representing the normalized value (\\(p\\)) of the minimum and maximum data point, called zero and one respectively. These parameters are agnostic to the underlying data type, such that if we have the data type-specific maps \\(n&#39;\\) and \\(u&#39;\\), the complete normalize and unnormalize maps are: \\[n(d) = \\text{zero} + n&#39;(d) \\cdot (\\text{one} - \\text{zero})\\] \\[u(p) = u&#39; \\bigg(\\frac{p - \\text{zero}}{\\text{one} - \\text{zero}} \\bigg)\\] To simplify, here’s what effect setting the two parameters to specific values has: Zero One Effect 0.05 0.95 Expands the margins by ~5% (actually 5.555…% since 0.05 / 0.9 = 0.0555…) 0.05 1.05 Shifts the expanse ‘up’ by 5% (e.g. moves x-axis 5% right) -0.50 1.50 Zooms into the middle 50% of the expanse (25 percentile goes to 0 and 75th to one) 3.3.4.2 Expanse Interface There are also other behaviours that expanses should support. For instance, we may want to be able to reset the expanse to defaults, retrain when the underlying data changes, and return nicely formatted breaks. How these behaviours are implemented, as well as other types of behavior, may be specific to the underlying data type. Overall, expanse interface may look something like this: interface Expanse&lt;T&gt; { normalize(value: T): number unnormalize(value: number): T defaultize(): this setZero(zero: number, default: boolean): this setOne(one: number, default: boolean): this freezeZero(): this freezeOne(): this move(amount: number): this expand(zero: number, one: number): this retrain(values: T[]): this breaks(n?: number): T[] } 3.3.4.3 Continuous Expanses The continuous expanse has as its underlying set \\([\\min, \\max] \\subseteq \\mathbb{R}\\). To understand how it works, let’s build it step by step. We start with the basic normalizing function: \\[n(d) = \\frac{d - \\min}{\\max - \\min}\\] This function takes some data value \\(d \\in [\\min, \\max]\\) and transforms it to \\([0, 1]\\). Most data visualization systems use a function like this at some step of the scaling processs - see scales::rescale and D3 normalize. This may work well for typical linear scales. However, we may also want to apply some transformation \\(f\\), such as square root or log. Then, to ensure that the observed data values still get normalized to \\([0, 1]\\), we need to apply the transformation to both \\(d\\) and the limits: \\[\\frac{f(d) - f(\\min)}{f(\\max) - f(\\min)}\\] Finally, as was discussed in EXPANSES, we want to be able to incorporate the zero and one paramaters, leading to the final normalizing function: \\[n(d) = \\text{zero} + \\frac{f(d) - f(\\min)}{f(\\max) - f(\\min)} \\cdot (\\text{zero} - \\text{one})\\] To obtain the unnormalizing function, we can simply invert the normalizing function: \\[u(p) = f^{-1} \\bigg\\{ f(\\min) + \\frac{p - \\text{zero}}{\\text{one} - \\text{zero}} \\cdot \\big[ f(\\max) - f(\\min) \\big] \\bigg\\}\\] The function transforms \\(x\\) to a percentage value \\(p \\in [0, 1]\\), provided \\(x\\) is within \\([\\min, \\max]\\). The value \\((\\max - \\min)\\) is also sometimes called the range (not to be confused with D3 range). We can invert the normalizing function and obtain the unnormalizing function, which is, for some percentage \\(p \\in [0, 1]\\): \\[u(p) = \\min + p \\cdot (\\max - \\min)\\] returns a value within the \\([\\min, \\max]\\) range, corresponding to the proportion of the maximum possible distance (range) from the origin (\\(\\min\\)). For example, \\(u(0.5)\\), returns a value that is located halfway between the limits. We can implement a simple continuous expanse like so: function identity&lt;T&gt;(x: T) { return x; } function expanseContinuous(min = 0, max = 1) { const [zero, one] = [0, 1] const [trans, inv] = [identity, identity] return { min, max, zero, one, trans, inv, range() { return this.max - this.min; }, transRange() { const { min, max, trans } = this; return trans(max) - trans(min); }, normalize(x: number) { const { min, zero, one, trans } = this; const normalized = (trans(x) - trans(min)) / this.transRange(); return zero + normalized * (one - zero); }, unnormalize(p: number) { const { min, zero, one, trans, inv } = this; return inv(trans(min) + ((p - zero) / (one - zero)) * this.transRange()); }, }; } const expanse1 = expanseContinuous(1, 10); console.log(expanse1.normalize(5)); console.log(expanse1.unnormalize(0.5)) ## 0.4444444444444444 ## 5.5 The functions \\(n, u\\) have several interesting properties. First off, they are inverses to each other and form an isomorphism, i.e. \\(u = n^{-1}\\) and \\(n = u^{-1}\\) such that \\(u(n(x)) = x\\) and \\(n(u(p)) = p\\). This also means that each function is a 1-to-1 mapping or bijection. In plain words, this means that we cannot get the same percentage by normalizing two different values and vice versa. As a result, we can keep switching between the normalized and unnormalized representations without losing any information: 3.3.4.3.1 Linearity Another important thing to note is that, while these types of normalizing functions are often called “linear” (e.g. scaleLinear() in D3), since their graphs form a straight line, they should not be confused with “linear functions”, since they do not satisfy the properties of linear functions, namely: Additivity: \\(\\text{normalize}(x + c) \\neq \\text{normalize}(x) + \\text{normalize}(c)\\) Homogeneity of degree 1: \\(\\text{normalize}(c \\cdot x) \\neq c \\cdot \\text{normalize(x)}\\). To illustrate, additivity does not hold when \\(\\min \\neq 0\\) because: \\[\\frac{(x + c) - \\min}{(\\max - \\min)}\\] \\[= \\frac{x - \\min}{\\max - \\min} + \\frac{c}{\\max - \\min}\\] \\[\\neq \\frac{x - \\min}{\\max - min} + \\frac{c - \\min}{\\max - \\min}\\] The same can be easily shown for the \\(\\text{unnormalize}\\) map and for homogeneity. Technically, this is due to a confusion between the definition of a “linear function” and a “linear polynomial”. The appropriate term to use would actually be “affine transformation.” Either way, if the minimum is not 0, we cannot expect the following to be equal: Or the following to be equal: However, if we keep in mind the fact that the normalizing function calculates the proportion of distance from the origin, we can see that the function in fact behaves linearly within the context of its limits. For example, consider the range \\([1, 10]\\). The value \\(5\\) is \\(4\\) units away from the lower limit, i.e. \\(5 - 1 = 4\\), so we can represent it, for example, as the sum of a value that is 3 units away and another that is one unit away, \\(n(5) = n(4) + n(2)\\): Likewise, again because \\(5\\) represents the distance of \\(4\\) units and \\(3\\) of \\(2\\), we can expect \\(n(5) = 2 \\cdot n(3)\\): 3.3.4.3.2 Transformations We can apply transformations to continuous expanses by transforming their limits. The outcome of this is that \\(\\min\\) and \\(\\max\\) still get mapped to \\(0\\) and \\(1\\) however, the graph of the function is no longer linear. Suppose we have non-linear function \\(f\\), along with an inverse \\(f^{-1}\\). Then: \\[n(x) = \\frac{f(x) - f(\\min)}{f(\\max) - f(\\min)}\\] \\[u(p) = f^{-1} \\bigg\\{f(\\min) + p \\cdot \\big[ f(\\max) - f(\\min) \\big] \\bigg\\}\\] For example, here’s how we could apply the transformation \\(\\bigg( f(x) = \\sqrt{x}, \\; f^{-1}(x) = x^2 \\bigg)\\) in code: Transformations such as these can be useful in two ways. First, sometimes we may be able to better see trends in the data when the data has been appropriately transformed. This is the case, for example, when plotting data which varies across orders of magnitude. In this case it may be useful to apply \\(\\log\\)-transformation. Second, transformations can also be helpful in situations where some graphical attributes are not perceived linearly. For example, when judging differently sized objects, viewers tend judge magnitude based on area rather than side or radius. As such, when drawing objects such as points or squares it can be helpful to apply square root as the inverse transformation. The idea is that, if one point has a data value that is \\(c\\) times bigger than another, it will have \\(\\sqrt{c}\\) times bigger radius and \\(c\\) times bigger area. Note that we are talking about the inverse transformation here, i.e. the transformation affecting the unnormalizing function. One thing to note is that the proportionality of the square-root transformation holds only when \\(\\min = 0\\). Otherwise: \\[\\sqrt{(\\min)^2 + cp \\cdot [(\\max)^2 - (\\min)^2]}\\] \\[= \\sqrt{c} \\cdot \\sqrt{(\\min)^2/c + p \\cdot [(\\max)^2 - (\\min)^2]}\\] \\[\\neq \\sqrt{c} \\cdot \\sqrt{(\\min)^2 + p \\cdot [(\\max)^2 - (\\min)^2]}\\] This is a problem in the existing packages. For example: 3.4 References Acton, Mike. 2014. “Data-Oriented Design and c++.” Luento. CppCon. https://www.youtube.com/watch?v=rX0ItVEVjHc. ———. 2019. “Building a Data-Oriented Future.” WeAreDevelopers. https://www.youtube.com/watch?v=u8B3j8rqYMw. Bayliss, Jessica D. 2022. “The Data-Oriented Design Process for Game Development.” Computer 55 (5): 31–38. Black, Andrew P. 2013. “Object-Oriented Programming: Some History, and Challenges for the Next Fifty Years.” Information and Computation 231: 3–20. Booch, Grady, Robert A Maksimchuk, Michael W Engle, Bobbi J Young, Jim Connallen, and Kelli A Houston. 2008. “Object-Oriented Analysis and Design with Applications.” ACM SIGSOFT Software Engineering Notes 33 (5): 29–29. Bostock, Michael, Vadim Ogievetsky, and Jeffrey Heer. 2011. “D\\(^3\\) Data-Driven Documents.” IEEE Transactions on Visualization and Computer Graphics 17 (12): 2301–9. Chambers, John M. 2014. “Object-Oriented Programming, Functional Programming and r.” Statistical Science 29 (2): 167–80. https://doi.org/10.1214/13-STS452. Codd, Edgar F. 1970. “A Relational Model of Data for Large Shared Data Banks.” Communications of the ACM 13 (6): 377–87. Dimara, Evanthia, and Charles Perin. 2019. “What Is Interaction for Data Visualization?” IEEE Transactions on Visualization and Computer Graphics 26 (1): 119–29. Fabian, Richard. 2018. “Data-Oriented Design.” Framework 21: 1–7. Fong, Brendan, and David I Spivak. 2019. An Invitation to Applied Category Theory: Seven Sketches in Compositionality. Cambridge University Press. Frame, Scott, and John W Coffey. 2014. “A Comparison of Functional and Imperative Programming Techniques for Mathematical Software Development.” Journal of Systemics, Cybernetics and Informatics 12 (2): 1–10. Gray, Jim, Surajit Chaudhuri, Adam Bosworth, Andrew Layman, Don Reichart, Murali Venkatrao, Frank Pellow, and Hamid Pirahesh. 1997. “Data Cube: A Relational Aggregation Operator Generalizing Group-by, Cross-Tab, and Sub-Totals.” Data Mining and Knowledge Discovery 1: 29–53. Gross, Carson. 2024. “The Grug Brained Developer.” https://grugbrain.dev. Hadar, Irit. 2013. “When Intuition and Logic Clash: The Case of the Object-Oriented Paradigm.” Science of Computer Programming 78 (9): 1407–26. Härkönen, Toni. 2019. “Advantages and Implementation of Entity-Component-Systems.” Jordan, Howell, Goetz Botterweck, John Noll, Andrew Butterfield, and Rem Collier. 2015. “A Feature Model of Actor, Agent, Functional, Object, and Procedural Programming Languages.” Science of Computer Programming 98: 120–39. Kelley, Andew. 2023. “A Practical Guide to Applying Data Oriented Design (DoD).” Handmade Seattle. https://www.youtube.com/watch?v=IroPQ150F6c. Krzywinski, Martin. 2013. “Axes, Ticks and Grids.” Nature Methods 10 (February): 183. https://doi.org/10.1038/nmeth.2337. Lawvere, F William, and Stephen H Schanuel. 2009. Conceptual Mathematics: A First Introduction to Categories. Cambridge University Press. MDN. 2024a. “Classes - JavaScript \\(\\vert\\) MDN.” MDN Web Docs. https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Classes. ———. 2024b. “Symbol - JavaScript \\(\\vert\\) MDN.” MDN Web Docs. https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Symbol. ———. 2024c. “Functions - JavaScript \\(\\vert\\) MDN.” MDN Web Docs. https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Functions. ———. 2024d. “JavaScript Language Overview - JavaScript \\(\\vert\\) MDN.” MDN Web Docs. https://developer.mozilla.org/en-US/docs/Web/JavaScript/Language_overview. Meyer, Bertrand. 1997. Object-Oriented Software Construction. Vol. 2. Prentice hall Englewood Cliffs. Michell, Joel. 1986. “Measurement Scales and Statistics: A Clash of Paradigms.” Psychological Bulletin 100 (3): 398. Moseley, Ben, and Peter Marks. 2006. “Out of the Tar Pit.” Software Practice Advancement (SPA) 2006. Murrell, Paul. 2005. R Graphics. Chapman; Hall/CRC. Nicolai Parlog. 2024. “Data Oriented Programming in Java 21.” Devoxx. https://www.youtube.com/watch?v=8FRU_aGY4mY. Nikolov, Stoyan. 2018. “OOP Is Dead, Long Live Data-Oriented Design.” CppCon. https://www.youtube.com/watch?v=yy8jQgmhbAU&amp;t=2810s. Observable. 2024. “D3-Scale \\(\\vert\\) D3 by Observable.” https://d3js.org/d3-scale. Parihar, Raj. 2015. “Branch Prediction Techniques and Optimizations.” University of Rochester, NY, USA. Raghavan, P., H. Shachnai, and M. Yaniv. 1998. “Dynamic Schemes for Speculative Execution of Code.” In Proceedings. Sixth International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems (Cat. No.98TB100247), 24. IEEE. https://doi.org/10.1109/MASCOT.1998.693711. Rust Foundation. 2024. “Pub - Rust.” https://doc.rust-lang.org/std/keyword.pub.html. Satyanarayan, Arvind, Ryan Russell, Jane Hoffswell, and Jeffrey Heer. 2015. “Reactive Vega: A Streaming Dataflow Architecture for Declarative Interactive Visualization.” IEEE Transactions on Visualization and Computer Graphics 22 (1): 659–68. Sharvit, Yehonathan. 2022. Data-Oriented Programming: Reduce Software Complexity. Simon; Schuster. Stevens, Stanley Smith. 1946. “On the Theory of Scales of Measurement.” Science 103 (2684): 677–80. Van Roy, Peter et al. 2009. “Programming Paradigms for Dummies: What Every Programmer Should Know.” New Computational Paradigms for Computer Music 104: 616–21. Wickham, Hadley. 2013. “Bin-Summarise-Smooth: A Framework for Visualising Large Data.” Had. Co. Nz, Tech. Rep. ———. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org. ———. 2019. Advanced r. Chapman; Hall/CRC. Wickham, Hadley, Thomas Lin Pedersen, and Dana Seidel. 2023. Scales: Scale Functions for Visualization. https://CRAN.R-project.org/package=scales. Wilkinson, Leland. 2012. The Grammar of Graphics. Springer. Will, Brian. 2016. “Object-Oriented Programming Is Bad.” Youtube. https://www.youtube.com/watch?v=QM1iUe6IofM. Ziemkiewicz, Caroline, and Robert Kosara. 2009. “Embedding Information Visualization Within Visual Representation.” In Advances in Information and Intelligent Systems, 307–26. Springer. Zig Software Foundation. 2024. “Documentation - the Zig Programming Language.” https://ziglang.org/documentation/master. The one exception may be panning barplots and histograms, where the y-axis upper y-axis limit may change but the lower should be fixed at 0, such that panning may shrink or stretch the bars, but not “lift” them up or move them down.↩︎ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]

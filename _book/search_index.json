[["system.html", "7 System description 7.1 Core requirements 7.2 High-level API (plotscaper) 7.3 Low-level implementation (plotscape)", " 7 System description This section contains a detailed description of the two software packages developed as part of this doctoral project: (plotscape and plotscaper): plotscape: Written in TypeScript/JavaScript, provides “low-level” interactive visualization utilities plotscaper: Written in R, provides a “high-level” wrapper/API for R users Web-technologies were chosen because they provide a simple and portable way to do interactive apps in R, having become the de facto standard thanks to good integration that packages such as htmlwidgets (Vaidyanathan et al. 2021) and Shiny (Sievert 2020). The functionality was split across two packages out of practical concerns; rather than relying on some JS-in-R wrapper library, plotscape was implemented in vanilla TypeScript/JavaScript directly, to achieve optimal performance and fine-grained control. plotscaper was then developed to provide a user-friendly R wrapper around plotscape’s functionalities. As of the time of writing, plotscape comprises of about 6,400 significant lines of code (SLOC; un-minified, primarily TypeScript but also some CSS, includes tests, etc…), and plotscaper contains about 500 SLOC of R code (both counted using cloc). The unpacked size of all files (including minified JS) is about 200 kilobytes for plotscape and 460 kilobytes for plotscaper, which is fairly modest compared to other interactive data visualization alternatives for R1. Both packages have fairly minimal dependencies. Since the two packages address fairly well-separable concerns - high-level API design vs. low-level interactive visualization utilites - I organize this section accordingly. First, I discuss general, high-level API concerns alongside plotscaper. Second, I delve into the low-level implementation details alongside plotscape. There are of course cross-cutting concerns with both packages and those will be addressed towards ends of the respective sections. However, first, let’s briefly review the core requirement of the package(s). 7.1 Core requirements To re-iterate, from Section 5, the core requirements for the high-level API (plotscaper) were: Provide a framework for creating and manipulating interactive figures geared towards data exploration Be accessible to a wide range of users with varying levels of experience Integrate well with popular tools within the R ecosystem, such as the RStudio IDE and RMarkdown These will be the subject of Section 7.2. However, to realize these goals, it was also necessary to design the low-level platform (plotscape) which could support them. The primary purpose of plotscape was to provide utilities for the interactive data visualization pipeline: Splitting the data into a hierarchy of partitions Computing and transforming summary statistics (e.g. stacking, normalizing) Mapping these summaries to visual encodings (e.g. x- and y-axis position, width, height, and area) Rendering geometric objects and auxiliary graphical elements Responding to user input and server requests, propagating any required updates throughout the pipeline Section 7.3 will discuss the above-mentioned tasks, and the data structures and algorithms used to support them. 7.2 High-level API (plotscaper) In Section 5, I already discussed some broad, theoretical ideas related to the package’s functionality. Here, I focus more on the concrete API - what plotscaper code looks like, how are users supposed to understand it, and why was the package designed this way. The goal is to provide a rationale for key design decisions and choices. 7.2.1 API design As mentioned in Section 5, a primary inspiration for plotscaper’s API was the popular R package ggplot2. In ggplot2, plots are created by chaining together a series of function calls, each adding or modifying a component of an immutable plot schema: library(ggplot2) # In ggplot, plots are created by chaining a series of function calls ggplot(mtcars, aes(wt, mpg)) + geom_point() + # The overloaded `+` operator serves to compose these calls scale_x_continuous(limits = c(0, 10)) # The call to ggplot() creates an immutable plot schema object plot1 &lt;- ggplot(mtcars, aes(wt, mpg)) names(plot1) ## [1] &quot;data&quot; &quot;layers&quot; &quot;scales&quot; &quot;guides&quot; &quot;mapping&quot; ## [6] &quot;theme&quot; &quot;coordinates&quot; &quot;facet&quot; &quot;plot_env&quot; &quot;layout&quot; ## [11] &quot;labels&quot; length(plot1$layers) ## [1] 0 # Adding components such as geoms returns a new schema object plot2 &lt;- ggplot(mtcars, aes(wt, mpg)) + geom_point() names(plot2) ## [1] &quot;data&quot; &quot;layers&quot; &quot;scales&quot; &quot;guides&quot; &quot;mapping&quot; ## [6] &quot;theme&quot; &quot;coordinates&quot; &quot;facet&quot; &quot;plot_env&quot; &quot;layout&quot; ## [11] &quot;labels&quot; length(plot2$layers) ## [1] 1 ggplot2 is well-loved by R users, as evidenced by the package’s popularity. However, its API presents certain limitations when building interactive figures. Specifically: Its design primarily focuses on individual plots. While facetting does make it possible to create multi-panel figures consisting of repeats of the same plot type (see facet_wrap() and facet_grid(), Wickham 2016), multi-panel figures with mixed plot types require the use of external packages such as gridExtra (Auguie 2017) or patchwork (Pedersen 2024). As discussed in Section 3, in interactive graphics, multi-panel figures composed of different plot types are essential, and as such, the single-plot model of ggplot2 is not sufficient. While the immutable schema model works well for static graphics, in interactive graphics, the ability to modify an already rendered figure can be extremely useful. For example, by directly mutating the figure state via a function call, we can easily set an interactive histogram binwidth to a precise value, rather than having to rely on some imprecise widgets/controls such as a slider. Many of the ggplot2’s core functions make heavy use of quotation and non-standard evaluation (NSE, Wickham 2019). While this style is fairly popular within the R ecosystem and does offer certain elegance/syntactic conciseness, it also complicates programmatic use (Wickham 2019). For instance, in ggplot2, to plot all pairwise combinations of the variables in a dataframe, we cannot simply loop over their names and supply these as arguments to the default aes function - instead, we have to parse the names within the dataframe’s environment (this is what the specialized aes_string function does). Again, in interactive contexts, the ability to easily manipulate figures with code is often highly desirable, and this makes NSE a hindrance (more on this later). The package was developed before widespread adoption of the pipe operator (both %*% from magrittr, Bache and Wickham 2022; and the native R |&gt; pipe, R Core Team 2024) and its use of the overloaded + operator is a noted design flaw (see Wickham, Hadley 2014). To address the single-plot focus limitation, a function-chaining approach similar to ggplot2 was adopted, however, with a focus on multi-panel figure composition. Functions primarily target the entire figure, adding or removing plots. However, specialized functions with selectors can also be used to modify individual plots. To enable mutable figure modification while retaining the benefits of immutability, most functions accept both immutable schemas and references to live figures (details in the following section). Finally, non-standard evaluation was avoided altogether, and functions can be composed using any valid pipe operator (in my examples, I prefer the base R |&gt; operator). 7.2.2 Basic example of plotscaper code The code below shows a basic example of an interactive figure. More advanced and realistic applications are shown in Section 8; this is example is only meant to provide a simple illustration of how interactive figures are built with plotscaper: library(plotscaper) create_schema(mtcars) |&gt; add_scatterplot(c(&quot;wt&quot;, &quot;mpg&quot;)) |&gt; add_barplot(c(&quot;cyl&quot;)) |&gt; set_scale(&quot;plot1&quot;, &quot;x&quot;, min = 0) |&gt; render() Figure 7.1: An example of a simple interactive figure in plotscaper. We first initialize the figure schema by calling create_schema() with the input data. Next, we chain a series of add_*() calls, adding individual plots. Further, we can also manipulate attributes of individual plots, by using, for instance, the set_scale() function to set the lower x-axis limit of the scatterplot to zero. All of these functions append instructions to the schema (more on that later, in Section 7.2.3). Finally, the schema is instantiated into a figure via the call to render(). There are several features of this process that bear explaining. 7.2.2.1 Figure vs. plot and selectors First, note that, as discussed in Section 7.2.1, all functions take as their first argument the entire figure and most also manipulate it, unlike in ggplot2 where the first argument is typically the schema of a single plot (unless when faceting is used). Thus the fundamental object being manipulated is the entire figure, rather than a single plot. Consequently, this design also necessitates the use of selectors for targeting individual plots, as seen in the set_scale() call above. I decided to use a simple string selector for this purpose. While alternative selection strategies, such as overloading the extraction ($) operator or using a separate selector function were considered, these all presented their own sets of trade-offs. In the end, I found the string selector to be the simplest and most straightforward solution. However, this design choice is open to being revisited in future major releases of the package. 7.2.2.2 Variable names Second, variable names are specified by simple string vectors. Formally, this means that the function arguments are not quoted (which is a bit unfortunate terminology since they are surrounded by quotation marks), i.e. in plotscaper we write add_scatterplot(c(\"x\", \"y\", \"z\")) instead of add_scatterplot(c(x, y, z)). While this style may be less familiar to some R users, and it does mean that, generally, specifying an encoding does require two extra key strokes, I believe the simplicity of using plotscaper programmatically makes it a worthwhile trade-off. For instance, in plotscaper, we can easily create an interactive scatterplot matrix (SPLOM) like so: column_names &lt;- names(mtcars)[c(1, 3, 4)] schema &lt;- create_schema(mtcars) for (i in column_names) { for (j in column_names) { if (i == j) schema &lt;- add_histogram(schema, c(i)) else schema &lt;- add_scatterplot(schema, c(i, j)) } } schema |&gt; render() Figure 7.2: An example of a programmatically-created figure (a scatterplot matrix). While Figure ?? could also be of course replicated with quotation/NSE, doing so may require knowledge of advanced R. While it is the case that many R users are used to the style of calling functions with “naked” variable names, a lot fewer likely posses the ability to write and manipulate these functions effectively. Even more fundamentally, R’s NSE is a form of meta-programming (Wickham 2019), which, despite its power, is often discouraged due to its potential impact on performance, safety, and readability (see e.g. Phung, Sands, and Chudnov 2009; the discussion at Handmade Hero 2025). To encourage programmatic use of plotscaper, I chose simple string vectors over quoted arguments, as the inherent trade-off just did not seem worth it. 7.2.2.3 Variables and encodings Third, note that, unlike in ggplot2, variable names are not meant to map directly to aesthetics such as x- or y-axis position or size. The reason for this is that, unlike ggplot2, plotscaper does not try to establish a direct correspondence between variables in the original data and the visual encodings/aesthetics. This is due to the fact that, in many common plot types, aesthetics really do not represent variables found in the original data, but instead ones which have been computed or derived in some way. Take, for instance, the following ggplot call: ggplot(mtcars, aes(x = mpg)) + geom_histogram() Overtly, it may seem as if the aes function simply maps the mpg variable to the x-axis. However, this interpretation is incorrect. Specifically, the variable which is actually mapped to the x-axis are the left and right edges of the histogram bins. These represent a derived variable, not found in the original data. Likewise, the y-axis variable represents counts within bins which are also a derived. Setting custom histogram breaks makes this lack of a direct correspondence even clearer: ggplot(mtcars, aes(x = mpg)) + geom_histogram(breaks = c(10, 15, 35)) Now it is easier to see that what gets mapped to the x-axis is not the mpg variable. Instead, it is the variable representing the histogram breaks. The mpg variable gets mapped to the plot only implicitly, as counts within the bins (the y-axis variable). Thus, in ggplot call, the semantics of aes(x = mpg, ...) are fundamentally different in geom_histogram as compared to, for example, geom_scatter. Arguing over the mapping between data variables andd aesthetics may seem like a minor nitpick, however, it is in fact a fundamental gap in the design. As discussed in Section 4, ggplot2 is based on the Grammar of Graphics model (Wilkinson 2012), which centers around the idea of composing plots out of independent, modular components. The fact that the semantics of aes are tied to the geoms (and stats) means that the ggplot2 utility functions are not really modular, in the sense of being truly independent. So what to do? To be perfectly frank, I have not found a perfect solution. In Section 4, I demonstrated that, in the general case involving transformations like stacking, stats and geoms cannot be truly independent. Ignoring that, the problem with aesthetics in plots like histograms is that, in some sense, we are putting the cart before the horse: ultimately, we want to plot derived variables, so we should specify these in the call to aes, however, we do not know what they will be before we add geom_histogram to the ggplot schema. So the schema creation process should perhaps be organized in a different way. As per Section 4, we could mirror the data visualization pipeline by doing something like: data |&gt; partition(...) |&gt; aggregate(...) |&gt; encode(...) |&gt; render(...) In fact, in broad strokes, this is how the data visualization pipeline is implemented in plotscape (see Section 7.3). However, this model does seem to have one significant downside: it does not lend itself easily to a simple declarative schema like that of ggplot2. Specifically, I have tried to coming up with such a schema, however, my efforts were largely unsuccessful2. This is why I opted in for the more traditional, nominal style of specifying plots in plotscaper, relying on the various add_* functions. While this style may seem less flexible, I hope I have made the case here that the underlying limitations are not an exclusive to plotscape/plotscaper, but extend to ggplot2 and all other GoG-based data visualization systems. I have simply chosen make these limitations explicit. If a better solution is found, it may be integrated into future releases of the package. A final point to mention is that, it could be argued that one benefit of the ggplot2 model where partitioning and aggregation logic is implicitly tied to geoms is that it makes it easier to mix different kinds of geoms. For instance, geom_histogram can be combined with geom_rug. 7.2.3 The scene and the schema A key part of the plotscaper API is the distinction between two classes of objects representing figures: schemas and scenes. Put simply, a schema is an immutable ledger or blueprint, while a scene is a live, rendered version of the figure. Both can be manipulated using (largely) the same set of functions/methods, however, what these functions actually do changes based on their target. As shown before, schema can be created with the create_schema() function: schema &lt;- create_schema(mtcars) |&gt; add_scatterplot(c(&quot;wt&quot;, &quot;mpg&quot;)) |&gt; add_barplot(c(&quot;cyl&quot;)) schema ## plotscaper schema: ## add-plot { type: scatter, variables: c(&quot;wt&quot;, &quot;mpg&quot;) } ## add-plot { type: bar, variables: cyl } str(schema$queue) ## List of 2 ## $ :List of 2 ## ..$ type: chr &quot;add-plot&quot; ## ..$ data:List of 3 ## .. ..$ type : &#39;scalar&#39; chr &quot;scatter&quot; ## .. ..$ variables: chr [1:2] &quot;wt&quot; &quot;mpg&quot; ## .. ..$ id : &#39;scalar&#39; chr &quot;d5f7a682-7059-4086-913a-f345d76d19f7&quot; ## $ :List of 2 ## ..$ type: chr &quot;add-plot&quot; ## ..$ data:List of 3 ## .. ..$ type : &#39;scalar&#39; chr &quot;bar&quot; ## .. ..$ variables: chr &quot;cyl&quot; ## .. ..$ id : &#39;scalar&#39; chr &quot;910e5f18-6990-47d8-91e6-f71006e5e5e8&quot; As you can see, the object created with create_schema() is essentially just a list() of messages. Modifying the schema by calling functions such as add_scatterplot() or set_scale() simply appends a new message to the list, similar to how objects of class ggplot are modified by the corresponding functions and the + operator in ggplot2 (Wickham 2016). This design makes the schema easy to transport (e.g. as JSON) and modify programmatically. Finally, rendering the schema into a figure requires an explicit call to render(). Note that this approach is different from the popular style of implicit rendering (used in e.g. ggplot2), where an object is rendered via an overloaded print() method; however, there is a good reason for this which will be discussed later. The call to render() turns the schema gets turned into a live, interactive figure by constructing an htmlwidgets widget (Vaidyanathan et al. 2021). This bundles up the underlying data and plotscape code (JavaScript, HTML, CSS) into a standalone widget which may get rendered inside, for example, RStudio viewer or RMarkdown document. All of the schema messages also get forwarded to the widget and applied sequentially, creating the figure. Note that, under this model, the schema merely records state-generating steps, not the state itself, and all of the actual state lives on the scene. This design avoids state duplication between the R session (server) and web-browser-based figure (client), eliminating the need for synchronization. While this client-heavy approach deviates from typical client-server architectures, where state predominantly resides on the server, it is essential for achieving responsive interactive visualizations. By keeping most of the state and computation on the client and avoiding round-trips to the server, we can achieve fast updates in response to user interaction (such as mouse movement during linked selection; this is also why features like linked selection tend to have poor performance in server-centric frameworks like Shiny, Chang et al. 2024). Conversely, as will be discussed below, while the R session (server) may occasionally send and receive messages, these are by comparison much less latency-sensitive, making a “thin” server perfectly viable. 7.2.4 Client-server communication While inside an interactive R session (e.g., in RStudio IDE (Posit 2024)), creating a plotscaper figure via calling render() also automatically launches a WebSockets server (using the httpuv package, Cheng et al. 2024). This server allows for live, two-way communication between the R session (server) and the figure (client). By assigning the output of render() to a variable, users can save a handle to this server, and call functions which query the figure’s state or cause mutable, live updates. For instance: # The code in this chunk is NOT EVALUATED - # it only works only inside interactive R sessions, # not inside static RMarkdown/bookdown documents. scene &lt;- create_schema(mtcars) |&gt; add_scatterplot(c(&quot;wt&quot;, &quot;mpg&quot;)) |&gt; add_barplot(c(&quot;cyl&quot;)) |&gt; render() # Render the scene scene # Add a histogram, modifying the figure in-place scene |&gt; add_histogram(c(&quot;disp&quot;)) # Set the scatterplot&#39;s lower x-axis limit to 0 (also in-place) scene |&gt; set_scale(&quot;plot1&quot;, &quot;x&quot;, min = 0) # Select cases corresponding to rows 1 to 10 scene |&gt; select_cases(1:10) # Query selected cases - returns the corresponding # as a numeric vector which can be used in other # functions or printed to the console scene |&gt; selected_cases() # [1] 1 2 3 4 5 6 7 8 9 10 As noted earlier, most plotscaper functions are polymorphic S3 methods which can accept either a schema or a scene as the first argument. When called with schema as the first argument, they append a message to the schema, whereas when called with scene as the first argument (and while inside an interactive R session), they send a WebSockets request through the httpuv server, causing a live-update the figure or the server to respond back with data. In more abstract terms, with respect to these functions, the process of rendering a schema is a functor/homomorphism, meaning that we can either call the functions on the schema and then render it, or immediately render the figure and then call the functions, and the result will be the same (provided no user interaction happens in the meantime). The one exception to this rule are state-querying functions such as selected_cases(), assigned_cases(), and get_scale(). These functions send a request to retrieve the rendered figure’s state and so it makes little sense to call them on the schema3. 7.2.5 HTML embedding As htmlwidgets widgets, plotscaper figures are essentially static webpages. As such, they can be statically embedded in HTML documents such as those produced by RMarkdown (Allaire et al. 2024) or Quarto (Allaire and Dervieux 2024). More specifically, when a plotscaper figure is rendered, htmlwidgets (Vaidyanathan et al. 2021) is used to bundle the underlying HTML, CSS and JavaScript. The resulting widget can then be statically embedded in any valid HTML document, or saved as a standalone HTML file using the htlwidgets::saveWidget function. This is in fact how plotscaper figures are rendered in the present thesis. As mentioned above, since client-server communication requires a running server, statically rendered figures cannot be interacted with through code, in the way described in Section 7.2.4. However, within-figure interactive features such as linked selection and querying are entirely client-side, and as such work perfectly fine in static environments. This makes plotscaper a very useful and convenient tool to use in interactive reports. 7.3 Low-level implementation (plotscape) This section describes the actual platform used to produce and manipulate interactive figures, as implemented in plotscape. I first discuss some key implementation details, including reactivity and rendering. Then I provide a detailed listing of the system’s components and their functionality. Key concepts are explained via example code chunks. All of these represent valid TypeScript code, and selected examples are even evaluated, using the Bun runtime (“Bun” 2025). The reason why TypeScript was chosen over R is that many of the examples are much clearer with explicit type signatures. Further, since plotscape is written in TypeScript, many of the examples are taken directly from the codebase, albeit sometimes modified for consistency or conciseness. A note on the coding style used in the examples. Many of the code examples use TypeScript’s namespace feature to bundle related functionality together, similar to how a class bundles data and behaviour together in object-orient programming (OOP), however, unlike in OOP, the namespace serves purely as a static container unlike a class which has to be instantiated4. Further, alongside a given namespace, many examples also include a type (interface) of the same name. TypeScript allows type and value overloading, such that a type (e.g. interface Foo {}) and a runtime variable (const Foo = 123) with the same name can exist simultaneously, within the same scope. Namespaces are runtime variables and so they do not clash with types names. This type-namespace style may seem like object-oriented programming (OOP) with extra steps, i.e. where someone might write const foo = new Foo(); const bar = foo.bar(), I write const foo = Foo.create(); const bar = Foo.bar(foo), however, I did choose this style for a reason. First, thanks to TypeScript’s structural typing, the functions exported from a namespace can be called with any variables that match type signature, not just class instances. This greatly increases code reusability. Second, this approach also promotes functions that operate on plain data, which simplifies reasoning: data and code can be considered separately, and one can exist perfectly fine in the absence of the other5. Finally, this data-oriented approach also avoids a hidden cost of OOP: to support runtime polymorphism, class instances have to store pointers to virtual method tables, increasing their size and the potential for cache misses. Plain data structures are often more performant when polymorphism is not required, and in the specific situations where it is, alternative runtime dispatch methods can still be implemented. Thus, for these reasons, I chose to use this style when implementing plotscape and use it here as well for consistency. 7.3.1 Reactivity A key implementation detail of all interactive applications is reactivity: how a system responds to input and propagates changes. However, despite the fact that interactive user interfaces (UIs) have been around for a long time, there still exist many different, competing approaches to handling reactivity. A particularly famous6 example of this is the web ecosystem, where new UI frameworks seem to keep emerging all the time, each offering its unique spin on reactivity (see e.g. Ollila, Mäkitalo, and Mikkonen 2022). This makes choosing the right reactivity model challenging. Furthermore, reactivity is paramount in interactive data visualization systems due to many user interactions having cascading effects. For instance, when a user changes the binwidth of an interactive histogram, the counts within the bins need to be recomputed, which in turn means that scales may need to be updated, which in turn means that the entire figure may need to be re-rendered, and so on. Also, unlike other types of UI applications, interactive data visualizations have no upper bound on the number of UI elements - the more data the user can visualize the better. This makes efficient updates crucial. While re-rendering a button twice may not be a big deal for a simple webpage or GUI application, unnecessary re-renders of a scatterplot with tens-of-thousands of data points may cripple an interactive data visualization system. Because of the reasons outlined above, reactivity was key concern for plotscape. While developing the package, I had evaluated and tried out several different reactivity models, before finally settling on a solution. Given the time and effort invested in this process, I believe it is valuable to give a brief overview of these models and discuss their inherent advantages and disadvantages, before presenting my chosen approach in Section 7.3.1.5. 7.3.1.1 Observer pattern One of the simplest and most well-known methods for modeling reactivity is the Observer pattern (Gamma et al. 1995). Here’s a simple implementation: // Observer.ts export namespace Observer { export function create&lt;T&gt;(x: T): T &amp; Observer { return { ...x, listeners: {} }; } export function listen(x: Observer, event: string, callback: () =&gt; void) { if (!x.listeners[event]) x.listeners[event] = []; x.listeners[event].push(callback); } export function dispatch(x: Observer, event: string) { if (!x.listeners[event]) return; for (const cb of x.listeners[event]) cb(); } } const person = Observer.create({ name: `Joe`, age: 25 }); Observer.listen(person, `age-increased`, () =&gt; console.log(`${person.name} is now ${person.age} years old.`) ); person.age = 26; Observer.dispatch(person, `age-increased`); Internally, an Observer object stores a dictionary where the keys are the events that the object can dispatch or notify its listeners of, and values are arrays of callbacks7. Listeners listen (or “subscribe”) to specific events by adding their callbacks to the relevant array. When an event occurs, the callbacks in the appropriate array are iterated through and executed in order. The Observer pattern easy to implement and understand, and, compared to alternatives, also tends to be fairly performant. However, a key downside is that the listeners have to subscribe to the Observer manually. In other words, whenever client code uses Observer values, it needs to be aware of this fact and subscribe to them in order to avoid becoming stale. Further, the logic for synchronizing updates has to be implemented manually as well. For instance, by default, there is no mechanism for handling dispatch order: the listeners who were subscribed earlier in the code are called first8. Moreover, shared dependencies can cause glitches and these have to be resolved manually as well. See for instance the following example: import { Observer } from &quot;./Observer&quot; function update(x: { name: string; value: number } &amp; Observer, value: number) { x.value = value; console.log(`${x.name} updated to`, x.value); Observer.dispatch(x, `updated`); } const A = Observer.create({ name: `A`, value: 1 }); const B = Observer.create({ name: `B`, value: A.value * 10 }); const C = Observer.create({ name: `C`, value: A.value + B.value }); Observer.listen(A, `updated`, () =&gt; update(B, A.value * 10)); Observer.listen(A, `updated`, () =&gt; update(C, A.value + B.value)); Observer.listen(B, `updated`, () =&gt; update(C, A.value + B.value)); update(A, 2); // C will get updated twice The example above shows what is in reactive programming known as the diamond problem. We have three reactive variables A, B, and C, such that B depends on A, and C depends simultaneously on A and B. Since C depends on A and B, it has to subscribe to both. However, C is not aware of the global context of the reactive graph: it does not know that B will update any time A does. As such, an update to A will trigger two updates to C despite the fact that, intuitively, it should only cause one. Without careful management of dependencies, this reactive graph myopia that the Observer pattern exhibits can create computational bottlenecks, particularly in high-throughput UIs such as interactive data visualizations. Consider an interactive histogram where users can either modify binwidth or directly set breaks. If both are implemented as reactive parameters, a poorly managed dependency graph (e.g., breaks dependent on binwidth, and rendering dependent on both) will result in unnecessary re-renders, impacting performance at high data volumes. 7.3.1.2 Streams A radically different approach to reactivity is offered by streams (see e.g. Abelson and Sussman 2022). Instead of events directly modifying data state, streams separate event generation from event processing, modeling the latter as pure, primarily side-effect-free transformations. These transformations can then be composed via usual function composition to build arbitrarily complex processing logic. Finally, due to the separation between the stateful event producers and stateless event transformations, this approach aligns closely with methods such as generators/iterators as well as functional programming more broadly (Abelson and Sussman 2022; Fogus 2013), and has implementations in numerous functional programming languages and libraries, most notably the polyglot Reactive Extensions library (also known as ReactiveX, Rxteam 2024). Consider the following implementation of a stream which produces values at 200-millisecond intervals and stops after 1 second: function intervalSteam(milliseconds: number, stopTime: number) { let streamfn = (x: unknown) =&gt; x; const result = { pipe }; function pipe(fn: (x: any) =&gt; unknown) { const oldStreamfn = streamfn; streamfn = (x: unknown) =&gt; fn(oldStreamfn(x)); return result; } const startTime = Date.now(); let time = Date.now(); const interval = setInterval(() =&gt; { time = Date.now(); const diff = time - startTime; if (diff &gt;= stopTime) clearInterval(interval); streamfn(diff); }, milliseconds); return result; } const stream = intervalSteam(200, 1000) stream .pipe((x) =&gt; [x, Math.round((x / 7) * 100) / 100]) .pipe((x) =&gt; console.log( `${x[0]} milliseconds has elapsed (${x[1]} milliseconds in dog years)` ) ); As you can see, the event producer (stream) is defined separately from the event processing logic, which is constructed by piping the result of one operation into the next. Because of the associativity of function composition, the stream actually exhibits properties of a functor, meaning that the order of composition - either through direct function composition or .pipe() chaining - does not affect the result. Additionally, while the stream transformations themselves are (generally) stateless, they can still produce useful side-effects (as can be seen on the example of the console.log call above). Further, because of this fact, they also lend themselves well to modeling asynchronous or even infinite data sequences (Abelson and Sussman 2022; Fogus 2013). While streams can be extremely useful in specific circumstances, their utility as a general model for complex UIs (beyond asynchronous operations) is debatable. Specifically, the inherent statefulness of UIs conflicts with the stateless nature of streams: stateless computations inside the stream have to leak into the rest of the application somewhere. Delineating which parts of the logic should go into streams versus which should be bound to UI components adds unnecessary complexity for little real benefit. Consequently, streams are likely not the optimal choice for interactive data visualization, where some state is unavoidable. 7.3.1.3 Virtual DOM Within the web ecosystem, a common way of handling reactivity involves something called the virtual DOM (VDOM). This approach, popularized by web frameworks such as React (Meta 2024) and Vue (Evan You and the Vue Core Team 2024), involves constructing an independent in-memory tree structure which provides a virtual representation of the UI. Reactive events are bound to nodes or “components” of this tree. Whenever an event occurs, changes cascade throughout the VDOM, starting with the associated component and propagating down through its children. Finally, the VDOM is compared or “diffed” against the actual UI, and only the necessary updates are applied. Note that, despite being named after the web’s DOM, the VDOM is a general concept, not tied to any specific programming environment. The VDOM provides a simple and convenient solution to reactive graph challenges such as the diamond problem described in Section 7.3.1.1, as evidenced by the massive popularity of web frameworks such as React or Vue. However, compared to alternatives, it also presents some significant performance trade-offs. Specifically, events near the root component trigger a cascade of updates which propagates throughout a large portion of the tree, even when there is no direct dependence between these events and the child components. Moreover, since the only way for two components to share a piece of state is through their parent, the model naturally encourages a top-heavy hierarchy, further compounding the issue. Finally, depending on the nature and implementation of the UI, the diffing process may be more trouble than its worth: while in a webpage, updating a button or two is a fast operation, in a data visualization system, it may be more efficient to re-render an entire scatterplot from scratch rather than trying to update select few points. 7.3.1.4 Signals Another approach to reactivity that has been steadily gaining traction over the recent years, particularly within the web ecosystem, are signals (also known as fine-grained reactivity). Popularized by frameworks such Knockout (knockoutjs 2019) and more recently Solid JS (Solid Core Team 2025), this approach has recently seen a wave adoptions by many other frameworks including Svelte (Rich Harris and the Svelte Core Team 2024) and Angular (Google 2025), and has even seen adoption outside of the JavaScript ecosystem, such as in the Rust-based framework Leptos (Leptos Core Team 2025). Signal-based reactivity is built around a core pair of primitives: signals and effects. Signals are reactive values which keep track of their listeners, similar to the Observer pattern (Section 7.3.1.1). However, unlike Observers, signals do not need to be subscribed to manually. Instead, listeners automatically subscribe to signals by accessing them, which is where effects come in. Effects are side-effect-causing functions which respond to signal changes, typically by updating the UI, and play a key role in the signal-based automatic subscription model. While signal-based reactivity might appear complex, its basic implementation is surprisingly straightforward. The following example is based on a presentation by Ryan Carniato, the creator of Solid JS (2023): export namespace Signal { export function create&lt;T&gt;(x: T): [() =&gt; T, (value: T) =&gt; void] { // A set of listeners, similar to Observable const listeners = new Set&lt;() =&gt; void&gt;(); function get(): T { listeners.add(Effect.getCurrent()); return x; } function set(value: T) { x = value; for (const l of listeners) l(); } // Returns a getter-setter pair return [get, set]; } } export namespace Effect { const effectStack = [] as (() =&gt; void)[]; // A stack of effects export function getCurrent(): () =&gt; void { return effectStack[effectStack.length - 1]; } export function create(effectfn: () =&gt; void) { function execute() { effectStack.push(execute); // Pushes itself onto the stack effectfn(); // Runs the effect effectStack.pop(); // Pops itself off the stack } execute(); } } const [price, setPrice] = Signal.create(100); const [tax, setTax] = Signal.create(0.15); const priceWithTax = () =&gt; Math.round(price() * (1 + tax()) * 100) / 100; // ^ Derived values automatically become signals as well Effect.create(() =&gt; console.log( `The current price is` + `${priceWithTax()}` + `(${price()} before ${tax() * 100}% tax)` ) ); setPrice(200); setTax(0.12); The key detail to notice is the presence of the global stack of effects. Whenever an effect is called, it first pushes itself onto the stack. It then executes, accessing any signals it needs along the way. These signals in turn register the effect as a listener, by accessing it as the top-most element of the effect stack. When the effect is done executing, it pops itself off the stack. Now, whenever one of the accessed signals changes, the effect re-runs again. Crucially, making a derived reactive value is as simple as writing a callback: if an effect calls a function using a signal, it also automatically subscribes to that signal (see the example of priceWithTax() above). Importantly, the effect subscribes only to this signal and not the derived value itself. In other words, effects only ever subscribe to the leaf nodes of the reactive graph (signals). Derived values computed on the fly (and, if necessary, can be easily memoized9), and event ordering is simply managed via the runtime call stack. Signals provide an elegant solution to many problems with reactivity. They automate subscription to events, prevent unnecessary updates, ensure correct update order, and, due to their fine-grained nature, are generally highly performant compared to more cumbersome methods like the virtual DOM. However, again, signals do also introduce their own set of trade-offs. Chief among these, signal’s reliance on the call stack for event ordering necessitates their implementation as functions (getter-setter pairs), rather than plain data values. While techniques like object getters/setters or templating (as seen in SolidJS, Solid Core Team 2025) can be used to hide this fact, it does nevertheless add an extra layer of complexity. Similarly, many features important for performance, like memoization and batching, also require treating signals as distinct from plain data. Having code consist of two sets of entities - plain data and signals - ultimately impacts developer ergonomics. 7.3.1.5 Reactivity in plotscape and final thoughts At the start of the project, I had used the Observer pattern for modeling reactivity. However, I had the idea of letting the users to define reactive parameters that could be used at arbitrary points in the data visualization pipeline. This had led me to explore the various models of reactivity described above, and even do a full rewrite of plotscape with signals at one point. However, eventually, I ended up reverting back to the Observer pattern. The primary reason was developer ergonomics. While many properties of signals like automatic subscription were appealing, the need to manage both data and signals as distinct entities proved cumbersome. Deciding which components of my system and their properties should be plain data versus signals added an additional overhead and made refactoring more complex. Furthemore, I also found that, in the interactive data visualization pipeline, reactivity could be aligned with the four discrete stages: partitioning, aggregation, scaling, and rendering. Specifically, reactive values could be introduced in batch right at the start of each of these four steps, simplifying the reactive graph as well as the overall mental model. Introducing reactivity at other points seem to offer limited practical benefit. Thus, despite the limitations of the Observer pattern, the structured nature of the problem (interactive data visualization pipelines) ultimately makes it a good solution in my eyes. 7.3.2 System components This section discusses the core components of plotscape, detailing their functionality, implementation, and interconnections. The goal is to give an overview and provide a rationale for the design of key parts of the system. As before, TypeScript code examples are provided, and, in general, these map fairly closely to the real codebase. 7.3.2.1 Indexable One of the fundamental considerations when implementing a data visualization system is how to represent a data variable: a generalized sequence of related values. Clearly, the ability to handle fixed-length arrays is essential, however, we may also want to be able to treat constants or derived values as variables. To give an example, in a typical barplot, the y-axis base is a constant, typically zero. While we could hypothetically append an array of zeroes to our data, it is much more convenient and memory efficient to simply use a constant (0) or a callback/thunk (() =&gt; 0). Similarly, at times, arrays of repeated values can be more optimally represented as two arrays: a short array of “labels” and a long array of integer indices (i.e. what base R’s factor class does). Thus, representing data effectively calls for a generalization of a data “column” which can encompass data types beyond fixed-length arrays. The type Indexable&lt;T&gt; represents such a generalization of a data column. It is simply a union of three simple types: Indexable&lt;T&gt; = T | T[] | ((index: number) =&gt; T) In plain words, an Indexable&lt;T&gt; can be one of the following three objects: A simple (scalar) value T A fixed-length array of T’s (T[]) A function which takes an index as an argument and returns a T That is, Indexables generalize arrays, providing value access via an index. Arrays behave as expected, scalar values are always returned regardless of the index, and functions are invoked with the index as the first argument (this functionality is provided by Getters). As a final note, Indexables are somewhat similar to Leland Wilkinson’s idea of data functions (see Wilkinson 2012, 42), although there are some differences (Wilkinson’s data functions are defined more broadly). 7.3.2.2 Getter A Getter&lt;T&gt; is used to provide a uniform interface to accessing values from an Indexable&lt;T&gt;. It is simply a function which takes an index and returns a value of type T. To construct a Getter&lt;T&gt;, we take an Indexable&lt;T&gt; and dispatch on the underlying subtype. For illustration purposes, here is a simplified implementation: // Getter.ts export type Getter&lt;T&gt; = (index: number) =&gt; T; export namespace Getter { export function create&lt;T&gt;(x: Indexable&lt;T&gt;): Getter&lt;T&gt; { if (typeof x === `function`) return x; else if (Array.isArray(x)) return (index: number) =&gt; x[index]; else return () =&gt; x; } } we can then create and use Getters like so: import { Getter } from &quot;./Getter&quot; const getter1 = Getter.create([1, 2, 3]) const getter2 = Getter.create(99); const getter3 = Getter.create((index: number) =&gt; index - 1); console.log(getter1(0)); console.log(getter2(0)); console.log(getter3(0)); Note that, by definition, every Getter&lt;T&gt; is also automatically an Indexable&lt;T&gt; (since it is a function of the form (index: number) =&gt; T). This means that we can use Getters to create new Getters. There are also several utility functions for working with Getters. The first is Getter.constant() which takes in a value T and returns a thunk returning T (i.e. () =&gt; T). This is useful, for example, when T is an array and we always want to return the whole array (not just a single element): import { Getter } from &quot;./Getter&quot; const getter4 = Getter.constant([`A`, `B`, `C`]) console.log(getter4(0)) console.log(getter4(1)) Another useful utility function is Getter.proxy(), which takes a Getter and an array of indices as input and returns a new Getter which routes the access to the original values through the indices: import { Getter } from &quot;./Getter&quot; const getter5 = Getter.proxy([`A`, `B`, `C`], [2, 1, 1, 0, 0, 0]); console.log([0, 1, 2, 3, 4, 5].map(getter5)) This function becomes particularly useful when implementing other data structures such as Factors. 7.3.2.3 Dataframe In many data analytic workflows, a fundamental data structure is that of a two-dimensional table or dataframe. As mentioned in Section [REFERENCE], we can represent this data structure as either a dictionary of columns or a list of rows, with the column-wise representation having some advantages for analytical workflows. Thus, in plotscape, a Dataframe is a dictionary columns, with the extra twist that the columns don’t have to be fixed-length arrays but instead Indexables: interface Dataframe { [key: string]: Indexable } For example, the following is a valid instance of a Dataframe: const data = { name: [`john`, `jenny`, `michael`], age: [17, 24, 21], isStudent: true, canDrive: (index: number) =&gt; data.age[index] &gt; 18, }; Most functions in plotscape operate column-wise, however, here’s how the dataframe above would look like as a list of rows if we materialized using a hypothetical Dataframe.rows() function: import { Dataframe } from &quot;./Dataframe&quot; const data = { name: [`john`, `jenny`, `michael`], age: [17, 24, 21], isStudent: true, canDrive: (index: number) =&gt; data.age[index] &gt; 18, }; console.log(Dataframe.rows(data)) One important thing to mention is that, since a Dataframe can contain different Indexable subtypes as columns, we need to make sure that information about the number of rows is present and non-conflicting. That is, all fixed-length arrays in the data must have the same length, and, if there are non-fixed length variables (constants, derived variables/functions) present, we need to make sure that either at least one fixed-length array is included in the data or the non-fixed length arrays carry appropriate metadata. While in a traditional OOP style, these length constraints might be enforced as a class invariant during instantiation and within methods, my system adopts a more dynamic approach. Since a Dataframe is a simple dictionary, the length constraints are checked dynamically (via a Dataframe.checkLength() utility function). This occurs whenever the integrity of a Dataframe becomes a key concern, such as when initializing a Scene or when rendering. This style is more in line with JavaScript’s dynamic nature: in JavaScript, class instances are just objects, and there is nothing preventing the users from adding or removing properties at runtime. Further, dataframes with typical dimensionality (fewer columns than rows, \\(p &lt;&lt; n\\)), the performance cost of checking column’s length is negligible when compared to row-wise operations, such as rendering or summary statistics calculations. If performance were to become an issue for high-dimensional datasets (\\(p &gt;&gt; n\\)), the system could always be enhanced with memoization or caching. 7.3.2.4 Factors As discussed in Section 4, when visualizing, we often need to split our data into a set of disjoint subsets organized into partitions. Further, as mentioned in Section 4.2.4, these partitions may be organized in a hierarchy, such that multiple subsets in one partition may be unioned together to form another subset in a coarser, parent partition. Factors provide a way to represent such data partitions and the associated metadata. They are similar to base R’s factor S3 class, although there are some important differences which will be discussed below. Factor has the following interface: interface Factor&lt;T extends Dataframe&gt; { cardinality: number; indices: number[]; data: T parent?: Factor; } Here, cardinality represents the number of unique parts that a partitions consists of (e.g. 2 for a binary variable, 3 for a categorical variable with 3 levels, and so on). Data points map to the parts via a “dense” array of indices, which take on values in 0 ... cardinality - 1 and have length equal to the length of the data10. For instance, the following array of indices - [0, 1, 1, 0, 2, 0] - should be interpreted as meaning that the first part is composed of cases one, four, and six, the second part is composed of cases two and three, and the third part is composed of the case five (keeping in mind JavaScript’s zero-based indexing). The data associated with factor’s levels is stored in the data property, which is composed of arrays/Indexables with length equal to the factor’s cardinality. For instance, if a factor is created from a categorical variable with three levels - A, B, and C, then the rows the the data may look something like this: [{ label: \"A\" }, { label: \"B\" }, { label: \"C\" }]. Finally, the optional parent property denotes a factor representing the parent partition. There are a couple of important things to discuss. First, cardinality technically represents the same information as could be obtained by counting the number of unique values in indices, however, for many operations on Factors, it is beneficial to be able to access the cardinality in constant \\(O(1)\\) rather than linear \\(O(n)\\) time that would result from having to loop through the indices. Such is the case, for example, when constructing product factors or when initializing arrays of summaries. Of course, care must be taken to ensure that cardinality and indices stay synchronized under factor transformations. Second, the part’s metadata is stored in the data of type Dataframe. This represents a departure from e.g. base R’s factor class, where all metadata is stored as a flat vector of levels. For instance: cut(1:10, breaks = c(0, 5, 10)) ## [1] (0,5] (0,5] (0,5] (0,5] (0,5] (5,10] (5,10] (5,10] (5,10] (5,10] ## Levels: (0,5] (5,10] With Factor, the same information would be represented as: const factor: Factor = { cardinality: 2, indices: [0, 0, 0, 0, 0, 1, 1, 1, 1, 1], data: { binMin: [0, 5], binMax: [5, 10], }, }; Storing Factor metadata in a Dataframe offers several advantages as opposed to a flat vector/array. First, when partitioning data, we often want to store several distinct pieces of metadata. For example, when we bin numeric variable, like in the example above, we want to store both the lower and upper bound of each part’s bin. cut stores the multiple (two) pieces of metadata as a tuple, however, this approach becomes cumbersome when the dimensionality of the metadata grows. Further, metadata stored in a Dataframe becomes far easier to combine when taking a product of two factors. Since taking products of factors is a fundamental operation, underpinning features such as linked brushing, it makes sense to use metadata representation which facilitates this operation. While all Factors share the same fundamental structure - a data partition with associated metadata - factors can be created using various constructor functions. These constructor functions differ in what data they take as input and what metadata they store on the ouput, giving rise to several distinct Factor subtypes. These will be the subject of the next few sections. 7.3.2.4.1 Bijection and constant factors Factor.bijection and Factor.constant are two fairly trival factor constructor. Factor.bijection creates the finest possible data partition by assigning every case to its own part, whereas Factor.constant does the opposite and assigns all cases to a single part. The names of the reflect the mathematical index mapping functions: the bijective function \\(f(i) = i\\) for Factor.bijection and the constant function \\(f(i) = 0\\) for Factor.constant. Consequently, the cardinality of Factor.bijection is equal to the length of the data, while the cardinality of Factor.constant is always one. Both can be assigned arbitrary metadata, which must have length equal to the cardinality. Both functions have essentially the same signature: function bijection&lt;T extends Dataframe&gt;(n: number, data?: T): Factor&lt;T&gt; function constant&lt;T extends Dataframe&gt;(n: number, data?: T): Factor&lt;T&gt; In both cases, n represents the length of the data (the number of cases) and data represents arbitrary metadata. The variable n is used to construct an array of indices, which in the case of Factor.bijection is an increasing sequence starting at zero ([0, 1, 2, 3, ..., n - 1]) whereas in the case of Factor.constant it is simply an array of zeroes ([0, 0, 0, 0, ..., 0]). Technically, in this case, having an explicit array of indices is not necessary, and we could implement much of the same functionality via a callback (i.e. (index: number) =&gt; index for Factor.bijection and (index: number) =&gt; 0 for Factor.constant). However, for many operations involving factors, it is necessary to store the length of the data (n), and while it would be possible to define a separate n/length property on Factor, in the context of other factor types, I found it simpler to allocate the corresponding array. While this does have some small memory cost, there is no computational cost involved, since, by definition, the partition represented by a bijection or constant factor does not change11. Factor.bijection and Factor.constant their own specific use cases. Factor.bijection represents a one-to-one mapping and this makes it suitable for scatterplots and parallel coordinate plots. In contrast, Factor.constant is useful when we want to compute summaries across the entirety of the data, such as is the case for spinogram12. As a final interesting side-note, both Factor.bijection and Factor.constant can be interpreted through the lense of category theory, as terminal and initial objects within the category of data partitions, with morphisms representing products between partitions. That is, the product of any factor with a Factor.bijection always yields another Factor.bijection (making this a terminal object), whereas the product of any factor with Factor.constant will simply yield that factor (making this an initial object). 7.3.2.4.2 Discrete factors Another fairly intuitive factor constructor is Factor.from. It simply takes an array of values which can be coerced to string labels (i.e. have the .toString() method) and creates a discrete factor by treating each unique label as a factor level (this is essentially what base R’s factor class does). This gives rise to the following function signature: type Stringable = { toString(): string }; function from(x: Stringable[], options?: { labels: string[] }): Factor&lt;{ label: string[] }&gt; When creating a discrete factor with Factor.from, the resulting factor’s length matches the input array x. To compute the factor indices, the constructor needs to be either provided with an array of labels or these will be computed from x directly (by calling the .toString() method and finding all unique values). Assigning indices requires looping through the \\(n\\) values x and further looping through \\(k\\) labels, resulting in \\(O(n)\\) time complexity (assuming \\(k\\) is constant with respect to the size of the data). The factor metadata simply contains this array of labels (singular form label is used since it is the name of a dataframe column). Each index in indices then simply maps to one label. Finally, for easier inspection, labels may be sorted alphanumerically, though this does not affect computation in any way. The typical use case for Factor.from is the barplot. Here, we take an array of values, coerce these to string labels (if the values were not strings already), find all unique values, and then create an array of indices mapping the array values to the unique labels. The indices can then be used to subset data when computing summary statistics corresponding to the barplot bars. 7.3.2.4.3 Binned factors Arrays of continuous values can be turned into factors by binning. Factor.bin is the constructor function used to perform this binning. It has the following signature: type BinOptions = { breaks?: number[]; nBins?: number; width?: number; anchor?: number; } function bin(x: number[], options?: BinOptions): Factor&lt;{ binMin: number[]; binMax: number[] }&gt;; Again, as in the case of Factor.from, the length of the factor created with Factor.bin will match the length of x. To compute the factor indices, the values in x need to be assigned to histogram bins delimited by breaks. The breaks are computed based on either default values or the optional list of parameters (options) provided to the construct function. Note that the parameters are not orthogonal: for instance, histogram with a given number of bins (nBins) cannot have an arbitrary binwidth (width) and vice versa. Thus, if a user provides multiple conflicting parameters, they are resolved in the following order: breaks &gt; nBins &gt; width. Finally, the metadata stored on the Factor.bin output includes the limits of each bin binMin and binMax, giving the lower and upper bound of each bin, respectively. Indices are assigned to bins using a half-open intervals on breaks of the form (l, u], such that a value v is assigned to to a bin given by (l, u] if it is the case that l &lt; v &lt;= u. Assigning indices to bins requires looping through the \\(n\\) values of x, and further looping through \\(k\\) breaks13, resulting in \\(O(n)\\) time complexity (assuming \\(k\\) is fixed with respect to the size of the data). An important point to mention is that a naive approach of assigning bins to cases may lead to some bins being left empty, resulting in cardinality which is less than the number of bins and “sparse” indices (gaps in index values). For instance, binning the values [1, 2, 6, 1, 5] with breaks [0, 2, 4, 6] leaves the second bin ((2, 4]) empty, and hence the corresponding index value (1) will be absent from indices. To address this, plotscape performs an additional \\(O(n)\\) computation to “clean” the indices and ensure that the array is dense (i.e. indices take on values in 0 ... cardinality - 1, and each value appears at least once). While this additional computation may not be strictly necessary (i.e. some other systems may use “sparse” factor representation), I found the dense arrays of indices much easier to work with, particularly when it comes to operations like combining factors via products and subsetting the corresponding data. Further, even though this approach necessitates looping over indices twice, the combined operation still maintains an \\(O(n)\\) complexity. 7.3.2.4.4 Product factors As discussed in Section 4.2.3.2, a fundamental operation that underpins many popular types of visualizations, particularly when linked selection is involved, is the Cartesian product of two partitions. That is, assuming we have two Factors which partition our data into parts, we can create a new Factor consists of all unique intersections of those parts. To illustrate this idea better, take two factors represented by the following data (the data property is omitted for conciseness): { cardinality: 2, indices: [0, 0, 1, 0, 1, 1] }; { cardinality: 3, indices: [0, 1, 2, 0, 1, 2] }; If we take their product, we should end up with the following factor14: { cardinality: 4, indices: [0, 1, 3, 0, 2, 3] }; There are a couple of things to note here. First, note that the cardinality of the product factor (4) is greater than either of the cardinalities of the constituent factors (2, 3), but less than the product of the cardinalities (\\(2 \\cdot 3 = 6\\)). This will generally be the case: if the first factor has cardinality \\(a\\) and the second cardinality \\(b\\), the product will have cardinality \\(c\\), such that: \\(c \\geq a\\) and \\(c \\geq a\\)15 \\(c \\leq a \\cdot b\\)16 This is all fairly intuitive, however, actually computing the indices of a product factor presents some challenges. A naive idea might be to simply sum/multiply pairs of indices element-wise, however, this approach does not work: the sum/product of two different pairs of indices might produce the same value (e.g. in a product of two factors with cardinalities of \\(2\\) and \\(3\\), there are two different ways to get \\(4\\) as the sum of indices: \\(1 + 3\\) and \\(2 + 2\\)). Further, when taking the product of two factors, we may want to preserve the factor order, in the sense that cases associated with lower values of the first factor should get assigned lower indices. Because sums and products are commutative, this does not work. One crude solution shown in Section 4.2.3.2 is to treat the factor indices as strings and concatenate them elementwise. This works, but produces an unnecessary computational overhead. There is a better way. Assuming we have two factors with cardinalities \\(c_1\\) and \\(c_2\\), and two indices \\(i_1\\) and \\(i_2\\) corresponding to the same case, we can compute the product index \\(i_p\\) via the following formula: \\[\\begin{align} k &amp;= max(c_1, c_2) \\\\ i_{\\text{p}} &amp;= k \\cdot i_{\\text{1}} + i_{\\text{2}} \\tag{7.1} \\end{align}\\] This formula is similar to one discussed in Wickham (2013). Since \\(i_1\\) and \\(i_2\\) take values in \\(0 \\ldots c_1 - 1\\) and \\(0 \\ldots c_2 - 1\\), respectively17, the product index is guaranteed to be unique: if \\(i_1 = 0\\) then \\(i_p = i_2\\), if \\(i_1 = 1\\) then \\(i_p = k + i_2\\), and so on. Further, given a product index \\(i_p\\), we can recover the original indices: \\[\\begin{align} i_1 &amp;= \\lfloor i_p / k \\rfloor \\\\ i_2 &amp;= i_p \\mod k \\end{align}\\] This is useful when constructing the product factor data: we need to take all unique product indices and use them to proxy the data of the original two factors. Finally, since the the index corresponding to the first factor is multiplied by \\(k\\), it intuitively gets assigned a greater “weight” and the relative order of the two factors is preserved. See for example the following table of product indices of two factors with cardinalities 2 and 3: i_1 i_2 i_product 0 0 0 0 1 1 0 2 2 1 0 3 1 1 4 1 2 5 It should be mentioned that, as with binning, computing product indices based on Equation (7.1) creates gaps. Again, plotscape solves this by keeping track of the unique product indices and looping over the indices again to “clean” them, in \\(O(n)\\) time. Further, since we want to retain factor order, plotscape also sorts the unique product indices before running the second loop. Hypothetically, with \\(n\\) data points, there can be up \\(n\\) unique product indices (even when \\(c_1, c_2 &lt; n\\)), and so this sorting operation makes creating and updating product indices \\(O(n \\cdot \\log n)\\) worst-case time complexity. However, I contend that, most of the time, the length of the unique product indices will be a fraction of the length of the data, and, further, profiling during development did identify this operation as a computational bottleneck. If sorting did turn out to be a bottleneck, there may be sub-\\(O(n \\cdot \\log n)\\) algorithms which still preserve the factor order; however, I did not spend time trying to come up with such an algorithm. Finally, Factor.product is the only factor constructor which actually assigns to the parent property of the output Factor. Specifically, the first factor always gets assigned as the parent of the product, creating a hierarchical structure. Technically, there are situations where a product of two factors is simply a “flat” product and not a hierarchy. This is the case, for example, when taking the product of two binned variables in a 2D histogram - the two factors are conceptually equal. However, in practice, this distinction rarely matters. Any computation involving a flat product can simply ignore the parent property, and the data is for all other intents and purposes equivalent. This similarity of flat and hierarchical products was also noted by Wilkinson, who used the terminology of cross and nest operators (Wilkinson 2012, 61). 7.3.2.5 Marker An important component which also serves the function of a Factor but deserves its own section is the Marker. Marker is used to represent group assignment during linked selection, making it a key component of plotscape. Moreover, while most of the components discussed so far can exist within the context of a single plot, Marker further differs by the fact that it is shared by all plots within the same figure. Marker has the following Factor-like interface which will be gradually explained below18: interface Marker { cardinality: number; indices: number[]; data: { layer: number[] }; transientIndices: number[]; } However, before delving into this interface, let’s first discuss some important theoretical concepts related to Marker. 7.3.2.5.1 Transient vs. persistent selection A key concept in the implementation of Marker is the distinction between transient and persistent selection (see also Urbanek 2011). By default, plotscape plots are in transient selection mode, meaning that linked selection operations (e.g. clicking, clicking-and-dragging) assign cases the transient selection status. This transient selection status is cleared by subsequent selection events, as well as many other interactions including panning, zooming, representation switching, and change of parameters. To make the results of selection persist across these interactions, the user can assign cases to persistent selection groups (currently, this is down by holding down a numeric key and performing regular selection). Persistent selections are removed only by a dedicated action (double-click). Importantly, a single data point can be simultaneously assigned to a persistent group and also be transiently selected. For example, a data point may belong to the base (unselected) group, transiently selected base group, persistent group one, transiently selected persistent group one, and so on. This means that plotscape has a very minimal version of selection operators (see Unwin et al. 1996; Theus 2002) baked-in: transient and persistent selection are automatically combined via intersection (the AND operator). While a full range of selection operators provides much greater flexibility, I contend that this simple transient-persistent model already provides a lot of practical utility. Specifically, it enables a common action in the interactive data visualization workflow: upon identifying an interesting trend with selection, a user may be interested in how the trend changes when conditioning on another (AND) subset of the data. The transient-persistent model makes this possible, without introducing the overhead having to learn how to use various selection operators. Figure 7.3: A screenshot of a plotscaper figure showing five different selection group assignments: base/no selection (light blue), group one (light green), group two (light red/pink), transiently selected group one (dark green), and transiently selected group two (dark red). The three remaining group assignments - transiently selected base, group three, transiently selected group three - are not shown (to prevent the figure from becoming too busy). Finally, note that, in the barplot (right), the transiently selected bar segments appear at the base of the bars. Final thing to note is that, during rendering, transient selection status is represented by paired colour shades, see Figure 7.3. That is, each persistent selection group is assigned a unique color, and transient selection is indicated by a darker shade of the same color. This pairing clearly visually differentiates the two selection modes while preserving the identity of the persistent groups. 7.3.2.5.2 Group assignment indices The key responsibility of Marker is manage linked selection status (the transient-persistent model discussed in Section 7.3.2.5.1 above). It does this very similarly to a Factor, by maintaining an array of group assignment indices. In plotscape, each data point can be assigned one of four primary (persistent) groups based on linked selection: base (no selection), persistent selection group one, persistent selection group two, and persistent selection group three. Further, as also discusssed above, each of those groups can be further split in two based on transient selection state (selected or unselected), resulting in \\(4 \\times 2 = 8\\) distinct group assignments (such that the cardinality of Marker is always 8). These group assignments are coded into indices as follows: group index Base 7 Group one 6 Group two 5 Group three 4 Transient base 3 Transient group one 2 Transient group two 1 Transient group three 0 As you can see from the table above, selection groups are mapped to indices in reverse order. Transient group indices have lower values than persistent ones, and persistent groups are sorted in descending order. This reverse ordering is designed to simplify stacking operations. Group assignment inherently implies importance or salience, and, and during stacking, we often want the more “important” selected groups have to higher precedence than less significant, unselected ones (see also Urbanek 2011). For instance, in stacked barplots, the visually salient highlighted bar segments are generally placed at the base (y-axis intercept), and likewise, in dotplots, the highlighted circles are placed at the center, meaning that both have to come first in the aggregation order. Having the group indices coded in reverse order eliminates the need for sorting during later stages of the data visualization pipeline. With four persistent groups (including base), many operations involving transient selection can be efficiently implemented using the third bit of the index. Specifically, as can be seen in table [REFERENCE], persistent indices range from 0 to 3 and transient indices range from 4 to 7. Thus, to determine whether a case is selected, we can do a simple bitwise check of the third bit. Likewise, to add or remove transient selection, we can set the third bit appropriately. It is important to mention that, while not a necessity, having four persistent groups works well when for the main purpose of Markers: taking products with other factors. With four groups, the number of unique group assignments is eight and the maximum index values is 2^3 - 1 = 7, resulting in dense Marker indices (i.e. all index values from 0 to cardinality - 1 are valid). If we wanted to implement more persistent selection groups, we could do so by setting a higher bit (e.g. the fourth bit for up to eight persistent groups), however, we may break the correspondence between cardinality and the number of unique indices. Practically, this may not cause any issues, however, as long as we ensure that the Marker cardinality is set to \\(2^{\\text{# of the transient bit}}\\) which may be different from the number of unique indices. However, I argue that, for most intents and purposes, four persistent selection groups are plenty. It is well-known that human visual capacity is limited, and having more than \\(4 \\times 2 = 8\\) different group assignments represented by different colors may overtax the visual system. Beyond that, the ability to have arbitrary number of persistent groups may be implemented in future versions in plotscape but has not been a priority up to this point. 7.3.2.5.3 Updating group assignment indices As may be already apparent, a key operations that marker has to support have to do with updating the group assignment indices. The Marker namespace exports the following three functions for this very purpose: namespace Marker { export function clearAll(marker: Marker): void {} export function clearTransient(marker: Marker): void {} export function update(marker: Marker, group: number, selectedIndices: number[]): void {} } Marker.clearAll clears all group assignments by setting them all indices to the base value (7). Marker.clearTransient clears transient indices only. This is also where the transientIndices property on the Marker interface comes handy. Since transient selection is removed by any selection group assignment, we know that the indices that were assigned to transient selection last are all the transient indices there are. As such, we can optimize the removal of transient selection removal by simply keeping track of which indices were assigned to transient selection last, and once Marker.clearTransient is called, simply iterating through these indices (instead of the whole array of indices). Finally, the Marker.update is the key function for Marker. It changes behavior based on the group argument. When group is one of the persistent groups, it simply iterates through selectedIndices, setting each corresponding index in the marker’s indices array to the group’s value. When group has a special transient selection value, it instead adds the transient selection status to each marker index via the bitwise operation described in Section 7.3.2.5.2. Finally, it should be mentioned that, since any selection group assignment removes transient selection, Marker.update always first calls Marker.clearTransient. 7.3.2.6 Aggregation: Reducers, reduceds, and summaries As discussed in Section 4.3, a key challenge when computing statistical summaries in interactive data visualization systems is doing so in a way that preserves the inherent hierarchy in the data. For instance, when stacking bar segments in a barplot that has been partitioned via linked selection, we want the stacking operation to respect the inherent hierarchical relationships between the segments and the bars. Further, as also demonstrated in Section 4.3, this setup suggests particular algebraic structures: groups and monoids. Reducers and associated data structures and procedures provide a way of addressing this challenge. 7.3.2.6.1 Reducers A Reducer&lt;T&gt; is simply a data container that stores functionality and metadata associated with a monoid on type T. It has the following interface: // Reducer.ts interface Reducer&lt;T&gt; { name: string; initialfn: () =&gt; T; reducefn: (prev: T, next: T) =&gt; T; } That is, a Reducer&lt;T&gt; has a name, a zero-argument function (or a “thunk”) producing a value of type T, and a binary function which takes two values of type T and produces another T. The Reducer namespace exports these Reducers. Here are a couple of examples: export namespace Reducer { export const sum: Reducer&lt;number&gt; = { name: `sum`, initialfn: () =&gt; 0, reducefn: (x, y) =&gt; x + y, }; export const product: Reducer&lt;number&gt; = { name: `product`, initialfn: () =&gt; 1, reducefn: (x, y) =&gt; x * y, }; export const max: Reducer&lt;number&gt; = { name: `max`, initialfn: () =&gt; -Infinity, reducefn: (x, y) =&gt; Math.max(x, y), }; export const concat: Reducer&lt;string&gt; = { name: `concat`, initialfn: () =&gt; ``, reducefn: (x, y) =&gt; x + y, // (string concatenation also uses `+` operator in JavaScript) }; } For reasons discussed in Section 4.3, a Reducer should be a monoid, meaning that the operation represented by reducefn should be associative, and unital (with respect to initialfn). That is, the following should hold: reducefn(reducefn(a, b), c) === reducefn(a, reducefn(b, c)) reducefn(a, initialfn()) === reducefn(initialfn(), a) === a These constraints are not actually enforced by the system: as discussed in Section 4.3.2.6, they would need to be checked with all possible inputs, which is not practical for large input domains such as floating point numbers or strings. However, the Reducer interface should hopefully be evocative of monoids, to users familiar with functional programming. 7.3.2.6.2 Reduced We can use a Reducer&lt;T&gt; to aggregate or fold an array of values of type T (T[]) over a Factor, resulting in an array of values of type T of the same length as the Factor’s cardinality. The underlying procedure is fairly straightforward and looks something like this: function reduce&lt;T&gt;(x: T[], factor: Factor, reducer: Reducer&lt;T&gt;): T[] { const result = Array&lt;T&gt;(factor.cardinality); // Initialize values for (let j = 0; j &lt; factor.cardinality; j++) { result[j] = reducer.initialfn(); } // Aggregate for (let i = 0; i &lt; factor.indices.length; i++) { const index = factor.indices[i]; result[index] = reducer.reducefn(result[index], x[i]); } return result; } After we apply this operation, we could just return the result array T[] as in the example above, however, this is limited. Specifically, as discussed in Section 4.3, in a data visualization system, we rarely want to treat summaries as a simple “flat” array of values. Instead, in order to correctly apply operations like stacking and normalizing, it is necessary for the summaries to “remember” how they have been summarized, over what partition, and which other summaries they relate to, within the hierarchy of summaries. This is what the Reduced data type is for19: interface Reduced&lt;T = unknown&gt; extends Array&lt;T&gt; { factor: Factor&lt;T&gt;, reducer: Reducer&lt;T&gt;, parent?: Reduced&lt;T&gt; original?: Reduced&lt;T&gt; } The way we use Reduced is that we replace the return type of the reduce function, and then, inside the function’s body, we simply assign the Factor and Reducer&lt;T&gt; arguments as the factor and reducer properties to the result, after it’s been computed: function reduce&lt;T&gt;(x: T[], factor: Factor, reducer: Reducer&lt;T&gt;): Reduced&lt;T&gt; { ... result.factor = factor; result.reducer = reducer; return result } That is, now we have the factor and reducer stored directly alongside array of summaries for future use when applying later transformations like stacking and normalization. This is also where the optional parent and original properties come in. The parent property refers to an array of summaries on the parent partition/Factor, whereas original refers to the original array of summaries, before any transformations like stacking/normalization have been applied (this is primarily useful when querying). Speaking of stacking and normalization, the Reduced namespace also exposes the stack, normalize, and shiftLeft functions with the following type signatures: export namespace Reduced { export function stack&lt;T&gt;(reduced: Reduced&lt;T&gt;): Reduced&lt;T&gt; {} export function normalize&lt;T&gt;(reduced: Reduced&lt;T&gt;, normalizefn: (x: T, y: T) =&gt; T): Reduced&lt;T&gt; {} export function shiftLeft&lt;T&gt;(reduced: Reduced&lt;T&gt;): Reduced&lt;T&gt; {} } These functions all take Reduced as the first argument and compute transformations that make use of the factor, reducer, and parent properties, returning a new Reduced. For instance, the stack function uses the factor property and its corresponding parent property to create an array of indices over which to “stack” the Reduced values, and then does exactly this, using the reducer. Similarly, the normalize function also creates a parent-factor dependent array of indices, and applies a binary function using the values of Reduced and parent, for instance, dividing each value by its parent values (such as is the case in a spineplot or spinogram). The shiftLeft function simply appends the neutral element (reducer.initialfn()) as the first element and removes the last element of Reduced, essentially “shifting” the values leftwards (again, this is used in a spinogram). 7.3.2.6.3 Summaries With the building blocks of Indexables, Factors, Reducers and Reduceds, we now have everything we need to compute the hierarchy of summaries described in Section 4.3 and more specifically Section 4.3.2.3. This is the exact purpose of the constructor function exported from the Summaries namespace:20: type ReducerTuple&lt;T = unknown&gt; = [Indexable&lt;T&gt;, Reducer&lt;T&gt;]; function create(summaries: Record&lt;string, ReducerTuple&gt;, factors: Factor[]): Dataframe[] The Summaries.create function takes in a dictionary of Indexable-Reducer pairs with matching generic and an array of Factors. It then repeatedly combines the factors pair-wise using Factor.product and then computes summaries for each new product factor using the Indexable-Reducer pairs in summaries. This results in an array of Dataframes with equal length as factors, holding the combined data. For example: const factor1 = Factor.from([`a`, `a`, `b`, `a`, `b`, `c`]) const factor2 = Factor.product(factor1, Factor.from([0, 0, 0, 1, 1, 1])) const summaries = { stat: [[10, 12, 9, 15, 11, 10], Reducer.sum] } const summarized = Summaries.create(summaries, [factor1, factor2]) Dataframe.rows(summarized[0]) // [ { label: `a`, stat: 37 }, { label: `b`, stat: 20 }, { label: `c`, stat: 10 } ] Dataframe.rows(summarized[1]) // [ { label: `a`, label$: `0`, stat: 22 }, { label: `a`, label$: `1`, stat: 15 }, ... ] Thus, the overt behavior of the Summaries.create() is to compute summaries using Reducers and combine the resulting data with the corresponding Factor data. However, under the hood, the function also does a couple more things. First, it links all reduced variables created from summaries with their parent-data counterpart (by assigning the parent property on the Reduceds). It also sets up a reactive graph, such that when a factor is updated, its corresponding data is updated, and so are its child factors and their corresponding data. Finally, to actually render the summarized data, the summaries need to be translated to aesthetic mappings. This happens via the Summaries.translate() function. Again, overtly, this function simply maps the array of Dataframes, however, under the hood it also manages reactivity. For instance, the following is a translating function in the definition of a histogram, taken almost directly from the codebase: const zero = () =&gt; 0; const coordinates = Summaries.translate(summarized, [ (d) =&gt; d, (d) =&gt; ({ x0: d.binMin, y0: zero, x1: d.binMax, y1: d.stat, }), (d) =&gt; ({ x0: d.binMin, y0: zero, x1: d.binMax, y1: Reduced.stack(d.stat), }), ]); Notice that the translating function actually maps three data sets. The first, representing a summary on the whole data set, is not used directly in histograms but it utility will be explained below. The second, representing summaries across histogram bins (whole bars), is used for axis limits and collision detection during selection. The third, representing summaries across the product of histogram bins and Marker status (bar segments), is used for rendering. Notice that, in the case of the y1 variable in the third data set, we use the Reduced.stack() function to stack the bar segments on top of each other, within the appropriate factor level and using the associated Reducer. The reason why the histogram data is summarized across three partitions - constant, bins, bins and marker - is to facilitate seamless transition between histograms and spinograms. Histograms and spinograms represent largely the same underlying data (summaries computed across bins), however, spinograms additionally require a whole-dataset summary to establish the upper (right) x-axis limit. By pre-computing summaries on the constant partition, we enable switching between histograms and spinograms without having to re-calculate the summaries, requiring only change in translation/aesthetic mapping. For illustration, here’s how the translation of the same underlying data into a spinogram looks like in plotscape: const coordinates = Summaries.translate(summarized, [ (d) =&gt; d, (d) =&gt; ({ x0: Reduced.shiftLeft(Reduced.stack(d.stat)), y0: zero, x1: Reduced.stack(d.stat), y1: one, }), (d) =&gt; ({ x0: Reduced.shiftLeft(Reduced.stack(Reduced.parent(d.stat))), y0: zero, x1: Reduced.stack(Reduced.parent(d.stat)), y1: Reduced.normalize(Reduced.stack(d.stat), (x, y) =&gt; x / y), }), ]); 7.3.2.7 Scales As discussed in Section 4.4, to visualize data, we need to translate values from the domain of the data to the domain of the graphical device (computer screen). In many data visualization packages, this work is done by specialized components called scales. The theoretical model of scales plotscape uses was already discussed in Section 4.4, with the key takeway being that a scales are fundamentally composed of two components: its domain and codomain. In Section 4.4.0.4, I briefly discussed the fact that many popular data visualization systems treat the domain and codomain as fundamentally different entities. I argued that this creates unnecessary complexity, and instead suggested a unified model where both the domain and codomain are represented by the same types. In this section, I will discuss the plotscape’s implementation of scales, Scale. In Section 4.4, I mentioned that a key idea for creating a generic model of scales involves conceptualizing scales as composed of two components, each translating values to and from their respective domain and the real numbers \\(\\mathbb{R}\\). In plotscape, these components are called Expanses and will be discussed later, in Section 7.3.2.8. However, for now, assume we have some generic type Expanse&lt;T&gt; and a corresponding namespace which exposes two functions: interface Expanse&lt;T = unknown&gt; {} export namespace Expanse { export function normalize&lt;T&gt;(expanse: Expanse&lt;T&gt;, x: T): number {} export function unnormalize&lt;T&gt;(expanse: Expanse&lt;T&gt;, x: number): T {} } Now, assuming we have some type and namespace Expanse that behaves this way, we can define the Scale interface as follows: interface Scale&lt;T, U&gt; { domain: Expanse&lt;T&gt;; codomain: Expanse&lt;U&gt;; zero: number; one: number; direction: 1 | -1; scale: number; mult: number; } The two primary properties of Scale are domain and codomain, both of type Expanse. The utility of the other properties relates to concepts discussed in Section 4.4.0.3.2 and will be discussed later. However, for now, we can focus solely on domain and codomain. These two properties are sufficient to implement the core functionality of Scale: the pushforward and pullback functions. To explain what these functions do, it may be easiest to just show the code: namespace Scale { export function pushforward&lt;T, V&gt;(scale: Scale&lt;T, V&gt;, x: T): V { const { domain, codomain } = scale; return Expanse.unnormalize(codomain, Expanse.normalize(domain, x)); } export function pullback&lt;T, V&gt;(scale: Scale&lt;T, V&gt;, x: V): T { const { domain, codomain } = scale; return Expanse.unnormalize(domain, Expanse.normalize(codomain, value)) } } The Scale.pushforward function takes as its arguments a Scale&lt;T, U&gt; and a value of type T (corresponding to the scale’s domain). It then converts this value to a number, using Expanse.normalize with domain as the first argument. Finally, it converts this number back to the value of type V (corresponding to the scale’s codomain), using Expanse.unnormalize, with the codomain as the first argument. Conversely, the Scale.pullback function performs the reverse operation: it first normalizes the value within codomain and then unnormalizes it within the domain. In this way, Scale.pullback acts as an inverse mapping of the type discussed in Section 4.4.0.3.4. Put simply, the Scale acts as just a kind of connector or “plumbing” between its corresponding domain and codomain, converting values from one space to the other. Thus, most of the heavy lifting related to scaling is done by the domain and codomain properties, both of type Expanse. However, before we go on to discuss Expanse, we should explore the remaining Scale properties. 7.3.2.7.1 Scale properties Besides domain and codomain, scale also has five numeric properties: zero, one, direction, scale, and mult. These properties act as parameters of the (implicit) intermediate domain. They facilitate a range of domain-and-codomain-agnostic scale operations, including zooming, panning, growing/shrinking, and reversing, and are applied during calls to Scale.pushforward or Scale.pullback. For instance, here’s how a “more full”21 implementation of Scale.pushforward looks like: export function pushforward&lt;T, U&gt;(scale: Scale&lt;T, U&gt;, x: T): U { const { domain, codomain, zero, one, direction, scale, mult } = scale let normalized = Expanse.normalize(domain, x); // Normalize to [0, 1] normalized = normalized * scale * mult; // Grow/shrink normalized = zero + normalized * (one - zero); // Apply margins normalized = 0.5 * (1 - direction) + direction * normalized; // Apply direction return codomain.unnormalize(normalized); } Let’s break down how these properties affect Scale.pushforward. First, normalized values get multiplied by the scale and mult: normalized = normalized * scale * mult; Together, these scale and mult jointly define a multiplicative factor that gets applied to all values during scaling. The reason for separating them into two properties is to allow for both default and interactive scaling. Often, we want to apply a default scaling factor (scale) while still enabling users to dynamically adjust the overall magnitude of multiplication (mult). For instance, in a barplot, bars might have a default maximum width corresponding to a fraction of the total plot width (scale), but users should still be able to interactively resize them (via mult). Second, as discussed in Section 4.4.0.3.2, the zero and one properties apply proportional margins to the normalized values: normalized = zero + normalized * (one - zero); For instance, assuming that by Expanse.normalize maps the “minimum” and “maximum” values of the domain to 0 and 1, respectively, setting zero to 0.1 and one to 0.9 expands the scale and applies a symmetric 10% margin on both ends. Importantly, this expansion is consistent across all concrete Expanse subtypes used for the domain and codomain. Further, as also discussed in Section 4.4.0.3.2, incrementing both values by the same amount effectively shifts the scale. Consequently, the implementation of panning is as simple as: export function move(scale: Scale, amount: number): void { scale.zero += amount; scale.one += amount; } Again, this implementation of panning is also domain-and-codomain-agnostic. Further, it can result in zero and one values outside of \\([0, 1]\\), which is fine since, as mentioned in Section 4.4.0.3.2, at times we may want to map points outside of the codomain (e.g. scatterplot points with x- and y- coordinates outside of the plot but with plot-overlapping radius). Finally, we can also use the zero and one properties to zoom into a specific region of the scale, using the Scale.expand function. The logic of this function is a bit more complicated, but can be understood with careful application of (highschool) algebra: function invertRange(min: number, max: number): number { const rangeInverse = 1 / (max - min); return [-min * rangeInverse, (1 - min) * rangeInverse]; } export function expand(scale: Scale, zero: number, one: number): void { const { zero: currentZero, one: currentOne, direction } = scale; const currentRange = currentOne - currentZero; // Reflect if direction is backwards if (direction === -1) [zero, one] = [1 - zero, 1 - one]; // Normalize the zoom values within the current range [zero, one] = [zero, one].map((x) =&gt; (x - currentZero) / currentRange); [zero, one] = invertRange(zero, one); // Reflect back if (direction === -1) [zero, one] = [1 - zero, 1 - one]; scale.zero = zero scale.one = one } The Scale.expand function zooms into a proportional region of the scale, using the two numeric arguments as boundaries of this region. For instance, Expand.scale(scale, 0.25, 0.75) zooms into the middle 50% of the scale. The key part of this operation is the invertRange function. This takes a numeric interval given by \\([\\min, \\max]\\) and inverts this, such that, when the simple linear normalizing function \\(n(x) = (x - \\min) / (\\max - \\min)\\) is applied to 0 and 1 with the new values, the result will be \\(n(0) = a\\) and \\(n(1) = b\\), respectively. The rest of the Scale.expand function is just book-keeping, handling the case where zero, one, and direction have been set to non-default values. Third and finally, the direction property gets applied to the normalized values in the following way: normalized = 0.5 * (1 - direction) + direction * normalized; The line of code above is essentially a branchless conditional. When direction is 1, then the first term in the sum evaluates to zero and the result is just the value normalized. Conversely, when direction is -1, the first term evaluates to one and the result is 1 - normalized, effectively reversing the normalized value. This approach avoids explicit branching, which can be expensive, particularly when the branches are unpredictable. Although the JavaScript engine may be able to optimize a simple if/else statement if it was used here, and branch prediction would likely be accurate within long-running loops, this micro-optimization may nevertheless be useful since scaling represents a “hot” code path: it has to occur during any rendering/selection. With this detailed explanation, the behavior of Scale.pushforward should be clear. The Scale.pullback function follows a similar pattern, but with inverse transformations applied in reverse order (i.e. direction first, then zero and one, and finally scale and mult). This concludes our examination of the Scale class. We will now proceed to discuss the type and behavior of its domain and codomain components: Expanses. 7.3.2.8 Expanses As discussed in the previous section, a Scale maps values between two spaces, defined by the domain and codomain properties, both of type Expanse. In other words, a Scale primarily acts as a sort of a connector, while the Expanse properties handle the core underlying transformations. Specifically, Expanse&lt;T&gt; is a generic type designed to convert values of type T to and from numerical representations, a process facilitated by the previously mentioned normalize and unnormalize functions. Beyond these core functions, it may be also beneficial to have additional generic functionality on each Expanse&lt;T&gt; subtype, namely the ability to “train” the expanse on new values and return a list of potential axis breaks. This leads to the following interface: Note that, up until this point, interfaces have been used to define components as a plain data containers, however, with Expanse&lt;T&gt;, it is now used differently: to define abstract functionality. The reason for this is that Expanse&lt;T&gt; is the first component of the system which absolutely requires runtime polymorphism. Specifically, the implementation of normalize and unnormalize has to vary significantly across different Expanse&lt;T&gt; subtypes, by necessity. For instance, converting numbers to numbers is a fundamentally different task than converting strings to numbers. Further, even when the same generic type T is used, there may be use cases for different implementations of the conversion process (as we will explore later). This makes polymorphism necessary. In current version of plotscape, I maintain the data-oriented approach of separate data-behaviour containers and hand-roll the polymorphism by dispatching on a type property, however, a case could be made for adopting object-oriented principles here and writing expanse subtypes as classes which implement the Expanse&lt;T&gt; interface (or even implementing Expanse&lt;T&gt; as an abstract class). This is really all there is to say about the Expanse&lt;T&gt; interface. The real functionality lies within its subtypes, which we will now explore. 7.3.2.9 Continuous expanses The most straightforward type of expanse is ExpanseContinuous, implementing an interface of Expanse&lt;number&gt;. It generalizes the linear mapping discussed in Section 4.4.0.1, values to and from a range given by [min, max]. Typically (but not always), this amounts to mapping a data value equal to min to zero and data value equal to max to 1, with intermediate values interpolated to (0, 1). Here is a simplified interface for the ExpanseContinuous data container: interface ExpanseContinuous { min: number; max: number; offset: number; trans: (x: number) =&gt; number; inv: (x: number) =&gt; number; ratio: boolean; } As mentioned above, the min and max denote the lower and upper bound of the data range, respectively. The offset property allows us to shift the data values by some pre-specified constant. This is useful, for example, when we want to ensure that the width of a spineplot bar is exactly 1 pixel less than the available space. The trans and inv properties allow us to apply non-linear transformations; they should, intuitively, be inverses of each other, and by default, they are set to the identity function ((x) =&gt; x). Finally, the ratio property, a boolean flag, indicates if the expanse is part of a ratio scale (see Section 3.3.3). If ratio is true, the min value is fixed at zero and cannot be modified, even by training on new data. The normalize and unnormalize functions in the ExpanseContinuous namespace are generalizations of the linear map: // ExpanseContinuous.ts export namespace ExpanseContinuous { export function normalize(expanse: ExpanseContinuous, x: number) { const { min, max, offset, trans } = expanse; return (trans(x - offset) - trans(min)) / (trans(max) - trans(min)); } export function unnormalize(expanse: ExpanseContinuous, value: x) { const { min, max, offset, trans, inv } = expanse; return inv(trans(min) + x * (trans(max) - trans(min))) + offset; } } Note that these functions take into account the non-linear transformations (trans, inv) and the offset. First, offset is applied to the value immediately, before normalization (or after, in the case of unnormalize). Second, the non-linear transformations are applied to the expanse’s limits as well as to the value, i.e. the value is normalized within the transformed space. The unnormalize function reverses this process, and also has to apply the inverse function. In mathematical notation: \\[\\begin{align} p = n(x) &amp;= [f(x - \\text{offset}) - f(\\min)] / [f(\\max) - f(\\min)] \\\\ x = u(p) &amp;= f^{-1} \\big[ f(\\min) + x \\cdot (f(\\max) - f(\\min)) \\big] + \\text{offset} \\end{align}\\] One can easily check that \\(n\\) and \\(u\\) are inverses of each other by taking one equation and solving for \\(x\\) or \\(p\\), respectively (assuming that \\(f^{-1}\\) exists). Back to code. The ExpanseContinuous.normalize and ExpanseContinuous.unnormalize work as expected: import { ExpanseContinuous } from &quot;./ExpanseContinuous&quot; const identity = (x) =&gt; x; const expanse = { min: 1, max: 16, offset: 0, trans: identity, inv: identity }; console.log(ExpanseContinuous.normalize(expanse, 4)); exp.trans = Math.sqrt; // Technically, we should also set inverse to square console.log(ExpanseContinuous.normalize(expanse, 4)); Finally, the ExpanseContinuous namespace also exports a train and breaks functions. The train functions simply finds a minimum and maximum value of an array of numbers and sets these as the new min and max (or just sets the max, if ratio is true). Breaks returns a list of “nice” axis breaks which are by computed via an algorithm inspired by base R’s pretty function (R Core Team 2024). 7.3.2.10 Point expanses ExpansePoint is the simpler of two types of discrete expanses, implementing an interface of Expanse&lt;string&gt;. It maps string values to equidistant positions within the \\([0,1]\\) interval, based on an ordered array of labels. Here is a simplified interface for its data container: interface ExpansePoint { labels: string[]; order: number[]; } The labels array stores the unique string values present in the data, sorted alphabetically. The order array is a simple array of indices of equal length as labels representing the order in which the labels get assigned to points in the \\([0, 1]\\) interval. By default, it is simply the increasing sequence 0 ... labels.length - 1. The normalize and unnormalize functions the ExpansePoint namespace exports simply map labels to equidistant point in the [0, 1] interval or vice versa, while respecting the order: // ExpansePoint.ts export namespace ExpansePoint { export function normalize(expanse: ExpansePoint, x: string) { const { labels, order } = expanse; const index = order[labels.indexOf(x)]; if (index === -1) return index; return index / (labels.length - 1); } export function unnormalize(expanse: ExpansePoint, x: number) { const { labels, order } = expanse; index = Math.round(x * (labels.length - 1)); return labels[order[index]]; } } The normalize function maps a label to [0, 1] based on its position within the labels array (which may be shuffled by order), whereas the unnormalize function finds the closest label corresponding to numeric position by reversing the process. This works largely as we would expect: import { ExpansePoint } from &quot;./ExpansePoint&quot; const cities = [&quot;Berlin&quot;, &quot;Prague&quot;, &quot;Vienna&quot;] const expanse = { labels: cities, order: [0, 1, 2] }; console.log(cities.map(city =&gt; ExpansePoint.normalize(expanse, city))); exp.order[0] = 1; // Swap the order of the first two values exp.order[1] = 0; console.log(cities.map(city =&gt; ExpansePoint.normalize(expanse, city))); However, it is important to note that, due to the fact that there is a finite number of labels, the unnormalize function is not a full, two-sided inverse of normalize, merely a one-sided inverse or retraction (see e.g. Lawvere and Schanuel 2009). That is, while composing unnormalize with normalize results in the identity function, the converse is not true. For instance: import { ExpansePoint } from &quot;./ExpansePoint&quot; const cities = [&quot;Berlin&quot;, &quot;Prague&quot;, &quot;Vienna&quot;] const expanse = { labels: cities, order: [0, 1, 2] }; // Unnormalize works as an inverse one way console.log(cities.map(x =&gt; { return ExpansePoint.unnormalize(ExpansePoint.noralize(expanse, x)) })) // But not the other way around console.log(ExpansePoint.normalize(ExpansePoint.unnormalize(expanse, 0.6))) Finally, like ExpanseContinuous, the ExpansePoint namespace also contains a train function, which loops through a string array, finds all unique values, and assigns these to labels, as well as a breaks function, which simply returns the array of labels (ordered by order). Further, the namespace also contains a reorder function which modifies the order property based on a new array indices, allowing the labels to be shuffled. 7.3.2.11 Band expanses While ExpansePoint places values at equidistant points along \\([0, 1]\\), ExpanseBand places values at the midpoints of corresponding to bins or bands of variable widths. The simplified interface of ExpanseBand is the following: export interface ExpanseBand { labels: string[]; order: number[]; weights: number[]; cumulativeWeights: number[]; } Like ExpansePoint, ExpanseBand has the labels and order properties, which work exactly the same way as before. However, additionally, it also has the weights and cumulativeWeights properties, which are numeric arrays that define the width of each band. weights record the width of each band, and cumulativeWeights record the cumulative sums of the weights, which are used in the normalize and unnormalize functions. Thus, each time we update weights, we need to also update cumulativeWeights as well. The normalize and unnormalize functions in the ExpanseBand namespace map labels to and from the midpoint of their corresponding bands: export namespace ExpanseBand { export function normalize(expanse: ExpanseBand, value: string) { const { labels } = expanse; const index = labels.indexOf(value); return getMidpoint(expanse, index); } export function unnormalize(expanse: ExpanseBand, value: number) { const { labels, order, cumulativeWeights } = expanse; const weight = value * last(cumulativeWeights); let index = 0; while (index &lt; cumulativeWeights.length - 1) { if (cumulativeWeights[index] &gt;= weight) break; index++; } return labels[order[index]]; } function getMidpoint(expanse: ExpanseBand, index: number) { const { order, cumulativeWeights } = expanse.props; index = order[index]; const lower = cumulativeWeights[index - 1] ?? 0; const upper = cumulativeWeights[index]; const max = last(cumulativeWeights); return (lower + upper) / 2 / max; } } Notice that, because of the cumulative nature of the bands, the logic in the functions’ bodies is a bit more complicated. First, to normalize a label, we need to first find the index of the label and then return the corresponding midpoint of the band, taking weights and order into account. Second, to unnormalize, we actually have to loop through the array of cumulativeWeights - there is no way to determine which bin a normalized value belongs to in \\(O(1)\\) time (as far as I am aware). This is not much of a problem since the ExpanseBand.unnormalize is not used anywhere in the system (all scales implemented thus far use only ExpanseBand.normalize), however, it is important to be mindful of this. knitr::opts_chunk$set(eval = TRUE) References Abelson, Harold, and Gerald Jay Sussman. 2022. Structure and Interpretation of Computer Programs: JavaScript Edition. MIT Press. Allaire, JJ, and Christophe Dervieux. 2024. Quarto: R Interface to ’Quarto’ Markdown Publishing System. https://CRAN.R-project.org/package=quarto. Allaire, JJ, Yihui Xie, Christophe Dervieux, Jonathan McPherson, Javier Luraschi, Kevin Ushey, Aron Atkins, et al. 2024. Rmarkdown: Dynamic Documents for r. https://github.com/rstudio/rmarkdown. Auguie, Baptiste. 2017. gridExtra: Miscellaneous Functions for \"Grid\" Graphics. https://CRAN.R-project.org/package=gridExtra. Bache, Stefan Milton, and Hadley Wickham. 2022. Magrittr: A Forward-Pipe Operator for r. https://CRAN.R-project.org/package=magrittr. “Bun.” 2025. Bun. https://bun.sh. Chang, Winston, Joe Cheng, JJ Allaire, Carson Sievert, Barret Schloerke, Yihui Xie, Jeff Allen, Jonathan McPherson, Alan Dipert, and Barbara Borges. 2024. Shiny: Web Application Framework for r. https://CRAN.R-project.org/package=shiny. Cheng, Joe, Winston Chang, Steve Reid, James Brown, Bob Trower, and Alexander Peslyak. 2024. Httpuv: HTTP and WebSocket Server Library. https://CRAN.R-project.org/package=httpuv. Evan You and the Vue Core Team. 2024. “Vue.js.” https://vuejs.org. Fogus, Michael. 2013. Functional JavaScript: Introducing Functional Programming with Underscore. Js. \" O’Reilly Media, Inc.\". Gamma, Erich, Ralph Johnson, Richard Helm, Ralph E Johnson, and John Vlissides. 1995. Design Patterns: Elements of Reusable Object-Oriented Software. Pearson Deutschland GmbH. Google. 2025. “Angular.” https://angular.dev/guide/signals. Handmade Hero. 2025. “Compile-time introspection and metaprogramming.” Handmade Network. https://handmade.network/fishbowl/metaprogramming. knockoutjs. 2019. “Knockout : Home.” https://knockoutjs.com. Lawvere, F William, and Stephen H Schanuel. 2009. Conceptual Mathematics: A First Introduction to Categories. Cambridge University Press. Leptos Core Team. 2025. “Home - Leptos.” https://leptos.dev. Meta. 2024. “React.” https://react.dev. Ollila, Risto, Niko Mäkitalo, and Tommi Mikkonen. 2022. “Modern Web Frameworks: A Comparison of Rendering Performance.” Journal of Web Engineering 21 (3): 789–813. Pedersen, Thomas Lin. 2024. Patchwork: The Composer of Plots. https://CRAN.R-project.org/package=patchwork. Phung, Phu H, David Sands, and Andrey Chudnov. 2009. “Lightweight Self-Protecting JavaScript.” In Proceedings of the 4th International Symposium on Information, Computer, and Communications Security, 47–60. Posit. 2024. “RStudio IDE.” https://posit.co/products/open-source/rstudio. R Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/. Rich Harris and the Svelte Core Team. 2024. “Svelte.” https://svelte.dev. Rxteam. 2024. “ReactiveX.” https://reactivex.io. Ryan Carniato. 2023. “Revolutionary Signals.” YouTube. Youtube. https://www.youtube.com/watch?v=Jp7QBjY5K34&amp;t. Sievert, Carson. 2020. Interactive Web-Based Data Visualization with r, Plotly, and Shiny. Chapman; Hall/CRC. Solid Core Team. 2025. “SolidJS.” https://www.solidjs.com. Theus, Martin. 2002. “Interactive Data Visualization using Mondrian.” J. Stat. Soft. 7 (November): 1–9. https://doi.org/10.18637/jss.v007.i11. Unwin, Antony, George Hawkins, Heike Hofmann, and Bernd Siegl. 1996. “Interactive Graphics for Data Sets with Missing Values—MANET.” Journal of Computational and Graphical Statistics 5 (2): 113–22. Urbanek, Simon. 2011. “iPlots eXtreme: Next-Generation Interactive Graphics Design and Implementation of Modern Interactive Graphics.” Computational Statistics 26 (3): 381–93. Vaidyanathan, Ramnath, Yihui Xie, JJ Allaire, Joe Cheng, Carson Sievert, and Kenton Russell. 2021. Htmlwidgets: HTML Widgets for r. https://CRAN.R-project.org/package=htmlwidgets. Wickham, Hadley. 2013. “Bin-Summarise-Smooth: A Framework for Visualising Large Data.” Had. Co. Nz, Tech. Rep. Wickham, Hadley. 2014. “I’m Hadley Wickham, Chief Scientist at RStudio and creator of lots of R packages (incl. ggplot2, dplyr, and devtools). I love R, data analysis/science, visualisation: ask me anything! : r/dataisbeautiful.” https://www.reddit.com/r/dataisbeautiful/comments/3mp9r7/comment/cvi19ly. Wickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis (2e). Springer-Verlag New York. https://ggplot2.tidyverse.org. ———. 2019. Advanced r. Chapman; Hall/CRC. Wilkinson, Leland. 2012. The Grammar of Graphics. Springer. For instance, the plotly package takes up about 7.3 megabytes, which amounts to over 15x difference.↩︎ Part of the reason for this failure may have also been that, since plotscaper is an interactive data visualization system, integrating reactivity further complicates the process.↩︎ Of course, we could technically parse the list of messages to retrieve the figure’s state as it will be when the figure gets rendered. For instance with create_schema(...) |&gt; assign_cases(1:10, 2) |&gt; assign_cases(5:10, 3) we could parse the first five indices belonging to group 2 and the second belonging to group 3. However, since the user has to write the code to modify the figure’s state in the first place, the utility of this parsing is not clear.↩︎ That is, I use namespaces like a class with all static methods and properties.↩︎ This also makes some examples easier to read, since all of the variables used in a function’s body are declared in the signature; one does not have to scroll all the way up to the top of the class declaration to find the properties used inside the method’s body.↩︎ Or perhaps infamous.↩︎ In simpler implementations, a single array can be used instead of the dictionary; the listeners are then notified whenever the object “updates”.↩︎ This can be solved by adding a priority property to the event callbacks, and sorting the arrays by priority.↩︎ Memoizing a derived value can be done by creating a new signal and an effect that runs when the original value gets updated.↩︎ The indices indices being “dense” means all values in the range 0 ... cardinality - 1 appear in the array at least once.↩︎ Unless the length of the data changes. I have not implemented data streaming for plotscape yet, however, it would be easy to extend bijection factor for streaming by simply pushing/popping the array of indices.↩︎ Other use-cases may be plots involving a single geometric object such as density plots and radar plots, however, these are currently not implemented in plotscape.↩︎ In the general case where the histogram bins are not necessarily all of equal with; if all bins are known to have the same width, we can compute the bin index in constant \\(O(1)\\) time↩︎ Or a factor isomorphic to that one, up to the permutation of indices.↩︎ Equality only if either one or both of the factors are constant, or if there exists an isomorphism between the factors’ indices.↩︎ Equality only if all element-wise combinations of indices form a bijection/are unique.↩︎ Using zero-based indexing.↩︎ The actual implementation in the current version of plotscape differs slightly in several key aspects, however, I chose to use this simplified interface here for clarity.↩︎ Note that, in the example below, we assign string key properties to an array. This would be prohibited in many languages but is perfectly valid in JavaScript/TypeScript due to its highly dynamic nature.↩︎ Technically, the type signature of the real function as implemented in plotscape is a lot more complicated since it heavily relies on generics to correctly infer the type of the summarized data, however, the basic idea is the same.↩︎ This is almost the exact implementation, with the only difference being that in plotscape, domain and codomain can additionally be array-valued, so Scale.pushforward includes logic for handling this.↩︎ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]

[["background.html", "3 Background 3.1 Brief history of interactive data visualization 3.2 What even is interactive data visualization? 3.3 General data visualization theory 3.4 Summary", " 3 Background 3.1 Brief history of interactive data visualization Data visualization has a rich and intricate history, and a comprehensive treatment is beyond the scope of the present thesis. Nevertheless, in this section, I will provide a brief overview, with a particular focus on the later developments related to interactive visualization. For a more detailed historical account, readers should refer to Beniger and Robyn (1978), Dix and Ellis (1998), Friendly (2006), Friendly and Wainer (2021), or Young, Valero-Mora, and Friendly (2011). 3.1.1 Static data visualization: From ancient times to the space age The idea of graphically representing abstract information is very old. As one concrete example, a clay tablet recording a land survey during the Old Babylonian period (approximately 1900-1600 BCE) has recently been identified as the earliest visual depiction of the Pythagorean theorem (Mansfield 2020). Other examples of early abstract visualizations include maps of geographic regions and the night sky, and these were also the first to introduce the idea of a system of coordinates (Beniger and Robyn 1978; Friendly and Wainer 2021). Figure 3.1: Photos of the tablet Si. 427 which has recently been identified as the earliest depiction of the Pythagorean theorem (Mansfield 2020). Left: the obverse of the tablet depicts a diagram of a field, inscribed with areas. Right: the reverse of the tablet contains a table of numbers, corresponding to the calculation of the areas (source: Wikimedia Commons, Mansfield 2018). For a long time, coordinate systems remained tied to geography and maps. However, with the arrival of the early modern age, this was about to change. In the 16-17th century, the works of the 9th century algebraist Al-Khwarizmi percolated into Europe, and with them the idea of representing unknown quantities by variables (Kvasz 2006). This idea culminated with Descartes, who introduced the concept of visualizing algebraic relationships as objects in a 2D plane, forging a powerful link between Euclidean geometry and algebra (Friendly and Wainer 2021). Coordinate systems were thus freed of their connection to geography, and the x- and y-axes could now be used to represent an arbitrary “space” spanned by two variables. Descartes’ invention of drawing abstract relationships as objects in a 2D plane was initially only used to plot mathematical functions. However, it would not be long until people realized that observations of the real world could be visualized as well. A true pioneer in this arena was William Playfair, who popularized visualization as a way of presenting socioeconomic data and invented many types of plots still in use today, such as the barplot, lineplot, and pie chart (Friendly and Wainer 2021). Further, with the emergence of modern nation states in the 19th century, the collection of data and statistics (“things of the state,” Online Etymology Dictionary 2024) became widespread, leading to a “golden age” of statistical graphics (Beniger and Robyn 1978; Friendly and Wainer 2021; Young, Valero-Mora, and Friendly 2011). This period saw the emergence of other graphical lumnaries, such as Étienne-Jules Marey and Charles Joseph Minard (Friendly and Wainer 2021), as well as some ingenious examples of the use of statistical graphics to solve real-world problems, including John Snow’s investigation into the London cholera outbreak (Freedman 1999; Friendly and Wainer 2021) and Florence Nightingale’s reporting on the unsanitary treatment of wounded British soldiers during the Crimean War (Brasseur 2005), both of which lead to a great reduction of preventable deaths. Simultaneously, the field of mathematical statistics was also experiencing significant developments. Building upon the foundation laid by mathematical prodigies such as Jakob Bernoulli, Abraham de Moivre, Pierre Simon Laplace, and Carl Friedrich Gauss, early 19th century pioneers such as Adolph Quetelet and Francis Galton began developing statistical techniques for uncovering hidden trends in the newly unearthed treasure trove of socioeconomic data (Fienberg 1992; Freedman 1999). In the late 19th and early 20th century, these initial efforts were greatly advanced by the theoretical work of figures such as Karl Pearson, Ronald A. Fisher, Jerzy Neyman, and Harold Jeffreys, who established statistics as a discipline in its own right and facilitated its dissemination throughout many scientific fields (Fienberg 1992). As mathematical statistics gained prominence in the early 20th century, data visualization declined. Perceived as less rigorous than “serious” statistical analysis, it got relegated to an auxiliary position, ushering in “dark age” of statistical graphics (Friendly 2006; Young, Valero-Mora, and Friendly 2011). This development may have been partly driven by the early frequentist statisticians’ aspiration to establish statistics as a foundation for determining objective truths about the world and society, motivated by personal socio-political goals (see Clayton 2021). Be it as it may, while statistical graphics also did get popularized and entered the mainstream during this time, only a few interesting developments took place (Friendly and Wainer 2021). However, beginning in the late 1950’s, a series of developments took place which would restore the prominence of data visualization and make it more accessible than ever. Firstly, on the theoretical front, the work of certain academic heavy-weights greatly elevated data visualization and its prestige. Particularly, John Tukey (1962; 1977) fervently championed exploratory data analysis and placed data visualization in its centre. Around the same time, Jacques Bertin published his famous Sémiologie graphique (1967), which was one of the first works to attempt to lay out a comprehensive system of visual encodings and scales. Secondly, at the more applied level, the development of personal computers (see e.g. Abbate 1999) and high-level programming languages such as FORTRAN in 1954 (Backus 1978), made the process of rendering production-grade figures easier and more accessible than ever before. Combined, these developments fueled a surge in the use and dissemination of data visualizations. As the millennium drew to a close, several other important developments solidified the foundation of static data visualization. First, William Cleveland made significant contributions to the field, laying out many important principles for scientific data visualization (Cleveland 1985, 1993). Of note, his seminal study on the impact of the choice of visual encodings on statistical judgements remains widely cited today (Cleveland and McGill 1984). Similarly, Edward Tufte introduced essential principles for designing effective graphics, coining terms such as chartjunk and data-to-ink ratio (Tufte 2001). Finally, Leland Wilkinson’s groundbreaking Grammar of Graphics (2012) introduced a comprehensive system for designing charts based on simple algebraic rules, influencing nearly every subsequent software package and research endeavor in the field of visualization. 3.1.2 Early interactive data visualization: By statisticians for statisticians Compared to static data visualization, interactive data visualization is much more of a recent development. Consequently, less has been written about its history, owing to the shorter timeline, as well as the rapid evolution of software in the time since its inception and the proprietary nature of some systems. Nevertheless, the brief history of interactive data visualization is still rather compelling. Following the boom of static data visualization in the 1950’s, interactive data visualization would not be left far behind. It started with tools designed for niche, specialized tasks. For example, Fowlkes (1969) designed a system which allowed the users to view probability plots under different configurations of parameters and transformations, whereas Kruskal (1965) created a tool for visualizing multidimensional scaling. Figure 3.2: John Tukey showcasing the PRIM-9 system (left), with an example of a projected scatterplot (right, Fisherkeller, Friedman, and Tukey 1974). Screenshots were taken from a video available at the ASA Statistical Graphics Video Library. However, researchers soon recognized the potential of interactive data visualization as a general-purpose tool for exploring data. The first such general-purpose system was PRIM-9 (Fisherkeller, Friedman, and Tukey 1974). PRIM-9 allowed for exploration of multivariate data via interactive features such as projection, rotation, masking, and filtering. Following PRIM-9, the late 1980’s saw the emergence of a new generation of systems which provided an even wider range of capabilities. Tools like MacSpin (Donoho, Donoho, and Gasko 1988), Data Desk (Velleman and Paul 1989), XLISP-STAT (Tierney 1990), and XGobi (Swayne, Cook, and Buja 1998) introduced features such as interactive scaling, rotation, linked views, and grand tours (for a glimpse into these systems, excellent video-documentaries are available at ASA Statistical Graphics Video Library). Figure 3.3: Example of interactive control of histogram highlighting in XLISP-STAT. Note that, unlike in many current data visualization systems, aggregation plots were sensitive to data order (not commutative). This non-commutative behavior meant that, for instance, a highlighted segment could appear in the middle of a bar (as seen in the figure above) or multiple non-adjacent highlighted cases might appear as ‘stripes’ (figure reproduced from Tierney 1990). 3.1.2.1 Open-source Statistical Computing The proliferation of open-source, general-purpose statistical computing software such as S and R further democratized the access to interactive data visualization tools (see also Leeuw 2004). Building on XGobi’s foundation, GGobi (Swayne et al. 2003), expanded upon on XGobi and provided an integration layer for R. Other tools like MANET (Unwin et al. 1996) and Mondrian (Theus 2002) introduced sophisticated linking techniques, with features such as selection sequences, allowing the users to combine a series of selections via logical operators (see also Unwin et al. 2006). Further, iPlots (Urbanek and Theus 2003) implemented a general framework for interactive plotting in R, allowing not only for one-shot rendering interactive figures from R but also for direct programmatic manipulation. This package was later expanded expanded for big data capabilities in iPlots eXtreme (Urbanek 2011). Finally, the cranvas package (Xie, Hofmann, and Cheng 2014) introduced a set of reactive programming primitives that could be used for building the infrastructure underlying interactive graphics directly in R, within the model-view-controller (MVC) framework. Figure 3.4: Examples of interactive features in Mondrian (Theus 2002): selection operators (left) and mosaic plot with querying (right). Alongside the more general interactive data visualization frameworks mentioned above, there were also more specialized packages designed for specific techniques and models. For instance, KLIMT was developed for interactive visualization of classification and regression trees (Urbanek and Unwin 2001; Urbanek 2002). Similarly, packages like tourr (Wickham 2011), spinifex (Spyrison and Cook 2020), liminal (Lee 2021; Lee, Laa, and Cook 2022) provided tools for exploring large multivariate data sets via grand tour projections (see Cook et al. 1995). Another important milestone in the history of interactivity in R was the development of Shiny (Chang et al. 2024; see also Sievert 2020; Wickham 2021), a general framework for developing web apps in R. Shiny uses a client-server MVC-like model, whereby the user defines an R-based server and a web-based UI/controller layer (which is scaffolded in R, but which transpiles to HTML, CSS, and JavaScript). Bi-directional communication between the client and the server is handled via WebSockets (MDN 2025), facilitated through the httpuv package (Cheng et al. 2024). The primary advantage of Shiny is that it gives less technical R users the ability to easily create rich interactive web apps, including interactive data visualizations (by re-rendering static plots). The one major downside of Shiny is that, since every interactive event has to do a round-trip from the client to the R server and back again, high-frequency interactions (such as brushing scatterplot points) can become prohibitively slow, particularly at larger data volumes (although there are ways to mitigate this, Sievert 2020). Over time, there seems to have been a trend towards more of the specialized tools within the R community, and fewer of the general, high-level frameworks (although there were some notable exceptions, such as the loon package, Waddell and Oldford 2023). Currently, it seems that R users typically encounter interactive visualizations as part of Shiny (Chang et al. 2024) dashboards, or through R wrappers of interactive data visualization packages ported over from the JavaScript ecosystem (see Section 3.1.3). 3.1.2.2 Common features and limitations of early interactive systems A common thread among these interactive data visualization systems is that they were designed by statisticians with primary focus on data exploration. High-level analytic features such as linked views, rotation/projection, and interactive manipulation of model parameters made frequent appearance. While these features were powerful, they also contributed to a steeper learning curve, potentially limiting adoption by users without a strong data analytic background. Furthermore, these early tools were typically standalone applications, with only later packages like GGobi and iplots offering integration with other data analysis software and languages. Finally, they often offerered only limited customization options and this made them less suitable for data presentation. 3.1.3 Interactive data visualization and the internet: Web-based interactivity The end of the millennium marked the arrival of a new class of technologies which impacted interactive data visualization just as much as almost every other field of human endeavor. The rise of the internet in the mid 1990’s made it possible to create interactive applications that could be accessed by anyone, from anywhere. This was aided by the dissemination of robust and standardized web browsers, as well as the development of JavaScript as a high-level programming language for the web (for a tour of the language’s history, see e.g. Wirfs-Brock and Eich 2020). Soon, interactive visualizations became just one of many emerging technologies within the burgeoning web ecosystem. Early web-based interactive data visualization systems tended to rely on external plugins. Examples of these include Prefuse (Heer, Card, and Landay 2005) and Flare (developed around 2008, Blokt 2020), which leveraged the Java runtime and Adobe Flash Player, respectively. However, as browser technologies advanced, particularly as JavaScript’s performance improved thanks to advances in just-in-time compilation (JIT, see e.g. Clark 2017; Dao 2020), it became possible to create complex interactive experiences directly in the browser. This led to the emergence of several popular web-native interactive data visualization systems in the early 2010s, many of which remain widely used today. 3.1.3.1 D3 D3.js (Bostock 2022) is one of the earliest and most influential web-based visualization systems. As a general, low-level framework for visualizing data, D3 provides of a suite of specialized JavaScript modules for various aspects of the data visualization workflow, including data parsing, transformation, scaling, and DOM interaction. For instance, here’s how to create a basic scatterplot in D3: import * as d3 from &quot;d3&quot;; const plot = document.querySelector&lt;HTMLDivElement&gt;(&quot;#d3-plot&quot;)!; const data = [ { x: 1, y: 0.41 }, { x: 2, y: 4.62 }, { x: 3, y: 7.62 }, { x: 4, y: 6.54 }, { x: 5, y: 9.61 }, ]; const margin = { top: 10, right: 30, bottom: 30, left: 60 }; const width = parseFloat(plot.style.width); const height = parseFloat(plot.style.height); // Create a SVG element, resize it, and append it to #d3-plot const svg = d3 .select(&quot;#d3-plot&quot;) .append(&quot;svg&quot;) .attr(&quot;width&quot;, width + margin.left + margin.right) .attr(&quot;height&quot;, height + margin.top + margin.bottom) .append(&quot;g&quot;) .attr(&quot;transform&quot;, &quot;translate(&quot; + margin.left + &quot;,&quot; + margin.top + &quot;)&quot;); // Create x and y scales and append them to const scaleX = d3.scaleLinear().domain([0, 6]).range([0, width]); const scaleY = d3.scaleLinear().domain([10, 0]).range([0, height]); svg .append(&quot;g&quot;) .attr(&quot;transform&quot;, &quot;translate(0,&quot; + height + &quot;)&quot;) .call(d3.axisBottom(scaleX)); svg.append(&quot;g&quot;).call(d3.axisLeft(scaleY)); // Add points svg .append(&quot;g&quot;) .selectAll(&quot;dot&quot;) .data(data) .enter() .append(&quot;circle&quot;) .attr(&quot;cx&quot;, (d) =&gt; scaleX(d.x)) .attr(&quot;cy&quot;, (d) =&gt; scaleY(d.y)) .attr(&quot;r&quot;, 2); Figure 3.5: Example of a scatterplot built with D3.js. The code was taken from D3 Graph Gallery (Holtz 2022b) and adjusted to use ES6 syntax and slightly more informative variable names/comments. As you can see from Figure 3.5 and the corresponding code, D3 is a fairly low-level framework. Compared to typical high-level plotting functionalities such as those provided by base R or ggplot2 (R Core Team 2024; Wickham 2016), the user has to handle many low-level details such as scaling and appending of primitives explicitly. This is also the case with interaction. While D3 does provide some methods for handling reactive DOM events, it does not itself provide a system for dispatching and coordinating these events - instead, it delegates this responsibility to the user, and encourages the use of reactive Web frameworks such as React (Meta 2024), Vue (Evan You and the Vue Core Team 2024), or Svelte (Rich Harris and the Svelte Core Team 2024). Finally, D3.js visualizations are rendered as Scalable Vector Graphics (SVG) by default. This ensures lossless scaling but may impact rendering performance at high data volumes. While various unofficial alternative rendering engines based on the HTML 5 Canvas element or WebGL, do exist, there are no official libraries with such functionalities as of this date. 3.1.3.2 Plotly and Highcharts Building upon the low-level infrastructure that D3 provides, many packages such as Plotly.js (Plotly Inc. 2022) and Highcharts (Highsoft 2024) provide high-level abstractions which make the process of building interactive figures easier for the average user. Unlike D3 which provides low-level utilities such as data transformations, scales, and geometric objects, these packages provide a simple declarative framework for rendering entire plots using a static JSON schema. Here’s how we can render the same scatterplot in Plotly, using the R plotly package (Sievert 2020): library(plotly) data &lt;- data.frame(x = 1:5, y = c(0.41, 4.62, 7.62, 6.54, 9.61)) plot_ly(data, x = ~x, y = ~y) Figure 3.6: Example of a scatterplot with plotly. Here’s the correponding code in JavaScript: const data = [{ x: [1, 2, 3, 4, 5], y: [0.41, 4.62, 7.62, 6.54, 9.61], mode: &#39;markers&#39;, type: &#39;scatter&#39; }]; Plotly.newPlot(&#39;app&#39;, data); Clearly, compared to the D3 code used to create Figure 3.5, the code for creating Figure 3.6 is much terser. Many details, such as the axis limits and margins, point size and colour, gridlines, and widgets, are handled implicitly, via default values and automatic inference. Also, note that the figure provides some interactive features by default, such as zooming, panning, and tooltip on hover. Reactivity is handled automatically using systems built on the native DOM Event Target interface (MDN 2024). Highcharts provides a similar JSON-based interface for specifying plots. While perhaps slightly more flexible than Plotly, it also requires more verbose specifications. Because of the similarity, I will not provide a separate example here (interested reader should look up the package’s website, Highsoft 2024). Finally, like D3, both plotly.js and Highcharts also render the graphics in SVG by default. However, unlike D3, they both also provide alternative rendering engines based on WebGL (Highsoft 2022; Plotly Inc. 2024). This makes them more ergonomic for use with large data sets. 3.1.3.3 Vega and Vega-Lite Vega (Satyanarayan et al. 2015; Vega Project 2024d) is another popular interactive data visualization package. Like Plotly and Highcharts, Vega is also partially built upon the foundation of D3 and uses JSON schema for plot specification. However, Vega is more low-level and implements a lot of custom functionality. This allows it to offer more fine-grained customization of graphics and interactive behavior, leading to greater flexibility. However, this added flexibility does come at a cost. Compared to the high-level frameworks like Plotly and Highcharts, Vega is significantly more verbose. For instance, creating a scatterplot matrix with linked selection in Vega requires over 300 lines of JSON specification, not including the data and using default formatting (Vega Project 2024b). Vega-Lite (Satyanarayan et al. 2015) attempts to remedy this complexity by providing a high-level interface to Vega. Here’s how we can define a scatterplot with zooming, panning, and tooltip on hover in Vega-Lite: library(vegawidget) plot_spec &lt;- list( `$schema` = vega_schema(), width = 500, height = 300, data = list(values = data), mark = list(type = &quot;point&quot;, tooltip = TRUE), encoding = list( x = list(field = &quot;x&quot;, type = &quot;quantitative&quot;), y = list(field = &quot;y&quot;, type = &quot;quantitative&quot;) ), params = list(list(name = &quot;grid&quot;, select = &quot;interval&quot;, bind = &quot;scales&quot;)) ) plot_spec |&gt; vegawidget() Figure 3.7: Example of a scatterplot built with vegalite. Just for clarity, the R code above corresponds to the following declarative JSON schema: { $schema: &quot;https://vega.github.io/schema/vega-lite/v5.json&quot;, width: 500, height: 300, data: { values: [ { x: 1, y: 0.41 }, { x: 2, y: 4.62 }, { x: 3, y: 7.62 }, { x: 4, y: 6.54 }, { x: 5, y: 9.61 }] }, mark: {&quot;type&quot;: &quot;point&quot;, &quot;tooltip&quot;: true}, encoding: { x: { field: &quot;x&quot;, type: &quot;quantitative&quot; }, y: { field: &quot;y&quot;, type: &quot;quantitative&quot; } }, params: [{ name: &quot;grid&quot;, select: &quot;interval&quot;, bind: &quot;scales&quot; }] }; Note that the zooming and panning capability is provided by the params property, which declaratively specifies a list of plot parameters that can be modified by interaction (see Vega Project 2024c). In the case above, the specification creates a two-way binding between plot scales and mouse selection events (Vega Project 2024a). 3.1.3.4 Common features and limitations of web-based interactive systems In general, these contemporary web-based interactive data visualization systems offer a great deal of flexibility, making them well-suited to modern data presentation. However, all of this expressiveness does seem to come at a cost. Compared to the earlier statistical graphics systems, described in Section 3.1.2, many of the more advanced features that used to be common are either missing or require a significant effort to implement, such that they are only accessible to expert users. This is evidenced by their infrequent appearance in documentation and example gallery pages. For instance, the R Graph Gallery entry on Interactive Charts (Holtz 2022a) features multiple interactive figures implemented in the JavaScript libraries described above. However, all of these examples show only surface-level, single-plot interactive features such zooming, panning, hovering, 3D rotation, and node repositioning. The Plotly Dash documentation page on Interactive Visualizations (Plotly Inc. 2022) does feature two examples of simple linked cross-filtering, however, the vast majority of visualizations in the Plotly R Open Source Graphing Library documentation page (Plotly Inc. 2022) show examples only surface-level interactivity. Similarly, VegaLite Gallery pages on Interactive Charts (Vega Project 2022) feature many examples, however, only a limited number of examples show linked or parametric interactivity (see e.g. Interactive Multiview Displays). Finally, the Highcharter Showcase Page (Kunst 2022) does not feature any examples of linking. Even when more advanced features such as linking and parametric manipulation are supported, they are often limited in some way. For instance, take the following quote from the website of Crosstalk, a package designed to enable linking between web-based interactive widgets created with the htmlwidgets R package (Vaidyanathan et al. 2021) or Shiny (Chang et al. 2024): “Crosstalk currently only works for linked brushing and filtering of views that show individual data points, not aggregate or summary views (where “observations” is defined as a single row in a data frame). For example, histograms are not supported since each bar represents multiple data points; but scatter plot points each represent a single data point, so they are supported.” Posit (formerly RStudio Inc.) (2025) Of course, with enough effort and programming skill, these web-based visualization systems can still be used to create rich interactive figures with arbitrarily sophisticated features. However, doing so often requires stepping down a level of abstraction and dealing with low-level language primitives, defeating the purpose of using these high-level libraries in the first place. It also creates a barrier to entry for casual users (Keller, Manz, and Gehlenborg 2024), which may in fact explain why interactive visualizations are nowadays primarily used for data presentation, not data exploration (Batch and Elmqvist 2017). Within these frameworks, creating rich interactive visualizations may be a task better suited to dedicated developers working inside large organizations, rather than individual researchers or analysts. 3.2 What even is interactive data visualization? If it looks like a duck, swims like a duck, and quacks like a duck, then it probably is a duck. […] The irony is that while the phrase is often cited as proof of abductive reasoning, it is not proof, as the mechanical duck is still not a living duck Duck Test entry, (Wikipedia 2022) In the previous section (Section 3.1), I provided an overview of the history and present state of interactive data visualization, discussing a number of features and systems. However, while doing so, I avoided one crucial question: what constitutes an interactive data visualization? Surprisingly, despite the widespread popularity of interactive data visualizations, there is no universally agreed-upon definition of interactivity (Vanderplas, Cook, and Hofmann 2020). Within the data visualization literature, the terms “interactive” and “interaction” are rarely explicitly defined. And even when they are, the definitions are often incongruent or even contradictory (see e.g. Dimara and Perin 2019; Elmqvist et al. 2011; Pike et al. 2009). Finally, similar ambiguity extends to other terms related concepts, such as the “dashboard” (Sarikaya et al. 2018). This lack of a clear consensus makes the task of discussing interactive data visualization difficult. Ignoring the issue could lead to confusion, while a truly comprehensive dive into the terminology surrounding interactive data visualization could become excessively dense, as evidenced by the existence of entire research papers dedicated to the topic (see e.g. Dimara and Perin 2019; Elmqvist et al. 2011). To address this issue, this section aims to provide a concise overview of how interactivity has been conceptualized in the literature. The goal is to establish a clear framework for understanding “interactive” and “interaction” within the context of this thesis. 3.2.1 Interactive vs. interacting with First, the term “visualization” in “interactive data visualization” can be interpreted in two different ways: As a noun: a concrete chart or figure As a nominalized verb: the process or practice of interacting with such a figure In the data visualization literature, both meanings are frequently employed, leading to a significant amount of conceptual ambiguity (see Dimara and Perin 2019; Pike et al. 2009; Yi et al. 2007). Some researchers concentrate on the mathematical and computational aspects of interactive data visualization, discussing specific systems and software implementations (see e.g. Buja, Cook, and Swayne 1996; Kelleher and Levkowitz 2015; Leman et al. 2013; Wills 2008). Others focus on the cognitive or human-computer interaction (HCI) aspects, exploring the impact different interactive techniques have on the users’ ability to derive insights from the data (see e.g. Brehmer and Munzner 2013; Dimara and Perin 2019; Dix and Ellis 1998; Pike et al. 2009; Quadri and Rosen 2021; Yi et al. 2007). The problem is further exacerbated by the highly interdisciplinary nature of the field, spanning statistics, computer science, applied mathematics, business analytics, HCI, and cognitive psychology (see Brehmer and Munzner 2013; Dimara and Perin 2019). Consequently, literature search of the term “interactive data visualization” can yield a broad set of results, many of which may be only tangentially related. To address this conceptual ambiguity, I believe it is necessary to clearly define key terms. Throughout thesis, the term “interactive data visualization” to primarily denote concrete charts or figures. When referring to the process or practice, I will attempt to employ more active phrasing, such as “interacting with a visualization” or “user’s interaction with a visualization”. While there may still be a few places where the term “visualization” is used in the nominalized verb form, the intended meaning should be unambiguously inferable from the context. 3.2.2 Interactive enough? However, even when we use the term “interactive data visualization” to refer to concrete charts or figures, the meaning still remains fairly ambiguous. When can we call a figure interactive? What features does it have to contain? While these questions may seem simple, it is hard to find consensus about them in the data visualization literature (Vanderplas, Cook, and Hofmann 2020). The criteria for what makes a figure “interactive” vary a lot from researcher to researcher, such that the same figure may be considered interactive by some but not by others. Some researchers ascribe to a broad definition of interactive data visualization, such that any figure which can be dynamically manipulated by the user may be considered interactive (Brodbeck, Mazza, and Lalanne 2009). For others, the important factor is the speed of the computer’s responses to user input, with faster updates translating to greater interactivity (Becker and Cleveland 1987; Buja, Cook, and Swayne 1996; see also Wilhelm 2003). Some of these researchers also differentiate between “interactive” and “dynamic” manipulation, where interactive manipulation involves discrete actions such as pressing a button or selecting an item from a drop-down menu, whereas dynamic manipulation involves continuous actions, like moving a slider or clicking-and-dragging (Rheingans 2002; Jankun-Kelly, Ma, and Gertz 2007; see also Dimara and Perin 2019). However, for many other researchers, interactivity is a much narrower concept, hinging on high-level analytic features that enable efficient exploration of the data. These features include the ability to generate different views of the data (by e.g. zooming, panning, sorting, and filtering), and the reactive propagation of changes between connected or “linked” parts of the figure (Kehrer et al. 2012; Buja, Cook, and Swayne 1996; Keim 2002; Unwin 1999; C. Chen et al. 2008). Notably, these are also the features which enable the famous visual information-seeking mantra: “overview first, zoom and filter, then details-on-demand” (Shneiderman 2003). A similar idea also appears in visual analytics research, in the distinction between “surface-level” (or “low-level”) and “parametric” (or “high-level”) interactions, where surface-level interactions manipulate attributes of the visual domain only (e.g. zooming and panning), whereas parametric interactions manipulate parameters of some underlying mathematical model or algorithm (Leman et al. 2013; Self et al. 2018; Pike et al. 2009). Ultimately, for researchers ascribing to this narrower view of interactivity, the presence of an interactive feature does not necessarily justify calling a figure “interactive”, if it does not promote the acquisition of statistical insights above and beyond those provided by a static figure. For instance, while the ability to pick the colour of scatterplot points via an interactive palette widget may allow us to produce a more aesthetically pleasing result, it does not, generally, generate any new analytical understanding. Table 3.1 provides a concise overview of the various perspectives on interactivity discussed above. It is important to note that this table is meant to serve as a reference point for subsequent discussions within the text and is not intended to provide an exhaustive review. For more comprehensive taxonomies of interactive visualization systems and features, see e.g. Dimara and Perin (2019), Yi et al. (2007). Table 3.1: Table 3.2: Summary of the perspectives on interactivity Name Details User interaction The user can interactively manipulate the figure in some way Real-time updates The user’s interactions propagate into the visualization with little to no lag Plot- and data-space manipulation The user can interactively explore different parts of the data set by doing actions which effectively amount to “subsetting” rows of the data (e.g. zooming, panning, and filtering) Linked views The user’s interactions propagate across multiple plots (e.g. linked highlighting) Parametric updates The user can manipulate the parameters of some underlying mathematical model or algorithm (e.g. histogram bins, grand tour projections, etc…) 3.2.3 Complexity of interactive features The way we define interactivity is not just a matter of taste or preference: it has a significant impact on the complexity and feasibility of our systems. As we will see in Section 3.2.5, some simple features are fairly easy to implement, requiring just a thin interactive layer over a static data visualization system, whereas others come with a significant overhead, requiring an entirely different framework than static visualization. To illustrate the point with a particularly blunt example, many programming languages support a read-evaluate-print loop (REPL). This allows interactive code execution from the command line: the user inputs code, the interpreter evaluates it, outputs results, and waits for more input. If the language supports plotting, using the REPL to generate plots could be interpreted as an exercise in interactive data visualization, since the user can interact with the command line to modify visual output, and, if they type fast enough, updates can appear almost instantly (thus satisfying the user interaction and real-time update definitions of interactivity, see Table 3.1). This interpretation would turn many programming languages into “interactive data visualization systems”. However, I contend that this interpretation stretches the contemporary understanding of interactivity. While the command line was historically considered a highly interactive user interface (see e.g. Foley 1990; Howard and MacEachren 1995), advancements in processor speeds and the proliferation of highly-responsive graphical user interfaces (GUIs) have shifted user expectations. These days, interactivity tends to be associated with direct manipulation of visual elements and immediate feedback (Dimara and Perin 2019; Urbanek 2011; Vanderplas, Cook, and Hofmann 2020). Consequently, a REPL is unlikely to be considered an interactive data visualization platform by most contemporary users. But even with figures that are manipulated directly, there still are considerable differences in what different features imply for implementation requirements. Some features, like changing color or opacity of points in a scatterplot affect only the visual attributes of the plot and not the underlying data representation. This makes them simple to implement as they do not require any specialized data structures or complex computations, and the primary cost lies in re-rendering the visualization. In contrast, some interactive features require a lot more infrastructure. For instance, filtering, linked highlighting, or parametric interaction require specialized data structures and algorithms beyond those that would be required in static plots. This is because, each time the user engages in an interaction, entirely new summaries of the underlying data may need to be computed. To give a concrete example, when a user selects several points in a linked scatterplot (see Section 3.2.5.8), we first have to find the ids of all the selected cases, recompute the statistics underlying all other linked plots (such as counts/sums in barplots or histograms), train all of the relevant scales, and only then can we re-render the figure. Likewise, when interactively manipulating a histogram’s binwidth, we need to recompute the number of cases in each bin whenever the binwidth changes. To maintain the illusion of smooth, “continuous” interaction (Dimara and Perin 2019), these computations need to happen fast, and as such, computational efficiency becomes imperative at high data volumes. 3.2.4 Working definition As discussed in previous sections, the definition “interactive data visualization” varies across fields and researchers. Moreover, when building interactive data visualization systems, different definitions imply varying levels of implementation complexity. Thus, we need to establish clear criteria for our specific definition. Data visualization can be broadly categorized into two primary modes: presentation and exploration. While both modes share a bulk of common techniques, each comes with a different set of goals and challenges (Kosara 2016). Data presentation starts from the assumption that we have derived most of the important insights from our data already, and the goal is now to communicate these insights clearly and make an impactful and lasting impression (Kosara 2016). In contrast, data exploration begins from a position of incomplete knowledge - we accept that there are facts about our data we might not be aware of. Thus, when we explore data with visualizations, the goal is to help us see what we might otherwise miss or might not even think to look for (Tukey et al. 1977; Unwin 2018). However, it is not always the case that more complex visuals necessarily translate to better statistical insights. In static visualization, it is a well-established that plots can include seemingly sophisticated features which do not promote the acquisition of statistical insights in any way (Cairo 2014, 2019; Gelman and Unwin 2013; Tufte 2001). Similarly, adding interactivity to a visualization does not always improve its statistical legibility (see e.g. Abukhodair et al. 2013; Franconeri et al. 2021). I propose to treat interactive features the same way we treat visual features in static visualization. Specifically, I propose the following working definition: When building interactive data visualization systems, we should prioritize interactive features which promote statistical understanding. If we accept this proposition, then several important consequences follow. First, we must favor high-level, data-dependent, parametric interactions over the purely graphical ones. That is not to say that purely graphical interactive features cannot useful. For instance, in the case of overplotting, changing the size or alpha of points in a scatterplot can help us see features that would otherwise remain hidden. Nevertheless, I argue that the ability to see entirely new representations of the data is what makes some interactive data visualizations systems particularly powerful. The interactive features that enable this, such as linked highlighting and parameter manipulation, go beyond aesthetics, and empower the users to explore the data in a much more dynamic way, compared to static graphics. 3.2.5 Common interactive features This section describes several common types of interactive features that tend to frequently appear in general interactive data visualization systems. It is only meant as an overview (for more comprehensive taxonomies of interactive features, see Dimara and Perin 2019; Unwin et al. 2006; Yi et al. 2007). For each feature, I highlight its core properties, common use cases, and implementation requirements. 3.2.5.1 Changing size and opacity One of the simplest and most widely-implemented interactive features is the ability to adjust the size and opacity of geometric objects. This feature gives the user the ability to dynamically shrink or grow objects and make semi-transparent, fully transparent, or opaque. The ability to shrink objects or make them semi-transparent can be particularly useful at high data volumes, since this can reveal trends that may be otherwise hidden due to overplotting. For example, in scatterplots, shrinking points and making them semi-transparent makes it possible to identify high-density regions and can in fact provide an approximation to a 2D kernel density plot (see e.g. Dang, Wilkinson, and Anand 2010). The same applies to all other types of plots where the where objects or glyphs may be plotted on top of each other at high densities, such as parallel coordinate plots (Theus 2008). This feature usually fairly easy to implement, since it involves manipulating visual attributes only. Specifically, in many interactive systems, size and alpha multipliers are independent parameters of the visual representation, which do not depend on the underlying data in any way. In other words, when we manipulate size or opacity of geometric objects in our plots, we do not need to worry about what data these objects represent. Compared to other interactive features, this makes it relatively simple to add this functionality to an existing static visualization system (see Braşoveanu et al. 2017). 3.2.5.2 Zooming and panning Another two significantly related interactive features are zooming and panning. They are often used in tandem, and both involve interactive manipulation of scale limits. For this reason, I discuss them here simultaneously, in a single subsection. Zooming, depicted in Figure 3.8, allows the user to magnify into a specific region of a plot. A common approach involves creating a rectangular selection and the axis scales are then automatically adjusted to match this region, however, other techniques do exist, for instance a symmetric zoom centered on a point using a mouse wheel. Zooming is useful because it allows the user to get a better sense of the trend within the magnified region, and discover patterns that may be otherwise obscured due to overplotting or improper aspect ratio (see e.g. Buja, Cook, and Swayne 1996; Dix and Ellis 1998; Unwin 1999; Theus 2008; Yi et al. 2007). Figure 3.8: Zooming involves shrinking the axis limits to obtain a more detailed view of the data. Typically, the user selects a rectangular region of the plot (left) and the plot scales are then adjusted so that the region fills up the entire plot area (right). Notice the change in the axis limits. After zooming, it is useful to retain the ability to navigate the wider plot region while preserving the current zoom level and aspect ratio. Panning addresses this need. By performing some action, typically right-click and drag, the user can move the center of the zoomed-in region around, exploring different areas of the plot. Figure 3.9: Panning involves moving the axis limits while retaining the same zoom level and axis ratio. After zooming into a rectangular region (top row), the user can around the plot region, usually by clicking and dragging (bottom row). Zooming and panning can be implemented by manipulating scales only, and this also makes them generally fairly straightforward to implement, similar to changing size and opacity. However, there are a few issues to consider. First, whereas continuous axes can be be zoomed and/or panned by simply modifying the axis limits, zooming discrete axes requires a bit more nuance (see e.g. Wilkinson 2012). Second, it is often desirable to give the user the ability to zoom-in multiple levels deep, and this makes maintaining a reversible history of previous zoom-states essential (Unwin 1999). Third, at times, it can be useful to link scale updates across multiple plots, such that, for example, zooming or panning a plot in a scatterplot matrix produces the same actions in other plots with the same variable on one of the axes. Finally, an advanced feature that can be also quite useful is semantic or logical zooming (Keim 2002; Unwin 1999; Yi et al. 2007). This technique goes beyond magnifying objects; it also increases the level of detail the objects display as the user zooms in. Semantic zooming can be particularly powerful when combined with hierarchical data such as geographic information, however, it also introduces additional complexity, since the effects of the zoom action propagate beyond x- and y-axis scales. 3.2.5.3 Querying Querying is another popular interactive feature that is usually fairly straightforward to implement. As shown in Figure 3.10, the way querying is typically implemented is that when a user mouses over a particular geometric object, a small table of key-value pairs is displayed via a tool-tip/pop-up, showing a summary of the underlying data point(s) (Urbanek and Theus 2003; Xie, Hofmann, and Cheng 2014). This makes it possible to look up precise values that would otherwise be available only approximately via the visual representation. Figure 3.10: Querying involves hovering over an object to display its associated data values in a table or pop-up. Notice that this can include both plotted values (weight, mileage) as well as values that are not directly represented in the plot (car name, cylinders). Querying is useful because it combines the best features of graphics and tables. Specifically, it allows the user to overcome Tukey’s famous prescriptions: “graphics are for the qualitative/descriptive […] never for the carefully quantitative (tables do that better)”, and: “graphics are for comparison […] not for access to individual amounts” (Tukey 1993). By providing the option to query individual objects, the user can seamlessly transition between the high-level analytic overview of the graphic and low-level quantitative detail of a table. This facilitates high-precision analytic tasks, such as identifying specific outliers or calculating exact magnitudes of differences (Unwin et al. 2006). Additionally, querying also allow us to show more information than is displayed via the visual encodings alone (see again Figure 3.10). Specifically, whereas most plots can encode only two or three variables, we can assign an arbitrary number of key-value pairs to the rows of the query table/pop-up. However, it is crucial to balance the level of detail against visual clutter. Too many rows may overtax the attention of the user and also can lead to clipping/overplotting issues, if the query table cannot fit inside the plotting area. Further, there are better methods for retrieving very detailed information from interactive visualizations. Finally, while querying is also one of the more straightforward features, its implementation does present certain challenges. First, a naive implementation might simply display derived data values in the state just before they are mapped to visual attributes via scales, however, these are not always the most informative. For instance, in a stacked barplot, returning the original (unstacked) values is more useful than the stacked ones. Second, aggregate plots such as barplots or histograms do generally present some design decisions (see Unwin et al. 2006). In the case of one-to-one plots such as scatterplots, query data for an object (point) can be obtained by simply retrieving the corresponding row. However, in aggregate plots like barplots and histograms, a single object may correspond to multiple rows. This necessitates summarizing the underlying data, and often there may be no single “correct” summary. For instance, when querying a bar in a barplot, should we return the sum of the underlying continuous variable, some other numeric summary such as the mean or maximum, the set of all unique values, multiple of these summaries, or perhaps something else entirely? Similar ambiguities arise when querying objects which are partially selected or highlighted (see Section 3.2.5.8): should the query return summaries corresponding to the entire object, the highlighted parts, or both? 3.2.5.4 Sorting and reordering With plots of discrete (unordered) data, a highly useful feature can be to sort or reorder objects based on some criterion (see Unwin 2000; Unwin et al. 2006). For example, with barplots, in the absence of other ordering rules, bars are typically ordered by the lexographical order of the x-axis variable. However, sometimes, we can glean interesting patterns by sorting the bars in some other order, for example by their height, see Figure 3.11. Figure 3.11: Sorting or reordering can highlight interesting trends. For instance, sorting lexicographically ordered bars (left) by bar height (right) in the figure above immediately reveals a significant gap between the five tallest bars and the rest (gray dashed line). There are more sophisticated ways to sort objects in a plot than just sorting bars by height, however. For instance, in plots which show multiple summary statistics, any may serve as the basis for the sorting rule; for instance a boxplot may be sorted by the median, upper and lower quartile, the maximum, and the minimum (Unwin et al. 2006). Likewise, in the presence of selection/highlighting, objects may be sorted by the summary statistic on the highlighted parts. Alternatively, some systems allow users to permute the order of discrete scales manually by swapping the position of categories pairwise, a feature which can be particularly useful in parallel coordinate plots (Unwin et al. 2006; Urbanek 2011). Finally, in the presence of many categories, sorting may also be usefully combined with lumping categories below a certain threshold together (Unwin 2000). Like zooming and panning, basic sorting typically involves the manipulation of axis scales only, making it also a fairly straightforward feature to implement. However, the more sophisticated sorting features can pose non-trivial implementation challenges (Unwin et al. 2006). For instance, sorting by custom summary statistics or manually permuting discrete scale order may require specialized system components and behavior. 3.2.5.5 Parametric interaction As discussed in Section 3.2.3, another valuable class of interactive features are those which affect the computation of the summary statistics underlying the graphic Wilhelm (2003). These features extend beyond simple manipulation of visual attributes, requiring that user interaction penetrates much deeper into the data visualization pipeline. Fundamentally, these features involve the manipulation of the parameters of some underlying mathematical model or algorithm. An illustrative and popular example of parameter manipulation is dynamically changing histogram binwidth or anchor. Assuming a fixed binwidth \\(w\\) and an anchor \\(a\\), we can describe a histogram via a function \\(h\\) that, for each observation of a continuous variable \\(x_i\\) returns an index \\(j\\) of the corresponding bin, such that, for an ordered list of bins breaks \\(b_j\\), we have \\(x_i \\in [b_{j}, b_{j + 1})\\)1: \\[h(x_i; a, w) = \\lfloor (x_i - a) / w \\rfloor + 1\\] Thus, a histogram really is a kind of a mathematical model, and can in fact be seen as a crude form of density estimation (see e.g. Bishop and Nasrabadi 2006, 4:120–22). Manipulating histogram bins amounts to manipulating the parameters of the function \\(h\\). Crucially, unlike changes to surface-level visual attributes like size or opacity, changing binwidth or anchor requires recomputing the underlying summary statistics (Urbanek 2011). As noted in Section 3.2.3, these changes can have significant downstream effects. For instance, increasing the binwidth may cause certain bins to contain more data points than the current maximum, potentially requiring the adjustment of the upper y-axis limit, to prevent the bars from overflowing the plotting area. There are other, more complex types of parametric interaction, than just changing histogram binwidth or anchor. These include, for example, modifying the bandwidth of a kernel density estimator, specifying the number of clusters in a clustering algorithm, or manipulating splitting criteria in classification and regression trees, as well as regularization parameters in smooth fits (for some more examples, see Leman et al. 2013; Self et al. 2018). Because parametric interaction necessitates recalculating the plot’s underlying summary statistics, it is both more computationally expensive and as well as more difficult to implement. The interactive system must be able to respond to user input by recomputing relevant summaries and updating dependent plot parameters. In some systems such as Shiny (Chang et al. 2024), the common approach is to re-render the entire plot from scratch each time any interaction occurs. However, this can become prohibitively expensive when these deep, parametric interactions are combined with rapid interactions closer to the surface of the visualization pipeline. Thus, the development of generic and efficient data visualization pipelines still remains an open research problem (Wickham et al. 2009; Vanderplas, Cook, and Hofmann 2020; Xie, Hofmann, and Cheng 2014). 3.2.5.6 Animation and projection A particularly useful form of parametric interaction involves the ability to control a continuous traversal through a series of states, observing the resulting changes as animation. This technique is especially useful when combined with projective techniques such as the grand tour (see C. Chen et al. 2008; for a recent comprehensive review, see Lee et al. 2022), and for this reason I discuss them both here, within the same subsection. A common and straightforward application of interactive animation is visualizing transitions in data subsets ordered by a specific variable, such as time. A particularly famous example of this technique is the interactive animation of the Gapminder data set (Rosling and Zhang 2011), which illustrates the joint evolution of GDP and life expectancy for countries worldwide. The interactive control of the timeline (play, pause, and pan) empowers users to explore time-dependent trends within this relatively high-dimensional data set, revealing trends that would be challenging to visualize by other means. For instance, the visualization clearly depicts the profound drop in both GDP and life expectancy during the second world war, followed by the subsequent rapid recovery and growth after 1945. Interactive animation becomes particularly powerful when coupled with techniques like the grand tour (Asimov 1985; Buja and Asimov 1986; Cook et al. 1995), designed for exploring high-dimensional datasets. Because data visualizations are typically limited to two dimensions, effectively representing high-dimensional data is challenging. The grand tour technique addresses this issue by projecting the data onto a series of lower-dimensional (two-dimensional) subspaces, interpolating between these projections, and animating the results to create a “tour” of different data views (Cook et al. 1995). By surveying this series of projections, the users may discover high-dimensional outliers, clusters, or non-linear dependencies (Wickham 2011), and this discovery can be greatly aided by interactive controls of the animation’s timeline or even manual control of the tour’s direction (C. Chen et al. 2008; Lee et al. 2022). Finally, the technique also integrates well with other interactive features, such as linked selection and querying/tooltips (Cook et al. 1995; Wickham 2011; Lee, Laa, and Cook 2022; Lee et al. 2022). The implementation complexity of interactive animation varies considerably depending on its application. While animating data subsets based on a single variable, as in the Gapminder visualization (Rosling and Zhang 2011), presents no greater implementation challenges than previously discussed techniques, computing the grand tour path requires specialized algorithms (see, e.g., C. Chen et al. 2008, for a brief description). However, if the data subsets corresponding to each animation frame are pre-computed, the animation itself is generally fairly straightforward to implement. 3.2.5.7 Representation switching Another specialized kind of parametric (or semi-parametric) interaction involves changing the representation of the underlying data. It is well known that the same data can often be visualized using various sets of visual encodings (Wilkinson 2012), with some being more effective for answering specific questions than others. Enabling users to switch between these various representations provides greater flexibility for data exploration (Yi et al. 2007). However, for certain plot types, changing the representation involves more than just altering surface-level visual attributes; it also necessitates recalculating derived statistics. A typical example is switching between a barplot and a spineplot, see Figure 3.12. Barplots are effective for comparing absolute quantities. Specifically, by encoding categories along the x-axis and continuous quantities along the y-axis (bar height), we can easily compare the quantities across categories. Color-coding parts of the bars as segments allows us to visualize a second categorical variable, enabling subgroup comparisons of absolute values. However, barplots are less well-suited for comparing the proportions represented by these segments, particularly when bar heights vary considerably. Figure 3.12: Switching representation can be an effective way to derive new insights from the data. A barplot (left) represents the same underlying data as a spineplot (right), however, the former is better for comparing absolute counts whereas the latter is better for comparing proportions. Note that in the spineplot, it is much easier to see that the proportion of the red cases is the same in categories B and C. Spineplots, on the other hand, present a way of visualizing the same sort of data as a barplot while making it much easier to compare proportions. Specifically, in a spineplot, the heights of the bars are all normalized to 1, such that the segments show a proportion of the total, and the original values are instead encoded as the bar width, which is stacked along the x-axis. Thus, the fixed height of bars makes it easy to compare the segments proportionally. Other examples of representation switching include switching from a histogram to spinogram (a normalized version of the histogram) and switching between aggregate and one-to-one geometric objects (e.g. boxplot and pointclouds, parallel coordinate plots and individual points, see Wilhelm 2003). 3.2.5.8 Linked selection Linked selection, also known as linked brushing, linked highlighting, or linked views, is often considered one of the most versatile and powerful interactive data visualization features (see e.g. Becker and Cleveland 1987; Buja, Cook, and Swayne 1996; Wilhelm 2003; Heer and Shneiderman 2012; Ward, Grinstein, and Keim 2015; Ware 2019). Fundamentally, it involves creating a figure with multiple “linked” plots. The user can then click or click-and-drag over objects in one plot, and the corresponding cases are highlighted across all the other plots, see Figure 3.13. This makes it possible to quickly quickly explore trends across different dynamically-generated subsets of the data (Dix and Ellis 1998). The ability to quickly materialize alternative views of the data makes this a particularly effective tool for data exploration (Wilhelm 2008; Wills 2008). (ref:henderson) Figure 3.13: Linked selection involves highlighting the same cases across all plots. The user can select some objects in one plot, such as points in a scatterplot (top left), and the corresponding cases are higlighted in all the other plots. Source of the underlying data is the mtcars dataset (ref:henderson). Despite the fact that the user experience of linked selection is usually fairly intuitive, there are many subtle considerations that go into implementing the feature (for a good overview, see Wilhelm 2008). First, there is the issue of how the user makes the selection. Typically, clicking selects a single objects and clicking-and-dragging selects multiple objects in a rectangular region (similar to how selecting files and folders works on desktop GUIs of most operating systems). In some systems, the users may also drag the selection region around (“brushing”), form a continuous “lasso” selection, select lines in a particular angular range, or points at a particular distance from a centroid (see e.g. Hauser, Ledermann, and Doleisch 2002; Splechtna et al. 2018; Wills 2008). Further, when one variables is continuous and the other is derived (such as the x- and y-axes in a histogram), the interaction may also be simplified by restricting selection/brushing to the primary axis (Satyanarayan et al. 2016). Finally, the selections can be combined by various operators such as OR, AND, NOT, and XOR, to form unions, intersections, and other types of logical subsets (Theus 2002; Urbanek and Theus 2003; Wills 2000, 2008). Second, there is the issue of who should dispatch and respond to selection events. In presentation-focused interactive data visualization and dashboarding systems, this responsibility is kept flexible, such that some plots may only dispatch, only respond, do both, or neither (Satyanarayan et al. 2015, 2016). However, in systems focused on data exploration, the convention is typically for all plots to both dispatch and respond to selection events, such that they may be interacted with in the same way. (Theus 2002; Urbanek and Theus 2003; Urbanek 2011). Third, there is the issue of what to link. In the case of data represented by a two-dimensional table or data frame, the most common method is to link cases taken on the same observational level (identity linking), such that each row gets assigned a value representing the selection status (Urbanek and Theus 2003; Wilhelm 2008; Wills 2008). However, in the case of more complex data, more advanced linking schemes are also available, such as hierarchical and distance-based linking (Wilhelm 2008; Urbanek 2011). Third, there is the issue of displaying selection. This issue will be touched upon in more detail later, in Section 4. Briefly, Wilhelm (2008) identifies three methods for displaying linked selection: replacement, overlaying, and repetition. Replacement involves replacing the entire plot with a new graphic; overlaying involves superimposing the objects representing the selected subsets on top of the original objects; and repetition involves displaying the selected objects alongside the original ones. Wilhelm identifies issues with all three techniques, although he does seem to generally come down in favor of repetition (however, see my argument in Section 4.3.1.4). A fourth and final issue in linked selection, and arguably one of the core concerns of the present thesis, is consistency. This topic will be coming up again and again, particularly in Section 4. Consistent and predictable features are a cornerstone of good user interface design (see e.g. Ruiz, Serral, and Snoeck 2021). However, as discussed above, the design an interactive data visualization system supporting linked selection presents many design decisions, each with its own set of implementation constraints. Achieving a consistent user interface through the right combination of these decisions is a known challenge (Urbanek 2011; Pike et al. 2009). For example, while the approach of allowing objects to independently dispatch and display selection events offers great flexibility, it can also lead to a less intuitive user experience. Put simply, when users select objects in one linked plot by clicking them, they might reasonably expect the same behavior in other plots. If that is not the case (if, for instance, other plots support only displaying but not dispatching selection events), their expectations may be violated, leading to a subpar user experience. Thus, giving all plots the ability to dispatch and display selection events may be a desirable goal. However, as I will repeatedly demonstrate in this thesis, this places some fundamental very constraints on the objects in these plots, and the summary statistics they represent. As an example of a visualization type which presents some difficulties for linked selection, consider the lineplot in Figure 3.14. In a lineplot, lines are draw by connecting pairs of points in an ordered sequence. This presents some fundamental problems for linked selection. First, when points are selected, should we highlight the line segments starting at the selected points, ending at the selected points, or e.g., half a of a line segment on each side of the point? Either way, if we highlight the segments, we are faced with the problem that the line object (unlike e.g. a barplot bar) is not commutative with respective to the data order: segments have to be drawn in the order in which they appear in the data, and this can lead to striped “candy-cane-like” patterns which are not easy to interpret. Alternatively, we could draw the selection via two separate lines, then we are faced with the question of how we should dispatch selection events on these lines, which are already conditional on selection. Either way lie complex trade-offs and design decisions. Like turning over a rock and uncovering a bed of creepy-crawlies, linked selection exposes a web of visualization design challenges that defy a simple and generic solution. Figure 3.14: Displaying selection is not always trivial. A good example is a lineplot (right). Whereas a point in a scatterplot displays a single case (row) and a bar in a barplot displays a closed subset of cases, a line segment in a lineplot segments connects two separate data points. As such, it’s not obvious how to handle the case when one point is selected and the other is not. Further, since the geometry of a segmented line is not commutative (row order matters), highlighting segments may result in a striped ‘candy cane’ pattern that may not be easily interpretable. 3.3 General data visualization theory The following sections briefly overviews several key, general topics in data visualization: the goals and purpose of visualizing data, the mechanisms of visual perception, the theory of scales and measurement, and graphics formats. While mainly discussed in the context of static visualization, these topics are equally relevant to interactive visualization and present some unique challenges. My goal is not to give an exhaustive review - each of these topics is substantial enough to serve as a thesis topic in its own. Instead, I just want to give a brief overview of these topics, highlight some important points, and provide background information that may be referred to later on in the thesis. 3.3.1 Visualization goals An important fact about data visualization is that, fundamentally, a chart can be used by many different people for many different things (for a review, see e.g. Brehmer and Munzner 2013; Franconeri et al. 2021; Sarikaya et al. 2018). For example, applied researchers may create figures as part of their workflow, aiming to better understand the data they had collected, spot errors and anomalies, and come up with new ideas and hypotheses (Brehmer and Munzner 2013; see also Kandel et al. 2012). Conversely, data scientists and data analysts in the public and private sector may visualize already familiar data sets to communicate important information, drive decisions, or convince or persuade stakeholders (Sarikaya et al. 2018). Finally, some figures may be created out of a sense of curiosity or for pure aesthetic enjoyment (Brehmer and Munzner 2013; Tufte 2001). Depending on the end-goals of the user and the desired target audience, certain visualization techniques, methods, or styles may become more useful than others. As mentioned in Section 3.2.1, much has been written about the goals and experiences a user might have while creating data visualizations. For instance, Brehmer and Munzner (2013) formalized a typology of abstract visualization tasks, based around three adverbs: why is a task is performed, how it is performed, and what does it pertain to. In the why part of their typology, they list the following reasons for why a user may engage in the process of visualizing data: to consume (present, discover, or enjoy), produce, search (lookup, browse, locate, and explore), and query (identify, compare, summarize). As another example, Pike et al. (2009) list the following high-level goals a user might have when interacting with a visualization: explore, analyze, browse, assimilate, triage, asses, understand, compare. And there are many other typologies and taxonomies of data visualization tasks and goals in the literature. Personally, when it comes to classifying interactive data visualization goals, I prefer the following short list provided by Ward, Grinstein, and Keim (2015): Exploration: The user wants to examine a data set Confirmation: The user wants to verify a fact or a hypothesis Presentation: The user wants to use the visualization to convince or inspire an audience Interactive presentation: The user wants to take the audience on a guided tour of key insights I believe this list maps fairly well onto interactive data visualization systems found in the wild, such as the ones discussed in Section 3.1. Specifically, as mentioned before, in the history of interactive data visualization, the earlier statistical systems seemed to primarily focus on exploration and confirmation, whereas the newer web-based systems seem to prioritize presentation. The interactive presentation category is interesting, since, I would argue, it is far more specific and less common than the other categories, however, by singling it out, Ward et al. make an interesting point. By incorporating time and intentionality, sequential interactive presentations, such as those found in the Graphics section of the New York Times (The New York Times Company 2025), really are quite unique. 3.3.2 Visual perception Another broad and important research topic in data visualization is visual perception. By engaging the human visual system, graphs serve as information channels, with varying throughput based on their inherent attributes. To maximize the effectiveness of visualizations, researchers have spent significant amount of effort figuring out best ways to engage the visual system. Fortunately, this has been quite fruitful, yielding many precise and actionable guidelines (for a review, see e.g. Franconeri et al. 2021; Quadri and Rosen 2021; Vanderplas, Cook, and Hofmann 2020). A landmark study in this area has been that of Cleveland and McGill (1984). Through a series of empirical experiments, the authors investigated the accuracy with which people judge quantities based on different visual encodings. They found that quantities encoded as position along a common scale tended to produce by far the most accurate judgements, followed by length-, and angle-based encodings (Cleveland and McGill 1984). These findings were later independently replicated on larger samples (see e.g. Heer and Bostock 2010), and extended to other visual encodings such as hue or density (see e.g. Demiralp, Bernstein, and Heer 2014; Saket et al. 2017; Reda, Nalawade, and Ansah-Koi 2018; see also Vanderplas, Cook, and Hofmann 2020). Ultimately, researchers have used this empirical data to create rankings of visual encodings, with widely accepted order being, roughly: position, length, area, angle, hue/intensity, and shape (see e.g. Mackinlay 1986; Franconeri et al. 2021; Vanderplas, Cook, and Hofmann 2020; some authors also emphasize task-dependence, see Quadri and Rosen 2021). Beyond Cleveland and McGill (1984), there have also been many findings from cognitive research which have been shown to enhance the effectiveness of visualizations. For instance, pre-attentive processing (treissman1985?), a phenomenon in which certain salient features of visual scenes are perceived extremely rapidly and in parallel, regardless of the number of distractors, has been successfully applied to data visualizations (see Vanderplas, Cook, and Hofmann 2020; for some applied examples, see Wilke 2019, 34 and 35). Similarly, research on optical illusions and misperceptions has also been used to highlight many potential data visualization pitfalls (see Franconeri et al. 2021; Ware 2019). Psychology has also yielded a more high-level, conceptual set of guidelines for creating effective visualizations: the Gestalt principles of visual perception (see e.g. Cairo 2012; Vanderplas, Cook, and Hofmann 2020; Ware 2019). Remarkably, these principles developed in the early 20th century by means of self-experimentation and in the absence of sound understanding of brain physiology, have stood the test of time and have been later independently validated by findings from cognitive neuroscience (Guberman 2017; Ware 2019). The key Gestalt principles for data visualization are: Proximity: Objects close to each other are perceived as part of a group Similarity: Objects with similar visual attributes are also grouped together Closure/common region: Regions with closed contours tend to be perceived as a single, unified object Figure and ground: One part of a visual scene is typically perceived as central (the “figure”), and the rest is perceived as background (the “ground”) These principles apply to visualizations in many ways. At times, they can manifest automatically, without conscious effort, such as when clouds of similarly coloured points in a scatterplot are automatically perceived as clusters (thanks to principles of proximity and similarity, Rosli and Cabrera 2015). Other times, the visualization author may choose to leverage the principles deliberately, to make a specific point, such as by grouping certain data points by drawing a closed shape around them or making them stand out by using hue and/or saturation constrast (Cairo 2012). 3.3.3 Scales and measurement Visualizing data involves mapping values to graphical attributes. As discussed in the previous section, certain visual attributes are better for visualizing particular types of data, and vice versa. However, even when we pick an appropriate visual attribute to represent our data with, there are still many choices in how to perform the mapping. For instance, suppose we have some variable \\(x\\) with values \\(\\{ 1, 2, 3 \\}\\). Should these be treated as magnitudes, a simple ordering, or even just category labels that may be permuted at will? In most data visualization systems, this metadata encoding of values into visual attributes is handled specialized components called scales or coordinate systems, and I will discuss their implementation in detail later, in Section 4.4.0.3. However, it is first necessary to discuss some theoretical issues involving scales. A particular challenge when discussing scales in data visualization is that the topic unavoidably intersects with a research area that has a particularly long and contentious history: theory of measurement (see e.g. Hand 1996; Michell 1986; Tal 2025). Theory of measurement (not to be confused with measure theory, with which it nevertheless shares some overlap) is the research area which tries to answer the deceptively simple question: what does it mean to measure something? This seemingly trivial problem has inspired long and fiery debates within the fields of mathematics, philosophy, and social science. Particularly, in psychology, where assigning numerical values non-physical phenomena such as moods and mental states is a central concern, the topic has garnered a significant amount of attention, creating a dense body of research (see e.g. Humphry 2013; Michell 2021). Arguably, the most influential work in this field has been that of Stevens (1946). In his fairly concise paper, Stevens defined a scale as method of assigning numbers to values, and introduced a four-fold classification classification, namely: nominal, ordinal, interval, and ratio scales (see Table 3.3). (ref:eq1)$ x’ = f(x)$, where \\(f\\) is a bijection Table 3.3: Table 3.4: Types of scales identified by Stevens (1946) Scale Structure Comparison Valid transformations Nominal Isomorphism Are \\(x\\) and \\(y\\) the same? \\(x&#39; = f(x)\\), where \\(f\\) is a bijection Ordinal Monotone map Is \\(x\\) is greater than \\(y\\)? \\(x&#39; = f(x)\\), where \\(f\\) is monotone Interval Affine transformation How far is \\(x\\) from \\(y\\)? \\(x&#39; = ax + b\\), for \\(a, b \\in \\mathbb{R}\\) Ratio Linear map How many times is \\(x\\) greater than \\(y\\)? \\(x&#39; = ax\\), for \\(a \\in \\mathbb{R}\\) The Steven’s (1946) typology is based on invariance under transformation. Specifically, for each class of scales, we define a set of transformations that preserve valid comparisons. The set of valid transformations shrinks as we move from one class of scales to another. For nominal scales, any kind of bijective transformation is valid. Intuitively, we can think of the scale as assigning labels to values, and any kind re-labeling is valid, as long as it preserves equality of the underlying values. For instance, given a nominal scale with three values, we can assign the labels \\(\\{ \\text{red}, \\text{green}, \\text{blue} \\}\\) or \\(\\{ \\text{monday}, \\text{tuesday}, \\text{wednesday} \\}\\) in any way we like, as long as each value maps to a unique label. This identifies the underlying mathematical structure as an isomorphism. Ordinal scales are more restrictive, since, on top of preserving equality, transformations also need to preserve order. For example, if we want to assign the labels \\(\\{ \\text{monday}, \\text{tuesday}, \\text{wednesday} \\}\\) to an ordinal scale with three values, there is only one way to do it that preserves the underlying order: assign the least values to \\(\\text{monday}\\), the middle value to \\(\\text{tuesday}\\), and the greatest value to \\(\\text{wednesday}\\) (assuming we order the labels/days in the usual day-of-week order). However, there is no notion of distance between the labels: we could just as well assign the values labels in \\(\\mathbb{N}\\) such as \\(\\{ 10, 20, 30 \\}\\), \\(\\{1, 2, 9999 \\}\\), and so on. Thus, the fundamental mathematical structure is that of a monotone map. Interval scales need to additionally preserve equality of intervals. This means that, for any three values \\(a, b,\\) and \\(c\\), if the distances between \\(a\\) and \\(b\\) and \\(b\\) and \\(c\\) are equal, \\(d(a, b) = d(b, c)\\), then so should be the distances between the scaled labels, \\(d^*(f(a), f(b)) = d^*(f(b), f(c)\\). For most real applications, this limits interval scales to the class of affine transformations of the form \\(f(x) = ax + b\\). A canonical example of an interval scale is the conversion formula of degrees Celsius to Fahrenheit: \\(f(c) = 9/5 \\cdot c + 32\\) (Stevens 1946). This example also highlights an important property of interval scales: the zero point can be arbitrary and ratios are not meaningful. Specifically, since the zero points of both Celsius and Fahrenheit scales were chosen based on arbitrary metrics (freezing temperatures of water and brine, respectively), it does not make sense to say that, e.g. 20°C is “twice as hot” as 10°C, in the same way that it does not make sense to say that 2000 CE is “twice as late” as 1000 CE. Finally, ratio scales also need to preserve the equality of ratios. Specifically, if \\(a/b = b/c\\) then \\(f(a)/f(b) = f(b) / f(c)\\). As a consequence, this also means that the scale must have a well-defined zero-point. Examples of ratio scales include physical magnitudes such as height and weight, which have a well-defined zero point (Stevens 1946). Steven’s (1946) typology sparked a considerable debate, on multiple fronts. First, since the original publication, many authors have sought to either expand upon or criticize Steven’s typology. However, despite some monumental efforts towards a unified theory, such as that of Krantz et al. (1971), measurement has remained a hotly debated topic to this day (see e.g. Michell 2021; Tal 2025). Second, more relevant to statistics, some authors such as Stevens (1951) and Luce (1959) have used the theory to come up with prescriptive rules for statistical transformations, suggesting that, for example, taking the mean of an ordinal variable is wrong since the meaning of the average operator is not preserved under monotone transformations. However, this issue was hotly contested by others, such as Lord (1953), Tukey (1986), and Velleman and Wilkinson (1993), who argued that many well-established statistical practices, such as rank-based tests and coefficients of variations, rely on such “impermissible” statistics but can nevertheless yield valuable insights. More broadly, these authors also argued that data is not meaningful on its own, but instead derives its meaning from the statistical questions it is used to answer (see also Wilkinson 2012). At this point, the discussion around measurement has arguably become far too dense and theoretical, and most data visualization researchers seem to avoid delving into it too deeply (see e.g. Wilkinson 2012). Nevertheless, there are still some areas where the issues of measurement and Steven’s typology do crop up. For instance, when scaling area based on a continuous variable, a common recommendation is to start the scale at zero to ensure accurate representations of ratios (see e.g. Wickham and Navarro 2024), aligning with Steven’s definition of a ratio scale. Likewise, the long-standing debate around whether the base of a barplot should always start at zero (see e.g. Cleveland 1985; Wilkinson 2012) also carries echoes of the measurement debate. Ultimately, it may yet require long time to settle the issues around measurement, however, there are definitely some ideas within the literature that data visualization can benefit from. 3.3.4 Graphics formats In order to draw an image, be in on a computer screen or a piece of paper, we first need some way to encode it. Currently, the two most overwhelmingly popular classes of image encoding formats are raster- or bitmap-based, and vector-based. Both come with an inherent set of advantages and disadvantages, which will be discussed in the relevant subsections below. 3.3.4.1 Raster graphics Raster (also known as bitmap) graphics represent images as two-dimensional grid of pixels (see e.g. Beatty 1983; Foley 1996; Shirley, Ashikhmin, and Marschner 2009). More specifically, images are encoded as an array of bytes, with a header of metadata (the number of rows and columns, the pixel datatype) and subsequent bytes representing individual pixels. Each pixel typically consists of three or four numerical intensities, such as the red, green, and blue color intensities, as well as opacity (alpha channel). The raster format is fairly natural, since it maps directly on to how most computer screens (LCD/LED) and printers (laser, ink-jet) represent images (Shirley, Ashikhmin, and Marschner 2009). Additionally, raster graphics have the advantage of having a constant size, independent of the complexity of the encoded image (a grid of \\(n \\times m\\) pixels is always at most just a grid of \\(n \\times m\\) pixels), and can be further compressed to minimize space via either lossless (GIF, TIFF PNG) or lossy (JPEG) compression formats. However, raster graphics also have certain disadvantages. The constant space complexity can also lead to excessive file sizes for simple yet high-resolution images, and images cannot be easily upscaled, without specialized methods (see e.g. Parsania, Virparia, et al. 2014; G. Chen et al. 2019). Additionally, another major disadvantage is that the raster (array of pixels) represents mutable state: when we draw something on a raster graphic, we replace the corresponding pixels with new values, and there may be no way to recover the original state. As such, when working with raster graphics that change over time, such as in interactive data visualization, a common practice is to keep all of the image state separate from the graphic and do a complete re-render each time the state changes. 3.3.4.2 Vector graphics Vector represent a foil to raster graphics. Instead of representing a 2D images via arrays of pixels, they represent them as more abstract specifications involving collections of geometric objects. That is, instead of explicitly defining the value of each pixel on the screen, vector graphics consist of declarative “recipes”, specifying what should be rendered, but not how. Vector graphics offer certain significant advantages. Notably, they are resolution-invariant, meaning that the same image can be upscaled and downscaled without any loss in quality. This is due to how vector images are rendered: prior to being sent to the output device (screen or printer), they are rasterized, and can adapt to the output device’s resolution. Additionally, their declarative nature makes vector graphics stateless, such that all components of a vector graphic are always open to modification, and any modification can always be reversed. This makes vector graphics particularly simple to animate and modify interactively. However, the declarative nature of vector graphics also has its costs. Since every geometric object needs to be represented explicitly, the size of the image grows proportional to its complexity. For instance, ignoring compression, the size of a vector image with 1,000 geometric objects should be some factor of thousand times larger than a vector image with one geometric object (i.e. the space complexity is \\(O(n)\\), compared to \\(O(1)\\) of raster images). The complexity-dependence also translates to slower rendering speeds, since large, complex image files require more compute to parse and rasterize. As a side-note, this also makes representing of highly detailed, realistic images, such as photos, challenging. Currently, the most popular vector graphics format is Scalable Vector Graphics (SVG, Quint 2003). Based on the XML markup language, SVG encodes an image as a tree of nodes representing (primarily) geometric objects, similar to how the browser encodes a webpage as a tree of nodes in the Document Object Model (DOM; represented in plain-text as HTML). SVG files are typically stored in plain-text format, with nodes enclosed by tags which may be given attributes. 3.4 Summary In this section, I reviewed the history, present state, and some general theory regarding interactive data visualization systems. There are a couple of key points which bear repeating. First, as discussed in Section 3.1, there seems to have been a historical split in the interactive data visualization community. Oversimplifying somewhat, the older branch, originating in statistics, has focused primarily on applied data analysis and exploration, while the younger branch, stemming from computer science and the web ecosystem, has emphasized customizability and data presentation. Currently, this historical development has resulted in a scarcity of user-friendly interactive data visualization tools for data exploration, particularly in the R ecosystem (see e.g. Batch and Elmqvist 2017; Keller, Manz, and Gehlenborg 2024). Second, among the past and present interactive data visualization systems, there is an abundance of interactive features. These features range widely in terms of usefulness, complexity, and difficulty of implementation: some are purely graphical whereas others require the presence of specialized data structures. Further, as was briefly foreshadowed in, for example, Section 3.2.5.8, many of these features cannot be simply tacked on top of arbitrary graphics, but instead have intricate connections to the style of the visualization itself. These connections, and particularly those related to linked selection, will be the subject of large parts of Section 4. Finally, I also briefly reviewed some general theory related to visual perception and scales. These topics are not specific to interactive data visualization, however, they are important to touch on as they will be referred to in other parts of the thesis. Next, I will dive deeper into the challenges encountered while developing interactive data visualization systems, by discussing the four primary steps of the data visualization pipeline: partitioning, aggregation, scaling, and rendering. References Abbate, J. 1999. “Getting small: a short history of the personal computer.” Proc. IEEE 87 (9): 1695–98. https://doi.org/10.1109/5.784256. Abukhodair, Felwa A, Bernhard E Riecke, Halil I Erhan, and Chris D Shaw. 2013. “Does Interactive Animation Control Improve Exploratory Data Analysis of Animated Trend Visualization?” In Visualization and Data Analysis 2013, 8654:211–23. SPIE. Asimov, Daniel. 1985. “The Grand Tour: A Tool for Viewing Multidimensional Data.” SIAM Journal on Scientific and Statistical Computing 6 (1): 128–43. Backus, John. 1978. “The History of Fortran i, II, and III.” ACM Sigplan Notices 13 (8): 165–80. Batch, Andrea, and Niklas Elmqvist. 2017. “The Interactive Visualization Gap in Initial Exploratory Data Analysis.” IEEE Transactions on Visualization and Computer Graphics 24 (1): 278–87. Beatty, John C. 1983. “Raster Graphics and Color.” The American Statistician 37 (1): 60–75. Becker, Richard A, and William S Cleveland. 1987. “Brushing Scatterplots.” Technometrics 29 (2): 127–42. Beniger, James R, and Dorothy L Robyn. 1978. “Quantitative Graphics in Statistics: A Brief History.” The American Statistician 32 (1): 1–11. Bertin, Jacques. 1967. Sémiologie Graphique: Les diagrammes, les réseaux, les cartes. Gauthier-Villars. Bishop, Christopher M, and Nasser M Nasrabadi. 2006. Pattern Recognition and Machine Learning. Vol. 4. 4. Springer. Blokt. 2020. “Flare \\(\\vert\\) Data Visualization for the Web.” Blokt - Privacy, Tech, Bitcoin, Blockchain &amp; Cryptocurrency. https://blokt.com/tool/prefuse-flare. Bostock, Mike. 2022. “D3.js - Data-Driven Documents.” https://d3js.org. Braşoveanu, Adrian MP, Marta Sabou, Arno Scharl, Alexander Hubmann-Haidvogel, and Daniel Fischl. 2017. “Visualizing Statistical Linked Knowledge for Decision Support.” Semantic Web 8 (1): 113–37. Brasseur, Lee. 2005. “Florence Nightingale’s Visual Rhetoric in the Rose Diagrams.” Technical Communication Quarterly 14 (2): 161–82. Brehmer, Matthew, and Tamara Munzner. 2013. “A Multi-Level Typology of Abstract Visualization Tasks.” IEEE Transactions on Visualization and Computer Graphics 19 (12): 2376–85. Brodbeck, Dominique, Riccardo Mazza, and Denis Lalanne. 2009. “Interactive Visualization - A Survey.” In Human Machine Interaction, 27–46. Berlin, Germany: Springer. https://doi.org/10.1007/978-3-642-00437-7_2. Buja, Andreas, and Daniel Asimov. 1986. “Grand Tour Methods: An Outline.” In Proceedings of the Seventeenth Symposium on the Interface of Computer Sciences and Statistics on Computer Science and Statistics, 63–67. Buja, Andreas, Dianne Cook, and Deborah F Swayne. 1996. “Interactive High-Dimensional Data Visualization.” Journal of Computational and Graphical Statistics 5 (1): 78–99. Cairo, Alberto. 2012. The Functional Art: An Introduction to Information Graphics and Visualization. New Riders. ———. 2014. “Graphics Lies, Misleading Visuals: Reflections on the Challenges and Pitfalls of Evidence-Driven Visual Communication.” In New Challenges for Data Design, 103–16. Springer. ———. 2019. How Charts Lie: Getting Smarter about Visual Information. WW Norton &amp; Company. Chang, Winston, Joe Cheng, JJ Allaire, Carson Sievert, Barret Schloerke, Yihui Xie, Jeff Allen, Jonathan McPherson, Alan Dipert, and Barbara Borges. 2024. Shiny: Web Application Framework for r. https://CRAN.R-project.org/package=shiny. Chen, Chun-houh, Wolfgang Härdle, Antony Unwin, Dianne Cook, Andreas Buja, Eun-Kyung Lee, and Hadley Wickham. 2008. “Grand Tours, Projection Pursuit Guided Tours, and Manual Controls.” Handbook of Data Visualization, 295–314. Chen, Genlang, Huanyu Zhao, Christopher Kuo Pang, Tongliang Li, and Chaoyi Pang. 2019. “Image Scaling: How Hard Can It Be?” IEEE Access 7: 129452–65. Cheng, Joe, Winston Chang, Steve Reid, James Brown, Bob Trower, and Alexander Peslyak. 2024. Httpuv: HTTP and WebSocket Server Library. https://CRAN.R-project.org/package=httpuv. Clark, Lin. 2017. “A Crash Course in Just-in-Time (JIT) Compilers.” Mozilla Hacks the Web Developer Blog. https://hacks.mozilla.org/2017/02/a-crash-course-in-just-in-time-jit-compilers. Clayton, Aubrey. 2021. Bernoulli’s Fallacy: Statistical Illogic and the Crisis of Modern Science. Columbia University Press. Cleveland, William S. 1985. The Elements of Graphing Data. Wadsworth Publ. Co. ———. 1993. Visualizing Data. Hobart press. Cleveland, William S, and Robert McGill. 1984. “Graphical Perception: Theory, Experimentation, and Application to the Development of Graphical Methods.” Journal of the American Statistical Association 79 (387): 531–54. Cook, Dianne, Andreas Buja, Javier Cabrera, and Catherine Hurley. 1995. “Grand Tour and Projection Pursuit.” Journal of Computational and Graphical Statistics 4 (3): 155–72. Dang, Tuan Nhon, Leland Wilkinson, and Anushka Anand. 2010. “Stacking Graphic Elements to Avoid over-Plotting.” IEEE Transactions on Visualization and Computer Graphics 16 (6): 1044–52. Dao, Chau. 2020. “The Nature and Evolution of JavaScript.” Bachelor's Thesis, Oulu University of Applied Sciences. Demiralp, Çağatay, Michael S Bernstein, and Jeffrey Heer. 2014. “Learning Perceptual Kernels for Visualization Design.” IEEE Transactions on Visualization and Computer Graphics 20 (12): 1933–42. Dimara, Evanthia, and Charles Perin. 2019. “What Is Interaction for Data Visualization?” IEEE Transactions on Visualization and Computer Graphics 26 (1): 119–29. Dix, Alan, and Geoffrey Ellis. 1998. “Starting simple: adding value to static visualisation through simple interaction.” In AVI ’98: Proceedings of the working conference on Advanced visual interfaces, 124–34. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/948496.948514. Donoho, Andrew W, David L Donoho, and Miriam Gasko. 1988. “MacSpin: Dynamic Graphics on a Desktop Computer.” IEEE Computer Graphics and Applications 8 (4): 51–58. Elmqvist, Niklas, Andrew Vande Moere, Hans-Christian Jetter, Daniel Cernea, Harald Reiterer, and TJ Jankun-Kelly. 2011. “Fluid Interaction for Information Visualization.” Information Visualization 10 (4): 327–40. Evan You and the Vue Core Team. 2024. “Vue.js.” https://vuejs.org. Fienberg, Stephen E. 1992. “A Brief History of Statistics in Three and One-Half Chapters: A Review Essay.” JSTOR. Fisherkeller, Mary Anne, Jerome H Friedman, and John W Tukey. 1974. “An Interactive Multidimensional Data Display and Analysis System.” SLAC National Accelerator Lab., Menlo Park, CA (United States). Foley, James D. 1990. “Scientific Data Visualization Software: Trends and Directions.” The International Journal of Supercomputing Applications 4 (2): 154–57. ———. 1996. Computer Graphics: Principles and Practice. Vol. 12110. Addison-Wesley Professional. Fowlkes, EB. 1969. “User’s Manual for a System Fo Active Probability Plotting on Graphic-2.” Tech-Nical Memorandum, AT&amp;T Bell Labs, Murray Hill, NJ. Franconeri, Steven L, Lace M Padilla, Priti Shah, Jeffrey M Zacks, and Jessica Hullman. 2021. “The Science of Visual Data Communication: What Works.” Psychological Science in the Public Interest 22 (3): 110–61. Freedman, David. 1999. “From Association to Causation: Some Remarks on the History of Statistics.” Statistical Science 14 (3): 243–58. Friendly, Michael. 2006. “A Brief History of Data Visualization.” In Handbook of Computational Statistics: Data Visualization, edited by C. Chen, W. Härdle, and A Unwin, III???–. Heidelberg: Springer-Verlag. Friendly, Michael, and Howard Wainer. 2021. A History of Data Visualization and Graphic Communication. Harvard University Press. Gelman, Andrew, and Antony Unwin. 2013. “Infovis and Statistical Graphics: Different Goals, Different Looks.” Journal of Computational and Graphical Statistics 22 (1): 2–28. Guberman, Shelia. 2017. “Gestalt Theory Rearranged: Back to Wertheimer.” Frontiers in Psychology 8: 1782. Hand, David J. 1996. “Statistics and the Theory of Measurement.” Journal of the Royal Statistical Society: Series A (Statistics in Society) 159 (3): 445–73. Hauser, Helwig, Florian Ledermann, and Helmut Doleisch. 2002. “Angular Brushing of Extended Parallel Coordinates.” In IEEE Symposium on Information Visualization, 2002. INFOVIS 2002., 127–30. IEEE. Heer, Jeffrey, and Michael Bostock. 2010. “Crowdsourcing Graphical Perception: Using Mechanical Turk to Assess Visualization Design.” In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, 203–12. Heer, Jeffrey, Stuart K. Card, and James A. Landay. 2005. “prefuse: a toolkit for interactive information visualization.” In CHI ’05: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, 421–30. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/1054972.1055031. Heer, Jeffrey, and Ben Shneiderman. 2012. “Interactive Dynamics for Visual Analysis: A Taxonomy of Tools That Support the Fluent and Flexible Use of Visualizations.” Queue 10 (2): 30–55. Highsoft. 2022. “Render Millions of Chart Points with the Boost Module Highcharts.” Highcharts. https://www.highcharts.com/blog/tutorials/highcharts-high-performance-boost-module. ———. 2024. “Highcharts - Interactive Charting Library for Developers.” Highcharts Blog \\(\\vert\\) Highcharts. https://www.highcharts.com. Holtz, Yan. 2022a. “Barplot with Variable Width - Ggplot2.” https://r-graph-gallery.com/81-barplot-with-variable-width.html. ———. 2022b. “Basic Scatterplot in D3.js.” https://d3-graph-gallery.com/graph/scatter_basic.html. Howard, David, and Alan M MacEachren. 1995. “Constructing and Evaluating an Interactive Interface for Visualizing Reliability.” In Congresso Da Associação Cartográfica Internacional–ICA, 17:321–29. Humphry, Stephen. 2013. “Understanding Measurement in Light of Its Origins.” Frontiers in Psychology 4: 113. Jankun-Kelly, TJ, Kwan-Liu Ma, and Michael Gertz. 2007. “A Model and Framework for Visualization Exploration.” IEEE Transactions on Visualization and Computer Graphics 13 (2): 357–69. Kandel, Sean, Andreas Paepcke, Joseph M Hellerstein, and Jeffrey Heer. 2012. “Enterprise Data Analysis and Visualization: An Interview Study.” IEEE Transactions on Visualization and Computer Graphics 18 (12): 2917–26. Kehrer, Johannes, Roland N Boubela, Peter Filzmoser, and Harald Piringer. 2012. “A Generic Model for the Integration of Interactive Visualization and Statistical Computing Using r.” In 2012 IEEE Conference on Visual Analytics Science and Technology (VAST), 233–34. IEEE. Keim, Daniel A. 2002. “Information Visualization and Visual Data Mining.” IEEE Transactions on Visualization and Computer Graphics 8 (1): 1–8. Kelleher, Curran, and Haim Levkowitz. 2015. “Reactive Data Visualizations.” In Visualization and Data Analysis 2015, 9397:263–69. SPIE. Keller, Mark S, Trevor Manz, and Nils Gehlenborg. 2024. “Use-Coordination: Model, Grammar, and Library for Implementation of Coordinated Multiple Views.” In 2024 IEEE Visualization and Visual Analytics (VIS), 166–70. IEEE. Kosara, Robert. 2016. “Presentation-Oriented Visualization Techniques.” IEEE Computer Graphics and Applications 36 (1): 80–85. Krantz, David H, Patrick Suppes, Duncan R Luce, and Amos Tversky. 1971. Foundations of Measurement Volume 1: Additive and Polynomial Representations. New York: Academic Press. Kruskal, J. B. 1965. “Multidimensional Scaling.” https://community.amstat.org/jointscsg-section/media/videos. Kunst, Joshua. 2022. Highcharter: A Wrapper for the ’Highcharts’ Library. Kvasz, Ladislav. 2006. “The History of Algebra and the Development of the Form of Its Language.” Philosophia Mathematica 14 (3): 287–317. Lee, Stuart. 2021. Liminal: Multivariate Data Visualization with Tours and Embeddings. https://CRAN.R-project.org/package=liminal. Lee, Stuart, Dianne Cook, Natalia da Silva, Ursula Laa, Nicholas Spyrison, Earo Wang, and H Sherry Zhang. 2022. “The State-of-the-Art on Tours for Dynamic Visualization of High-Dimensional Data.” Wiley Interdisciplinary Reviews: Computational Statistics 14 (4): e1573. Lee, Stuart, Ursula Laa, and Dianne Cook. 2022. “Casting Multiple Shadows: Interactive Data Visualisation with Tours and Embeddings.” Journal of Data Science, Statistics, and Visualisation 2 (3). Leeuw, Jan de. 2004. “On Abandoning Xlisp-Stat.” Journal of Statistical Software 13: 1–5. Leman, Scotland C, Leanna House, Dipayan Maiti, Alex Endert, and Chris North. 2013. “Visual to Parametric Interaction (V2pi).” PloS One 8 (3): e50474. Lord, Frederic M. 1953. “On the Statistical Treatment of Football Numbers.” Luce, R Duncan. 1959. “On the Possible Psychophysical Laws.” Psychological Review 66 (2): 81. Mackinlay, Jock. 1986. “Automating the Design of Graphical Presentations of Relational Information.” Acm Transactions On Graphics (Tog) 5 (2): 110–41. Mansfield, Daniel F. 2018. Wikimedia Commons. https://commons.wikimedia.org/wiki/File:Si427o.jpg. ———. 2020. “Perpendicular Lines and Diagonal Triples in Old Babylonian Surveying.” Journal of Cuneiform Studies 72 (1): 87–99. MDN. 2024. “EventTarget - Web APIs \\(\\vert\\) MDN.” MDN Web Docs. https://developer.mozilla.org/en-US/docs/Web/API/EventTarget. ———. 2025. “WebSocket - Web APIs \\(\\vert\\) MDN.” MDN Web Docs. https://developer.mozilla.org/en-US/docs/Web/API/WebSocket. Meta. 2024. “React.” https://react.dev. Michell, Joel. 1986. “Measurement Scales and Statistics: A Clash of Paradigms.” Psychological Bulletin 100 (3): 398. ———. 2021. “Representational Measurement Theory: Is Its Number Up?” Theory &amp; Psychology 31 (1): 3–23. Online Etymology Dictionary. 2024. “Statistics.” https://www.etymonline.com/word/statistics. Parsania, Pankaj, Paresh V Virparia, et al. 2014. “A Review: Image Interpolation Techniques for Image Scaling.” International Journal of Innovative Research in Computer and Communication Engineering 2 (12): 7409–14. Pike, William A, John Stasko, Remco Chang, and Theresa A O’connell. 2009. “The Science of Interaction.” Information Visualization 8 (4): 263–74. Plotly Inc. 2022. “Part 4. Interactive Graphing and Crossfiltering \\(\\vert\\) Dash for Python Documentation \\(\\vert\\) Plotly.” https://dash.plotly.com/interactive-graphing. ———. 2024. “Webgl.” https://plotly.com/python/webgl-vs-svg. Posit (formerly RStudio Inc.). 2025. “Crosstalk.” https://rstudio.github.io/crosstalk/index.html. Quadri, Ghulam Jilani, and Paul Rosen. 2021. “A Survey of Perception-Based Visualization Studies by Task.” IEEE Transactions on Visualization and Computer Graphics. Quint, Antoine. 2003. “Scalable Vector Graphics.” IEEE MultiMedia 10 (3): 99–102. R Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/. Reda, Khairi, Pratik Nalawade, and Kate Ansah-Koi. 2018. “Graphical Perception of Continuous Quantitative Maps: The Effects of Spatial Frequency and Colormap Design.” In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems, 1–12. Rheingans, Penny. 2002. “Are We There yet? Exploring with Dynamic Visualization.” IEEE Computer Graphics and Applications 22 (1): 6–10. Rich Harris and the Svelte Core Team. 2024. “Svelte.” https://svelte.dev. Rosli, Muhammad Hafiz Wan, and Andres Cabrera. 2015. “Gestalt Principles in Multimodal Data Representation.” IEEE Computer Graphics and Applications 35 (2): 80–87. Rosling, Hans, and Zhongxing Zhang. 2011. “Health Advocacy with Gapminder Animated Statistics.” Journal of Epidemiology and Global Health 1 (1): 11–14. Ruiz, Jenny, Estefanı́a Serral, and Monique Snoeck. 2021. “Unifying Functional User Interface Design Principles.” International Journal of Human–Computer Interaction 37 (1): 47–67. Saket, Bahador, Arjun Srinivasan, Eric D Ragan, and Alex Endert. 2017. “Evaluating Interactive Graphical Encodings for Data Visualization.” IEEE Transactions on Visualization and Computer Graphics 24 (3): 1316–30. Sarikaya, Alper, Michael Correll, Lyn Bartram, Melanie Tory, and Danyel Fisher. 2018. “What Do We Talk about When We Talk about Dashboards?” IEEE Transactions on Visualization and Computer Graphics 25 (1): 682–92. Satyanarayan, Arvind, Dominik Moritz, Kanit Wongsuphasawat, and Jeffrey Heer. 2016. “Vega-Lite: A Grammar of Interactive Graphics.” IEEE Transactions on Visualization and Computer Graphics 23 (1): 341–50. Satyanarayan, Arvind, Ryan Russell, Jane Hoffswell, and Jeffrey Heer. 2015. “Reactive Vega: A Streaming Dataflow Architecture for Declarative Interactive Visualization.” IEEE Transactions on Visualization and Computer Graphics 22 (1): 659–68. Self, Jessica Zeitz, Michelle Dowling, John Wenskovitch, Ian Crandell, Ming Wang, Leanna House, Scotland Leman, and Chris North. 2018. “Observation-Level and Parametric Interaction for High-Dimensional Data Analysis.” ACM Transactions on Interactive Intelligent Systems (TiiS) 8 (2): 1–36. Shirley, Peter, Michael Ashikhmin, and Steve Marschner. 2009. Fundamentals of Computer Graphics. AK Peters/CRC Press. Shneiderman, Ben. 2003. “The Eyes Have It: A Task by Data Type Taxonomy for Information Visualizations.” In The Craft of Information Visualization, 364–71. Elsevier. Sievert, Carson. 2020. Interactive Web-Based Data Visualization with r, Plotly, and Shiny. Chapman; Hall/CRC. Splechtna, Rainer, Michael Beham, Denis Gračanin, María Luján Ganuza, Katja Bühler, Igor Sunday Pandžić, and Krešimir Matković. 2018. “Cross-Table Linking and Brushing: Interactive Visual Analysis of Multiple Tabular Data Sets.” The Visual Computer 34 (6): 1087–98. Spyrison, Nicholas, and Dianne Cook. 2020. “Spinifex: An r Package for Creating a Manual Tour of Low-Dimensional Projections of Multivariate Data.” The R Journal 12 (1): 243–57. Stevens, Stanley Smith. 1946. “On the Theory of Scales of Measurement.” Science 103 (2684): 677–80. ———. 1951. “Mathematics, Measurement, and Psychophysics.” Swayne, Deborah F., Dianne Cook, and Andreas Buja. 1998. “XGobi: Interactive Dynamic Data Visualization in the X Window System.” J. Comput. Graph. Stat. 7 (1): 113–30. https://doi.org/10.1080/10618600.1998.10474764. Swayne, Deborah F., Duncan Temple Lang, Andreas Buja, and Dianne Cook. 2003. “GGobi: evolving from XGobi into an extensible framework for interactive data visualization.” Comput. Statist. Data Anal. 43 (4): 423–44. https://doi.org/10.1016/S0167-9473(02)00286-4. Tal, Eran. 2025. “Models and Measurement.” The Routledge Handbook of Philosophy of Scientific Modeling, 256–69. The New York Times Company. 2025. “Graphics.” The New York Times. https://www.nytimes.com/spotlight/graphics. Theus, Martin. 2002. “Interactive Data Visualization using Mondrian.” J. Stat. Soft. 7 (November): 1–9. https://doi.org/10.18637/jss.v007.i11. ———. 2008. “High-Dimensional Data Visualization.” In Handbook of Data Visualization, 152–75. Springer Science &amp; Business Media. Tierney, Luke. 1990. Lisp-Stat: An Object-Oriented Environment for Statistical Computing and Dynamic Graphics. New York: Wiley-Interscience. Tufte, Edward R. 2001. The Visual Display of Quantitative Information. Cheshire, Connecticut: Graphics Press LLC. Tukey, John W. 1962. “The Future of Data Analysis.” The Annals of Mathematical Statistics 33 (1): 1–67. Tukey, John W et al. 1977. Exploratory Data Analysis. Vol. 2. Reading, MA. Tukey, John W. 1986. “Data Analysis and Behavioral Science or Learning to Bear the Quantitative Man’s Burden by Shunning Badmandments.” The Collected Works of John W. Tukey 3: 391–484. ———. 1993. “Graphic Comparisons of Several Linked Aspects: Alternatives and Suggested Principles.” Journal of Computational and Graphical Statistics 2 (1): 1–33. Unwin, Antony. 1999. “Requirements for interactive graphics software for exploratory data analysis.” Comput. Statist. 14 (1): 7–22. https://doi.org/10.1007/PL00022706. ———. 2000. “Visualisation for Data Mining.” In International Conference on Data Mining, Visualization and Statistical System, séoul, Korea. Citeseer. ———. 2018. Graphical Data Analysis with r. Chapman; Hall/CRC. Unwin, Antony, George Hawkins, Heike Hofmann, and Bernd Siegl. 1996. “Interactive Graphics for Data Sets with Missing Values—MANET.” Journal of Computational and Graphical Statistics 5 (2): 113–22. Unwin, Antony, Martin Theus, Heike Hofmann, and Antony Unwin. 2006. “Interacting with Graphics.” Graphics of Large Datasets: Visualizing a Million, 73–101. Urbanek, Simon. 2002. “Different Ways to See a Tree-KLIMT.” In Compstat: Proceedings in Computational Statistics, 303–8. Springer. ———. 2011. “iPlots eXtreme: Next-Generation Interactive Graphics Design and Implementation of Modern Interactive Graphics.” Computational Statistics 26 (3): 381–93. Urbanek, Simon, and Martin Theus. 2003. “iPlots: High Interaction Graphics for r.” In Proceedings of the 3rd International Workshop on Distributed Statistical Computing. Citeseer. Urbanek, Simon, and Antony R Unwin. 2001. “Making Trees Interactive-KLIMT.” In Proc. Of the 33th Symposium of the Interface of Computing Science and Statistics. Citeseer. Vaidyanathan, Ramnath, Yihui Xie, JJ Allaire, Joe Cheng, Carson Sievert, and Kenton Russell. 2021. Htmlwidgets: HTML Widgets for r. https://CRAN.R-project.org/package=htmlwidgets. Vanderplas, Susan, Dianne Cook, and Heike Hofmann. 2020. “Testing Statistical Charts: What Makes a Good Graph?” Annual Review of Statistics and Its Application 7: 61–88. Vega Project. 2022. “Example Gallery: Interactive.” https://vega.github.io/vega-lite/examples/#interactive. ———. 2024a. “Binding a Parameter.” https://vega.github.io/vega-lite/docs/bind.html. ———. 2024b. “Brushing Scatter Plots Example.” Vega. https://vega.github.io/vega/examples/brushing-scatter-plots. ———. 2024c. “Dynamic Behaviors with Parameters.” https://vega.github.io/vega-lite/docs/parameter.html. ———. 2024d. “Vega and D3.” Vega. https://vega.github.io/vega/about/vega-and-d3. Velleman, Paul F, and Pratt Paul. 1989. “A Graphical Interface for Data Analysis.” Journal of Statistical Computation and Simulation 32 (4): 223–28. Velleman, Paul F, and Leland Wilkinson. 1993. “Nominal, Ordinal, Interval, and Ratio Typologies Are Misleading.” The American Statistician 47 (1): 65–72. Waddell, Adrian, and R. Wayne Oldford. 2023. Loon: Interactive Statistical Data Visualization. https://CRAN.R-project.org/package=loon. Ward, Matthew O, Georges Grinstein, and Daniel Keim. 2015. Interactive Data Visualization: Foundations, Techniques, and Applications. CRC Press. Ware, Colin. 2019. Information Visualization: Perception for Design. Morgan Kaufmann. Wickham, Hadley. 2011. “The Split-Apply-Combine Strategy for Data Analysis.” Journal of Statistical Software 40: 1–29. ———. 2016. Ggplot2: Elegant Graphics for Data Analysis (2e). Springer-Verlag New York. https://ggplot2.tidyverse.org. ———. 2021. Mastering Shiny. \" O’Reilly Media, Inc.\". Wickham, Hadley, Michael Lawrence, Dianne Cook, Andreas Buja, Heike Hofmann, and Deborah F Swayne. 2009. “The Plumbing of Interactive Graphics.” Computational Statistics 24: 207–15. Wickham, Hadley, and Danielle Navarro. 2024. Ggplot2: Elegant Graphics for Data Analysis (3e). https://ggplot2-book.org. Wikipedia. 2022. “Duck test - Wikipedia.” https://en.wikipedia.org/w/index.php?title=Duck_test&amp;oldid=1110781513. Wilhelm, Adalbert. 2003. “User interaction at various levels of data displays.” Comput. Statist. Data Anal. 43 (4): 471–94. https://doi.org/10.1016/S0167-9473(02)00288-8. ———. 2008. “Linked Views for Visual Exploration.” In Handbook of Data Visualization, 200–214. Springer Science &amp; Business Media. Wilke, Claus O. 2019. Fundamentals of Data Visualization: A Primer on Making Informative and Compelling Figures. O’Reilly Media. Wilkinson, Leland. 2012. The Grammar of Graphics. Springer. Wills, Graham. 2000. “Natural Selection: Interactive Subset Creation.” Journal of Computational and Graphical Statistics 9 (3): 544–57. ———. 2008. “Linked Data Views.” In Handbook of Data Visualization, 217–41. ch. II. 9. Springer Berlin/Heidelberg, Germany. Wirfs-Brock, Allen, and Brendan Eich. 2020. “JavaScript: the first 20 years.” Proc. ACM Program. Lang. 4 (HOPL): 1–189. https://doi.org/10.1145/3386327. Xie, Yihui, Heike Hofmann, and Xiaoyue Cheng. 2014. “Reactive Programming for Interactive Graphics.” Statistical Science, 201–13. Yi, Ji Soo, Youn ah Kang, John Stasko, and Julie A Jacko. 2007. “Toward a Deeper Understanding of the Role of Interaction in Information Visualization.” IEEE Transactions on Visualization and Computer Graphics 13 (6): 1224–31. Young, Forrest W, Pedro M Valero-Mora, and Michael Friendly. 2011. Visual Statistics: Seeing Data with Dynamic Interactive Graphics. John Wiley &amp; Sons. Technically, if there are any values \\(x_i &lt; a\\), we will have negative indices (\\(j &lt; 0\\)), and if all values are significantly larger than the anchor, such that \\(x_i &gt; a + w\\), the indices will not start at 1. So, to implement the histogram properly, we should shift all indices by subtracting the minimum index. Finally, if the histogram binwidth is not fixed, \\(h\\) becomes more complex as well.↩︎ "]]

[["problem-set.html", "3 Problem Set 3.1 Data representation 3.2 Data transformation 3.3 Scaling", " 3 Problem Set Designing an interactive data visualization system presents a unique set of challenges. Some of these have been already touched on in the Introduction. This section discusses these inherent challenges in greater depth, and begins exploring avenues for possible solutions. 3.1 Data representation Data visualization is, first and foremost, about data (it’s in the name). However, all data is not created equal. Information can come to us in various shapes and sizes, and the way the data is structured can have a significant impact on various aspects of the visualization system, including ease of use, maintainability, and performance. 3.1.0.1 Row-based vs. column-based A common model in many data analytic languages is that of two-dimensional table or data frame. Here, the data is organized in a dictionary of columns, with each column being a homogeneous array containing values of the same type. However, unlike in a matrix data structure, different columns can store values of different types (such as floats, integers, or strings). The dataframe object can also store optional metadata, such as row names, column labels, or grouping structure (R Core Team 2024; Bouchet-Valat and Kamiński 2023). Popular examples of this design include the S3 data.frame class in base R (R Core Team 2024), the tbl_df S3 class in the tibble package (Müller and Wickham 2023), the DataFrame class in the Python pandas package (Pandas Core Team 2024), the DataFrame class in the polars library (Team 2024), or the DataFrame type in the Julia DataFrame.jl package (Bouchet-Valat and Kamiński 2023). However, the column-based organization of data is not universal. For example, the popular JavaScript data visualization and transformation library D3 (Bostock 2022) models data frames as arrays of rows, with each row being its own separate dictionary. Likewise, certain types of databases store tables as lists of records, with each record having the shape of a dictionary (Abadi et al. 2013). Within a broader programming context, these two fundamental data layouts are referred to as the struct of arrays (SoA, also known as “parallel arrays”) versus the array of structs (AoS) data structures. SoA store data in a dictionary of arrays, similar to the column-based layout, whereas AoS store data in an arrays of dictionaries, similar to row-based layout. The distinction between SoA and AoS is a bit more nuanced, since structs can store a wider class of types than just plain data, such as functions and pointers, and this makes either layout better suited to certain programming styles. For example, in object oriented programming, behaviour is encapsulated alongside data in objects (via methods/member functions), and this makes the AoS the more natural data structure within this programming paradigm (replicating the same functionality with SoA is awkward, although some modern languages offer features which make this more convenient, see e.g. Zig Software Foundation 2024). 3.1.0.2 Performance The two data layouts also offer distinct performance characteristics. The column-based (SoA) layout is generally considered to be the one better for performance (see e.g. Acton 2014; Kelley 2023). Specifically, it benefits from two important features: better memory alignment and improved cache locality. First, homogeneous arrays offer better memory characteristics than heterogeneous structs. This is because they can be stored as contiguous blocks of memory with the same alignment, eliminating the need for padding and potentially leading to a significant reduction in memory footprint (see e.g. Rentzsch 2005; Kelley 2023). Second, the column-based data layout is better suited for pre-fetching. Specifically, when performing column-wise operations, the CPU can cache the contiguously-stored values more easily, often resulting in greatly improved performance (Abadi et al. 2013; Acton 2014; Kelley 2023). However, the row-based (AoS) layout can also perform well in certain situations. Specifically, it can outperform column-based stores when retrieving individual records/rows is key, hence why it is commonly used in traditional Online Transaction Processing databases (OLTP, Abadi et al. 2013). Additionally, it could be argued that the row-based layout can be more “natural” and offer better developer ergonomics for certain programming styles. 3.2 Data transformation As was already hinted at in the previous section, when visualizing data, we rarely plot the raw data directly. Instead, the quantities we visualize are often summaries or aggregates of some kind. Take for example a typical barplot. To draw the barplot, we first need to divide the data into parts, summarize each part by some metric, usually either count or sum of some variable, and finally, render the summaries as individual bars. Likewise, many other types of plots like boxplots or histograms include two distinct steps: splitting the data into parts and computing the summaries on these parts. This section discusses these two fundamental parts of the data visualization pipeline - partitioning and aggregation - and explores challenges associated with their implementation in an interactive data visualization system. While these operations might seem straightforward, my goal is to make the case that they actually come equipped with a lot of subtle structure, more than might necessarily meet the eye. 3.2.1 Partitioning the data 3.2.1.1 Leave no data out A common-sense guideline that many data visualization experts provide is that faithful visualizations should show the full data and leave nothing out. For instance, Cleveland (1985) argues that axis limits should generally be expanded so that data points at or near these limits are not arbitrarily obscured. Take, for example, the following two scatterplots: Figure 3.1: Without expanding axes, objects near the limits can become occluded. Left: axis limits match the data limits exactly, and so points at or near the axis limits (top-left and bottom-right corner of the plot) are “cut-off” and represented by smaller area, becoming less salient. Right: by expanding axis limits, we can ensure that all data is represented faithfully. In the left plot, the axis limits match the data limits exactly, whereas in the right plot, they are expanded by a small fraction (5%, ggplot2 default, Wickham 2016). The problem with the left plot is that the data points near the axis limits (top-left and bottom-right corner) are represented only by a fraction of the area: for example, the point in the bottom-right corner lies simultaneously at the limits of the x- and y-axis, and is thus represented only by 1/4 of the area of the points in the center of the plot. The example above shows how data can be obscured visually, even after all of the data points have been included in the plot (rendering all rows of the data set). Clearly then, when complete data is available, leaving information out by arbitrarily dropping rows is even less acceptable. This issue becomes more complicated in the presence of missing or incomplete data, however, there exist ways of dealing with that as well, see e.g. Unwin et al. (1996), Tierney and Cook (2023). Thus, ideally, the visualization should represent a surjective mapping from the space of the geometric objects to the underlying data set, such that, by default, there are no cases (rows of the data) which are marginalized or left out of the figure entirely (Ziemkiewicz and Kosara 2009). 3.2.1.2 Distinctness, disjointness, and comparison “To be truthful and revealing, data graphics must bear on the question at the heart of quantitative thinking: ‘compared to what’?” (Tufte 2001, 74). “Graphics are for comparison - comparison of one kind or another - not for access to individual amounts.” (Tukey 1993) In data visualization, a practice so ubiquitous that it is often overlooked is that, in most types of plots, each geometric object represents one disjoint part of the data. That is, each point, bar, line, or polygon typically represents a unique set of cases (rows of the data), with no overlap with the cases represented by the other objects. Why is this the case? This unconscious “law” might be rooted in the fundamental purpose of data visualization: comparison (Tufte 2001; Tukey 1993). When we visualize, we draw our graphics with the ultimate goal of being able to compare our data along a set of visual channels, such as position, length, size, or colour (Bertin 1983; Wilkinson 2012; Franconeri et al. 2021; Wilke 2019). This mirrors the comparisons we make in the real world, where we compare physical objects along their respective dimensions or attributes. With disjoint parts, there is a natural correspondence or bijection between the subsets of the data and their visual representation, see figure 3.2. Specifically, if we imagine the act of taking an object and picking the corresponding set of cases as a function, then, with disjoint parts, this function is invertible: we can pick an object, identify the corresponding set of cases, and then use that set to get back the original object. In plots where the objects do not represent disjoint subsets of the data, this correspondence is broken: if we identify cases corresponding to a single object, there is no way to go back from these cases to the original object. Figure 3.2: Disjoint subsets provide a one-to-one correspondence (bijection) between geometric objects and the data. Suppose we mark out the cases corresponding to one object (the left most bar). Top row: if each geometric object (bar) represents unique set of cases, we can easily go back and forth between the object and its underlying data (middle panel). Thus, the function of picking a set of cases corresponding to an object is bijective. Bottom row: when there is an overlap between the cases represented by each object, once we pick the set of cases corresponding to that object, there is no simple way to use that set to get the corresponding object back. To illustrate this idea on concrete data, take the following barplot representing vote share among the top three parties in the 2023 New Zealand general election (Electoral Commission New Zealand 2023): Figure 3.3: Geometric objects typically represent disjoint subsets of the data. The plot shows the vote share of the top three parties in the 2023 New Zealand general election, with each bar representing a unique subset of voters. Each bar represents a unique set of voters and thus the subsets of the data represented by the bars are disjoint. Technically, there is no hard and fast rule about this. For example, we could transform the first bar by taking a union of the votes of National and Labour parties, and represent the same underlying data the following way: Figure 3.4: Hypothetically, there is nothing preventing us from encoding the same information in multiple objects, and showing non-disjoint parts of our data. The plot shows the same underlying data as 3.2, with the leftmost bar representing a union of National and Labour voters. The two leftmost bars thus do not represent disjoint subsets of the data. For a more realistic example, see Figure 3.6. However, this way of representing the data has several problems. First, there is the issue of its suitability towards the main goal of the visualization. Specifically, when visualizing election data such as this one, we are typically interested in judging the relative number of votes each party received. The second barplot makes this comparison difficult. Specifically, in the second barplot, since the National bar represents a subset of the National OR Labour bar, we have to perform additional mental calculation if we want to find out how many votes Labour received and compare the absolute counts directly (Cleveland 1985). Second, we have metadata knowledge (see e.g. Wilkinson 2012; Velleman and Wilkinson 1993) about the data being disjoint - we know that, in the New Zealand parliament electoral system, each voter can only cast one vote for a single party. Finally, there is the issue of duplicating information: in the second barplot, the number of votes the National party received is counted twice, once in the leftmost bar and again in the second-left bar. This goes against the general principle of representing our data in the most parsimonious way (Tufte 2001). Even when our goal is not to compare absolute counts, there are usually better disjoint data visualization techniques available. For example, if we were interested in visualizing the proportion of votes that each party received, we could draw the following plot: Figure 3.5: Even when proportions are of interest, there are usually disjoint data visualization techniques available. The plot shows proportion of vote share of the top three parties in the 2023 New Zealand general election, with each bar segment again representing a unique subset of voters. By stacking the bar segments on top of each other, we can easily compare proportion of the total number of votes while retaining a parsimonious representation of our data. Each bar segments now again represents a unique subset of voters. The example above was fairly clear case of where a non-disjoint representation of the data would be the wrong choice, however, there are also more ambiguous situations. One such situation is when there are multiple attributes of the data which can be simultaneously present or absent for each case. For example, in 2020, a joint referendum was held in New Zealand on the question of legalization of euthanasia and cannabis. The two issues were simultaneously included on the same ballot. The legalization of euthanasia was accepted by the voters, with 65.1% of votes supporting the decision, whereas the legalization of cannabis was rejected, with 50.7% of voters rejecting the decision (Electoral Commission New Zealand 2020). The referendum data can be visualized in the following way: Figure 3.6: Realistic example of a non-disjoint data representation. The plot shows the vote share in the combined 2020 New Zealand referendum on euthanasia and cannabis, where the two issues were simultaneousy presented on the same ballot. The two bars each show (mostly) the same subset of ballot, with each ballot contributing to the height of one segment in each bar. By definition, both bars include votes which were cast by the same voter (ignoring the votes where no preference was given for either issue, Electoral Commission New Zealand 2020). Thus, the sets of voters that the two bars and the four bar segments represent overlap. In general, there is nothing inherently wrong with the plot above. Non-disjoint representations of the data can work well for certain data types such as set-typed data (see e.g. Alsallakh et al. 2014). In the context of static data visualization, plots like these can serve useful roles. However, even here, there is a simple way of representing the same data in a disjoint way - draw two separate plots: Figure 3.7: Non-disjoint data representations can often be turned into disjoint ones. The figure again shows the vote share in the combined 2020 New Zealand referendum on euthanasia and cannabis, however, now each issue is plotted in a separate plot, and thus each geometric object in each plot represents a unique subset of ballots/voters. Why should we care about whether our representations of the data are disjoint or not? I argue that, for many types of plots, disjointness presents a fundamentally better mental model: each geometric object encodes one unique set of observational units. Conversely, if the objects in our plots do not represent disjoint subsets, then we need to keep an additional model of how they are related in our head. This issue is particularly important in interactive visualization. The natural correspondence between geometric objects and disjoint subsets of the data makes certain interactions more intuitive, and conversely, overlapping subsets induce surprising behavior. For instance, when a user clicks on a bar in a linked barplot, they might be surprised if parts of the other bars within the same plot get highlighted as well: they intended to highlight that bar, not the others. Likewise, when querying, if our objects do not represent disjoint subsets of the data, we have to think about what querying means: are we querying the objects or the cases corresponding to the objects? Disjoint partitions simplify our mental model, and this may be the reason why some authors discuss interactive features in the context of partitions (see e.g. Buja, Cook, and Swayne 1996; Keim 2002). SQL aggregation queries (GROUP BY) are based on partitions (Hellerstein et al. 1999). 3.2.1.3 Plots as partitions In the two preceding sections, I have argued the plots in our interactive data visualization system should have have two fundamental properties: Plots should show the full data (surjective mapping) Geometric objects within these plots should represent distinct subsets (disjoint parts) These two properties suggest a fundamental model for plots: that of partitions. Specifically, I propose the following definition of a regular plot: Definition 3.1 (Regular plot) Regular plot is a plot where the geometric objects within one layer represent a partition of the data, such that there is a natural bijection between these objects and (possibly aggregated) parts of the original data. While, I have not been able to find references explicitly conceptualizing plots as partitions, some data visualization researchers have used the language of bijections when discussing graphics. For example, Dastani (2002) discusses plots as bijections (homomorphisms) between data tables and visual attribute tables, however, this is already in the context of aggregated data (e.g. rows of the data table for barplot already being sums/counts). Similarly, Ziemkiewicz and Kosara (2009) and Vickers, Faith, and Rossiter (2012) argued that, in order to be visually unambiguous, plots should represent bijections of the underlying data. However, under this strict view, only one-to-one representations of the data such as scatterplots and parallel coordinate plots would be permitted (and the authors do admit that aggregation can at times present an acceptable trade-off, despite the information loss, Ziemkiewicz and Kosara 2009). The approach I take is slightly different. Instead of modeling plots as bijections between cases and the geometric objects, I model them as bijections between parts of data and the geometric objects. In other words, the bijection is not between rows of the original data table and the geometric objects, but between subtables of the original data table and the geometric objects. This approach has the advantage that aggregation can be considered as part of the bijection. Finally, Wilkinson (2012, pp 210) and Keim (2002) have linked stacked plots to (hierarchical) partitions. 3.2.1.4 Partitions and products In a typical interactive plot, the data will be partitioned across multiple dimensions. To give a concrete example, suppose we want to draw the following barplot: We start with the following data, which includes a categorical variable (group) that we will plot along the x-axis, a variable representing selection status (selection) that we will color the bar segments with, and a continuous variable that we want to summarize (value): group selection value 1 A 1 12 2 A 1 21 3 A 2 10 4 B 1 9 5 B 2 15 6 C 1 15 7 C 2 12 8 C 2 13 To draw the individual bar segments, we need to sum the value variable across the cases corresponding to each segment. To do this, we first need to split our data into multiple small disjoint subsets according to the product of group and selection variables: # Using paste0() here to simulate a product of two factors product_factor &lt;- paste0(df$group, df$selection) split_dfs &lt;- split(df, product_factor) render_tables(split_dfs) group selection value A 1 12 A 1 21 group selection value 3 A 2 10 group selection value 4 B 1 9 group selection value 5 B 2 15 group selection value 6 C 1 15 group selection value 7 C 2 12 8 C 2 13 We could then summarize each small data set by summing value: summarized_dfs &lt;- lapply(split_dfs, function(x) { aggregate(value ~ ., data = x, sum) }) render_tables(summarized_dfs) group selection value A 1 33 group selection value A 2 10 group selection value B 1 9 group selection value B 2 15 group selection value C 1 15 group selection value C 2 25 Finally, to “stack” the segments on top of each other, we need to combine the summaries back together, according to the levels of group variable, and take the cumulative sum: grouped_dfs &lt;- split(summarized_dfs, sapply(summarized_dfs, function(x) x$group)) stacked_dfs &lt;- lapply(grouped_dfs, function(x) { x &lt;- do.call(rbind, x) x$value &lt;- cumsum(x$value) rownames(x) &lt;- NULL x }) render_tables(stacked_dfs) group selection value A 1 33 A 2 43 group selection value B 1 9 B 2 24 group selection value C 1 15 C 2 40 Now, we can combine the tables into one data set and render: combined_df &lt;- do.call(rbind, stacked_dfs) combined_df$selection &lt;- factor(combined_df$selection, levels = c(2, 1)) # Need to reverse data order for ggplot2 to layer segments appropriately combined_df &lt;- combined_df[6:1, ] ggplot(combined_df, aes(x = group, y = value, fill = selection)) + geom_col(position = position_identity(), col = &quot;white&quot;) Now, we have shown how we can compute summary statistics for a stacked barplot using a simple split-apply-combine pipeline (Wickham 2011). This is in fact what happens implicitly in most ggplot2 plots: ggplot(data, aes(x, y, fill = fill)) + geom_bar() In the call above, we partition the data set by the Cartesian product of the x, y, and fill variables. That is, we break the data into parts based on the unique combinations of these variables, and then compute whatever statistical transformation we need. See the following comment from the ggplot2 documentation (Wickham 2016): # If the `group` variable is not present, then a new group # variable is generated from the interaction of all discrete (factor or # character) vectors, excluding `label`. 3.2.1.5 Limits of simple product partitions For many types of plots, the simple strategy of taking products of factors to form a single “flat” partition of the data works reasonably well. However, for other types of plots, this flat model is not enough. To give a concrete example, let’s turn back to the barplot from the section 3.2.1.3. To draw the barplot, we first split our data into smaller tables, summarized each table by summing up the values, stacked the summaries by taking their cumulative sum, and finally used these to draw the bar segments. This gave us a good visualization for comparing absolute counts across the categories. However, what if we wanted to compare proportions? It turns out there is another type of visualization, called spineplot, which can be used to represent the same underlying as a barplot, however, is much more useful for comparing proportions: Figure 3.8: The same underlying data represented as a barplot (left) and a spineplot (right). A spineplot represents the same underlying statistic as a barplot (usually sums of counts). However, unlike in barplot, where the underlying statistic gets mapped to the y-axis position/bar height, in spineplot, the underlying summary statistic gets mapped to two aesthetics: the y-axis position and the bar width. Further, the y-axis position gets normalized, such that the heights of the different segments add up to one. The result is a visualization that makes it easy to compare relative proportions across the categories. The fact that the spineplot makes it easy to see relative proportions makes it a very useful visualization. Notice how, in Figure 3.8, the spineplot makes it much easier to see that the proportions of the red cases are same across the B and C groups. Thus, spineplot is a definitely desirable type of representation for categorical data, especially if we can use interactiveity easily switch between it and the barplot. However, despite the fact that barplot and spineplot are closely related visualizations, turning one into the other is not always a trivial exercise. Specifically, many declarative data visualization systems lack simple syntax for creating spineplots. For example, in ggplot2, there is currently no simple declarative way to define a spineplot. To create the right panel in Figure 3.8, the data had to first be wrangled into the right shape outside of the ggplot2 call, and the entire process took over 10 lines of code (using standard dplyr syntax). Why are spineplots tricky? The reason is that they force us to confront the hierarchical nature of graphics. Specifically, in a spineplot, the x- and y-axes represent the same data, however, this data is summarized and transformed along different levels of aggregation: Along the x-axis, we stack the summaries across the levels of a single factor Along the y-axis, we stack the summaries across the levels of a product of two factors and normalize them by the values within the levels of the parent factor. So, in a spineplot, it is not enough to simply break our data into \\(j \\cdot k\\) tables; instead, we need to break it into \\(j\\) tables on one level, \\(j \\cdot k\\) tables on another level, and preserve this hierarchical relationship in some way, such that 3.2.1.6 Partitions and hierarchy Keim (2002) stacked plots are suited for hierarchically-partitioned data. 3.2.1.7 Partition data structures 3.2.2 Computing summaries After we have partitioned our data, we need a way to summarize each part by a set of one or more summary statistics. 3.2.2.1 Reducers 3.3 Scaling Suppose we have partitioned our data and computed the relevant summary statistics. Now we need a way to to encode these summaries into visual attributes that we can then present on the computer screen. In most data visualization systems, this is done by specialized components called scales or coordinate systems (see e.g. Murrell 2005; Wickham 2016; Wilkinson 2012). 3.3.1 Theory of scales Within the data visualization literature, there exists a fairly large body of research on the theoretical properties of scales. A full treatment is beyond the scope of the present thesis. However, in this section, I will attempt to briefly summarize some important findings and concepts, and . 3.3.1.1 Philosophy, psychology, and measurement One challenge when discussing scales in data visualization is that the topic unavoidably intersects with a research area that has a particularly long and contentious history: theory of measurement (see e.g. Hand 1996; Michell 1986; Tal 2025). Theory of measurement (not to be confused with measure theory, with which it nevertheless shares some overlap) is the research area which tries to answer the deceptively simple question: what does it mean to measure something? This seemingly trivial problem has inspired long and fiery debates within the fields of mathematics, philosophy, and social science. Particularly, in psychology, where assigning numerical values non-physical phenomena such as moods and mental states is a central concern, the topic has garnered a significant amount of attention, creating a dense body of research (see e.g. Humphry 2013; Michell 2021). Arguably, the most influential work in this field has been that of Stevens (1946). In this fairly concise paper, Stevens defined a scale as method of assigning numbers to values, and introduced a four-fold classification classification, namely: nominal, ordinal, interval, and ratio scales (see Table 3.1). Table 3.1: Types of scales identified by Stevens (1946) Scale Structure Comparison Valid transformations Nominal Isomorphism Are \\(x\\) and \\(y\\) the same? \\(x&#39; = f(x)\\), where \\(f\\) is a bijection Ordinal Monotone map Is \\(x\\) is greater than \\(y\\)? \\(x&#39; = f(x)\\), where \\(f\\) is a monotone bijection Interval Affine transformation How far is \\(x\\) from \\(y\\)? \\(x&#39; = ax + b\\), for \\(a, b \\in \\mathbb{R}\\) Ratio Linear map How many times is \\(x\\) greater than \\(y\\)? \\(x&#39; = ax\\), for \\(a \\in \\mathbb{R}\\) The Steven’s (1946) typology is based on invariance under transformation. Specifically, for each class of scales, we define a set of transformations that preserve valid comparisons. The set of valid transformations shrinks as we move from one class of scales to another. For nominal scales, any kind of bijective transformation is valid. Intuitively, we can think of the scale as assigning labels to values, and any kind re-labeling is valid, as long as it preserves equality of the underlying values. For instance, given a nominal scale with three values, we can assign the labels \\(\\{ \\text{red}, \\text{green}, \\text{blue} \\}\\) or \\(\\{ \\text{monday}, \\text{tuesday}, \\text{wednesday} \\}\\) in any way we like, as long as each value maps to a unique label. This identifies the underlying mathematical structure as an isomorphism. Ordinal scales are more restrictive, since, on top of preserving equality, transformations also need to preserve order. For example, if we want to assign the labels \\(\\{ \\text{monday}, \\text{tuesday}, \\text{wednesday} \\}\\) to an ordinal scale with three values, there is only one way to do it that preserves the underlying order: assign the least values to \\(\\text{monday}\\), the middle value to \\(\\text{tuesday}\\), and the greatest value to \\(\\text{wednesday}\\) (assuming we order the labels/days in the usual day-of-week order). However, there is no notion of distance between the labels: we could just as well assign the values labels in \\(\\mathbb{N}\\) such as \\(\\{ 10, 20, 30 \\}\\), \\(\\{1, 2, 9999 \\}\\), and so on. Thus, the fundamental mathematical structure is that of a monotone map. Interval scales need to additionally preserve equality of intervals. This means that, for any three values \\(a, b,\\) and \\(c\\), if the distances between \\(a\\) and \\(b\\) and \\(b\\) and \\(c\\) are equal, \\(d(a, b) = d(b, c)\\), then so should be the distances between the scaled labels, \\(d^*(f(a), f(b)) = d^*(f(b), f(c)\\). For most real applications, this limits interval scales to the class affine transformations of the form \\(f(x) = ax + b\\). A canonical example of an interval scale is the conversion formula of degrees Celsius to Fahrenheit: \\(f(c) = 9/5 \\cdot c + 32\\) (Stevens 1946). Finally, ratio scales also need to preserve the equality of ratios. Specifically, if \\(a/b = b/c\\) then \\(f(a)/f(b) = f(b) / f(c)\\). As a consequence, this also means that the scale must have a well-defined zero-point. Steven’s (1946) typology inspired a considerable debate that I will touch on here only briefly. First, despite some monumental efforts towards unification, such as that by Krantz et al. (1971), measurement has to this day remained a hotly debated topic, with many diverging philosophical views and theories (see e.g. Michell 2021; Tal 2025). Second, more relevant to statistics, some authors used the theory to define “permissible” classes of statistical summaries of data. For example, some have suggested that the practice of taking the mean of values defined on an ordinal scale is not permissible, since the meaning of the average operator is not preserved under monotone transformations (Stevens 1951; Luce 1959). However, this issue was hotly contested by others such as Lord (1953), Tukey (1986), and Velleman and Wilkinson (1993), who argued that there are many well-established statistical practices which rely on “impermissible” statistics, such as rank-based tests, and that, more broadly, data derives its meaning from the statistical questions it is being used to answer. Ultimately, the discussion around measurement may have at this point become far too dense and theoretical for applied disciplines such as data visualization. Nevertheless, it does periodically crop up in some applied issues, such as the perennial problem of whether the base of a barplot should always start at zero (see e.g. Cleveland 1985; Wilkinson 2012). Thus, I thought it prudent to mention it here. 3.3.1.2 Visual perception Another important area in which there has been considerable amount of research is how scales apply to visual perception. Specifically, given that we use visual attributes such as position, color, length, or area to encode attributes of our data, an important question is 3.3.2 Applied scales 3.3.2.1 Scale transformation “Transformation is a critical tool for visualization or for any other mode of data analysis because it can substantially simplify the structure of a set of data.” Cleveland (1993), pp. 48 References Abadi, Daniel, Peter Boncz, Stavros Harizopoulos, Stratos Idreos, Samuel Madden, et al. 2013. “The Design and Implementation of Modern Column-Oriented Database Systems.” Foundations and Trends in Databases 5 (3): 197–280. Acton, Mike. 2014. “Data-Oriented Design and c++.” Luento. CppCon. https://www.youtube.com/watch?v=rX0ItVEVjHc. Alsallakh, Bilal, Luana Micallef, Wolfgang Aigner, Helwig Hauser, Silvia Miksch, and Peter Rodgers. 2014. “Visualizing Sets and Set-Typed Data: State-of-the-Art and Future Challenges.” Eurographics Conference on Visualization (EuroVis). Bertin, Jacques. 1983. Semiology of Graphics. University of Wisconsin press. Bostock, Mike. 2022. “D3.js - Data-Driven Documents.” https://d3js.org. Bouchet-Valat, Milan, and Bogumił Kamiński. 2023. “DataFrames.jl: Flexible and Fast Tabular Data in Julia.” Journal of Statistical Software 107 (September): 1–32. https://doi.org/10.18637/jss.v107.i04. Buja, Andreas, Dianne Cook, and Deborah F Swayne. 1996. “Interactive High-Dimensional Data Visualization.” Journal of Computational and Graphical Statistics 5 (1): 78–99. Cleveland, William S. 1985. The Elements of Graphing Data. Wadsworth Publ. Co. ———. 1993. Visualizing Data. Hobart press. Dastani, Mehdi. 2002. “The Role of Visual Perception in Data Visualization.” Journal of Visual Languages &amp; Computing 13 (6): 601–22. Electoral Commission New Zealand. 2020. “Official Referendum Results Released.” https://elections.nz/media-and-news/2020/official-referendum-results-released. ———. 2023. “E9 Statistics - Overall Results.” https://www.electionresults.govt.nz/electionresults_2023/index.html. Franconeri, Steven L, Lace M Padilla, Priti Shah, Jeffrey M Zacks, and Jessica Hullman. 2021. “The Science of Visual Data Communication: What Works.” Psychological Science in the Public Interest 22 (3): 110–61. Hand, David J. 1996. “Statistics and the Theory of Measurement.” Journal of the Royal Statistical Society: Series A (Statistics in Society) 159 (3): 445–73. Hellerstein, J. M., R. Avnur, A. Chou, C. Hidber, C. Olston, and V. Raman. 1999. “Interactive data analysis: the Control project.” Computer 32 (8): 51–59. https://doi.org/10.1109/2.781635. Humphry, Stephen. 2013. “Understanding Measurement in Light of Its Origins.” Frontiers in Psychology 4: 113. Keim, Daniel A. 2002. “Information Visualization and Visual Data Mining.” IEEE Transactions on Visualization and Computer Graphics 8 (1): 1–8. Kelley, Andew. 2023. “A Practical Guide to Applying Data Oriented Design (DoD).” Handmade Seattle. https://www.youtube.com/watch?v=IroPQ150F6c. Krantz, David H, Patrick Suppes, Duncan R Luce, and Amos Tversky. 1971. Foundations of Measurement Volume 1: Additive and Polynomial Representations. New York: Academic Press. Lord, Frederic M. 1953. “On the Statistical Treatment of Football Numbers.” Luce, R Duncan. 1959. “On the Possible Psychophysical Laws.” Psychological Review 66 (2): 81. Michell, Joel. 1986. “Measurement Scales and Statistics: A Clash of Paradigms.” Psychological Bulletin 100 (3): 398. ———. 2021. “Representational Measurement Theory: Is Its Number Up?” Theory &amp; Psychology 31 (1): 3–23. Müller, Kirill, and Hadley Wickham. 2023. Tibble: Simple Data Frames. https://CRAN.R-project.org/package=tibble. Murrell, Paul. 2005. R Graphics. Chapman; Hall/CRC. Pandas Core Team. 2024. “DataFrame — Pandas 2.2.3 Documentation.” https://pandas.pydata.org/docs/reference/frame.html. R Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/. Rentzsch, Jonathan. 2005. “Data Alignment: Straighten up and Fly Right.” IBM Developer. https://developer.ibm.com/articles/pa-dalign. Stevens, Stanley Smith. 1946. “On the Theory of Scales of Measurement.” Science 103 (2684): 677–80. ———. 1951. “Mathematics, Measurement, and Psychophysics.” Tal, Eran. 2025. “Models and Measurement.” The Routledge Handbook of Philosophy of Scientific Modeling, 256–69. Team, Polars Core. 2024. “Index - Polars User Guide.” https://docs.pola.rs. Tierney, Nicholas, and Dianne Cook. 2023. “Expanding Tidy Data Principles to Facilitate Missing Data Exploration, Visualization and Assessment of Imputations.” J. Stat. Soft. 105 (February): 1–31. https://doi.org/10.18637/jss.v105.i07. Tufte, Edward R. 2001. The Visual Display of Quantitative Information. Cheshire, Connecticut: Graphics Press LLC. Tukey, John W. 1986. “Data Analysis and Behavioral Science or Learning to Bear the Quantitative Man’s Burden by Shunning Badmandments.” The Collected Works of John W. Tukey 3: 391–484. ———. 1993. “Graphic Comparisons of Several Linked Aspects: Alternatives and Suggested Principles.” Journal of Computational and Graphical Statistics 2 (1): 1–33. Unwin, Antony, George Hawkins, Heike Hofmann, and Bernd Siegl. 1996. “Interactive Graphics for Data Sets with Missing Values—MANET.” J. Comput. Graph. Stat., June. https://www.tandfonline.com/doi/abs/10.1080/10618600.1996.10474700. Velleman, Paul F, and Leland Wilkinson. 1993. “Nominal, Ordinal, Interval, and Ratio Typologies Are Misleading.” The American Statistician 47 (1): 65–72. Vickers, Paul, Joe Faith, and Nick Rossiter. 2012. “Understanding Visualization: A Formal Approach Using Category Theory and Semiotics.” IEEE Transactions on Visualization and Computer Graphics 19 (6): 1048–61. Wickham, Hadley. 2011. “The Split-Apply-Combine Strategy for Data Analysis.” Journal of Statistical Software 40: 1–29. ———. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org. Wilke, Claus O. 2019. Fundamentals of Data Visualization: A Primer on Making Informative and Compelling Figures. O’Reilly Media. Wilkinson, Leland. 2012. The Grammar of Graphics. Springer. Ziemkiewicz, Caroline, and Robert Kosara. 2009. “Embedding Information Visualization Within Visual Representation.” In Advances in Information and Intelligent Systems, 307–26. Springer. Zig Software Foundation. 2024. “Documentation - the Zig Programming Language.” https://ziglang.org/documentation/master. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]

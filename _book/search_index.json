[["problem-set.html", "3 Problem Set 3.1 Data representation 3.2 Data transformation 3.3 Scaling", " 3 Problem Set Designing an interactive data visualization system presents a unique set of challenges. Some of these have been already touched on in the Introduction. This section discusses these inherent challenges in greater depth, and begins exploring avenues for possible solutions. 3.1 Data representation Data visualization is, first and foremost, about data (it’s in the name). However, all data is not created equal. Information can come to us in various shapes and sizes, and the way the data is structured can have a significant impact on various aspects of the visualization system, including ease of use, maintainability, and performance. 3.1.0.1 Row-based vs. column-based A common model in many data analytic languages is that of two-dimensional table or data frame. Here, the data is organized in a dictionary of columns, with each column being a homogeneous array containing values of the same type. However, unlike in a matrix data structure, different columns can store values of different types (such as floats, integers, or strings). The dataframe object can also store optional metadata, such as row names, column labels, or grouping structure (R Core Team 2024; Bouchet-Valat and Kamiński 2023). Popular examples of this design include the S3 data.frame class in base R (R Core Team 2024), the tbl_df S3 class in the tibble package (Müller and Wickham 2023), the DataFrame class in the Python pandas package (Pandas Core Team 2024), the DataFrame class in the polars library (Team 2024), or the DataFrame type in the Julia DataFrame.jl package (Bouchet-Valat and Kamiński 2023). However, the column-based organization of data is not universal. For example, the popular JavaScript data visualization and transformation library D3 (Bostock 2022) models data frames as arrays of rows, with each row being its own separate dictionary. Likewise, certain types of databases store tables as lists of records, with each record having the shape of a dictionary (Abadi et al. 2013). Within a broader programming context, these two fundamental data layouts are referred to as the struct of arrays (SoA, also known as “parallel arrays”) versus the array of structs (AoS) data structures. SoA store data in a dictionary of arrays, similar to the column-based layout, whereas AoS store data in an arrays of dictionaries, similar to row-based layout. The distinction between SoA and AoS is a bit more nuanced, since structs can store a wider class of types than just plain data, such as functions and pointers, and this makes either layout better suited to certain programming styles. For example, in object oriented programming, behaviour is encapsulated alongside data in objects (via methods/member functions), and this makes the AoS the more natural data structure within this programming paradigm (replicating the same functionality with SoA is awkward, although some modern languages offer features which make this more convenient, see e.g. Zig Software Foundation 2024). 3.1.0.2 Performance The two data layouts also offer distinct performance characteristics. The column-based (SoA) layout is generally considered to be the one better for performance (see e.g. Acton 2014; Kelley 2023). Specifically, it benefits from two important features: better memory alignment and improved cache locality. First, homogeneous arrays offer better memory characteristics than heterogeneous structs. This is because they can be stored as contiguous blocks of memory with the same alignment, eliminating the need for padding and potentially leading to a significant reduction in memory footprint (see e.g. Rentzsch 2005; Kelley 2023). Second, the column-based data layout is better suited for pre-fetching. Specifically, when performing column-wise operations, the CPU can cache the contiguously-stored values more easily, often resulting in greatly improved performance (Abadi et al. 2013; Acton 2014; Kelley 2023). However, the row-based (AoS) layout can also perform well in certain situations. Specifically, it can outperform column-based stores when retrieving individual records/rows is key, hence why it is commonly used in traditional Online Transaction Processing databases (OLTP, Abadi et al. 2013). Additionally, it could be argued that the row-based layout can be more “natural” and offer better developer ergonomics for certain programming styles. 3.2 Data transformation As was already hinted at in the previous section, when visualizing data, we rarely plot the raw data directly. Instead, the quantities we visualize are often summaries or aggregates of some kind. Take for example the typical barplot. To draw the barplot, we first need to divide the data into parts, summarize each part by some metric, usually either count or sum of some variable, and finally, render the summaries as individual bars. Likewise, many other types of plots such as boxplots or histograms include two distinct steps: splitting the data into parts and computing summaries on these parts. And, as we will see, even the plot types which show one-to-one representations of the data, such as scatterplots and parallel coordinates plots, can be viewed through the lens of partitions. This section discusses these two fundamental parts of the data visualization pipeline - partitioning and aggregation - and explores challenges associated with their implementation in an interactive data visualization system. While these operations might seem straightforward, I will make the case that they actually encode more structure than meets the eye. 3.2.1 Partitioning the data The initial step of the data visualization pipeline involves splitting data into parts. When working with typical two-dimensional tables or data frames, this operation can be imagined as slicing the table into smaller subsets of rows. Even during this simple process, two fundamental questions arise: How much of the original data should the subsets contain? What should be the relationship between these subsets? 3.2.1.1 Leave nothing out “If someone hides data from you, it’s probably because he has something to hide.” (Cairo 2016, 47) A common recommendation that many data visualization experts provide is that faithful representations should show the full data and leave nothing out. Visualizations which hide or obscure information, be it by negligence or by intent, cannot be considered truthful representations of the underlying data (Cairo 2016, 2019). Data hiding can occur in multiple ways. First, the data itself can be cherry-picked or massaged (see e.g. Lisnic et al. 2024). This is arguably the most egregious case, and can in some cases amount to malicious statistical practices such as HARKing or p-hacking (see e.g. Kerr 1998; Lisnic et al. 2024; Head et al. 2015). Second, even when showing the full data, some visualizations can obscure or downplay certain data features by incorrect utilization of design and visual encodings (Cairo 2016, 2019; Cleveland 1985; Ziemkiewicz and Kosara 2009). Finally, there is the issue of missing or incomplete data, where some data cannot be represented because it simply is not there. To give a concrete example of data hiding, consider axis limits. Cleveland (1985) argues that axis limits should generally be expanded to avoid inadvertently obscuring data near these limits. Take the following two scatterplots: Figure 3.1: Without expanding axis limits, objects at or near the limits become less salient. Left: axis limits match the data limits exactly, and so the points in the top-left and bottom-right corner of the plot) are represented by smaller area and the overall trend is distorted. Right: by expanding axis limits, we can ensure that trend is represented faithfully. In the left scatterplot, the axis limits match the data limits exactly, whereas in the right plot, they are expanded by a small fraction (5%, ggplot2 default, Wickham 2016). The left scatterplot provides a misleading representation of the underlying trend, as data points near the axis limits (top-left and bottom-right corners) are represented by a smaller area, compared to points near the centre of the plot. For instance, the point in the bottom-right corner of the plot lies simultaneously at the limits of the x- and y-axis, and is thus represented by one-quarter of the area of the points in the center. Clearly then, we can dismiss the first two data-hiding issues - cherry picking and incorrect use of design - as they involve deliberate misrepresentation. Unless there is a clear and justifiable reason, no data should be arbitrarily removed or discarded, and we should take care to provide good representations such that all data is represented faithfully. The issue of missing or incomplete data is a bit more complicated. While techniques of visualizing data with missing values do exist (see e.g. Unwin et al. 1996; Tierney and Cook 2023), these methods often depend on the specific visualization type and style. Nevertheless, the ideal model of a visualization should be that of a bijection, such that there is a clear mapping between the visualization and the data (Ziemkiewicz and Kosara 2009). 3.2.1.2 Distinctness, disjointness, and comparison “To be truthful and revealing, data graphics must bear on the question at the heart of quantitative thinking: ‘compared to what’?” (Tufte 2001, 74). “Graphics are for comparison - comparison of one kind or another - not for access to individual amounts.” (Tukey 1993) An overwhelmingly common practice in data visualization is to use geometric objects to represent distinct subsets of data. That is, in most common types of plots, each point, bar, line, or polygon typically represents a unique set of data points (rows of the data), with no overlap between objects within the same graphical layer. Despite being so common to border on a rule, this practice is often overlooked. Yet, is this practice universal? If it really were a rule, it would be vulnerable to a single counter-example, and indeed, such counter do exists. For example, certain visualizations of set-like data “double up” the contribution of data subsets, such that one subset of the data may be reflected in multiple objects (see e.g. Alsallakh et al. 2013, 2014). However, these types of visualizations are the exception rather than the norm. So where does this unconscious “law” of using objects to represent disjoint parts of the data come from? I would like to argue that it is be rooted in the fundamental purpose of data visualization: comparison (Tufte 2001; Tukey 1993). When we visualize, we draw our graphics with the ultimate goal of being able to compare our data along a set of visual channels, such as position, length, size, or colour (Bertin 1983; Wilkinson 2012; Franconeri et al. 2021; Wilke 2019). This mirrors the comparisons we make in the real world, where we compare physical objects along their respective dimensions or attributes. With disjoint parts, there is a natural correspondence or bijection between the subsets of the data and their visual representation, see Figure 3.2. Specifically, if our objects represents disjoint parts of the data, the function mapping the data parts to the geometric objects is invertible: we can pick an object, identify the corresponding subset of cases, and then use this subset to get back the original object. In plots where the objects do not represent disjoint subsets of the data, this correspondence is broken: if we identify cases corresponding to a single object, there is no simple way to map these cases back to the original object. Figure 3.2: Disjoint subsets provide a one-to-one correspondence (bijection) between geometric objects and the data. Suppose we mark out the cases corresponding to one object (the left most bar). Top row: if each geometric object (bar) represents unique set of cases, we can easily go back and forth between the object and its underlying data (middle panel). Thus, the function of picking a set of cases corresponding to an object is bijective. Bottom row: when there is an overlap between the cases represented by each object, once we pick the set of cases corresponding to that object, there is no simple way to use that set to get the corresponding object back. To illustrate this idea on concrete data, take the following barplot representing vote share among the top three parties in the 2023 New Zealand general election (Electoral Commission New Zealand 2023): Figure 3.3: Geometric objects typically represent disjoint subsets of the data. The plot shows the vote share of the top three parties in the 2023 New Zealand general election, with each bar representing a unique subset of voters. Each bar represents a unique set of voters and thus the subsets of the data represented by the bars are disjoint. Technically, there is no hard and fast rule about this. For example, we could transform the first bar by taking a union of the votes of National and Labour parties, and represent the same underlying data the following way: Figure 3.4: Hypothetically, there is nothing preventing us from encoding the same information in multiple objects, and showing non-disjoint parts of our data. The plot shows the same underlying data as 3.2, with the leftmost bar representing a union of National and Labour voters. The two leftmost bars thus do not represent disjoint subsets of the data. For a more realistic example, see Figure 3.6. However, this way of representing the data has several problems. First, there is the issue of its suitability towards the main goal of the visualization. Specifically, when visualizing election data such as this one, we are typically interested in judging the relative number of votes each party received. The second barplot makes this comparison difficult. Specifically, in the second barplot, since the National bar represents a subset of the National OR Labour bar, we have to perform additional mental calculation if we want to find out how many votes Labour received and compare the absolute counts directly (Cleveland 1985). Second, we have metadata knowledge (see e.g. Wilkinson 2012; Velleman and Wilkinson 1993) about the data being disjoint - we know that, in the New Zealand parliament electoral system, each voter can only cast one vote for a single party. Finally, there is the issue of duplicating information: in the second barplot, the number of votes the National party received is counted twice, once in the leftmost bar and again in the second-left bar. This goes against the general principle of representing our data in the most parsimonious way (Tufte 2001). Even when our goal is not to compare absolute counts, there are usually better disjoint data visualization techniques available. For example, if we were interested in visualizing the proportion of votes that each party received, we could draw the following plot: Figure 3.5: Even when proportions are of interest, there are usually disjoint data visualization techniques available. The plot shows proportion of vote share of the top three parties in the 2023 New Zealand general election, with each bar segment again representing a unique subset of voters. By stacking the bar segments on top of each other, we can easily compare proportion of the total number of votes while retaining a parsimonious representation of our data. Each bar segments now again represents a unique subset of voters. The example above was fairly clear case of where a non-disjoint representation of the data would be the wrong choice, however, there are also more ambiguous situations. One such situation is when there are multiple attributes of the data which can be simultaneously present or absent for each case. For example, in 2020, a joint referendum was held in New Zealand on the question of legalization of euthanasia and cannabis. The two issues were simultaneously included on the same ballot. The legalization of euthanasia was accepted by the voters, with 65.1% of votes supporting the decision, whereas the legalization of cannabis was rejected, with 50.7% of voters rejecting the decision (Electoral Commission New Zealand 2020). The referendum data can be visualized in the following way: Figure 3.6: Realistic example of a non-disjoint data representation. The plot shows the vote share in the combined 2020 New Zealand referendum on euthanasia and cannabis, where the two issues were simultaneousy presented on the same ballot. The two bars each show (mostly) the same subset of ballot, with each ballot contributing to the height of one segment in each bar. By definition, both bars include votes which were cast by the same voter (ignoring the votes where no preference was given for either issue, Electoral Commission New Zealand 2020). Thus, the sets of voters that the two bars and the four bar segments represent overlap. In general, there is nothing inherently wrong with the plot above. Non-disjoint representations of the data can work well for certain data types such as set-typed data (see e.g. Alsallakh et al. 2014). In the context of static data visualization, plots like these can serve useful roles. However, even here, there is a simple way of representing the same data in a disjoint way - draw two separate plots: Figure 3.7: Non-disjoint data representations can often be turned into disjoint ones. The figure again shows the vote share in the combined 2020 New Zealand referendum on euthanasia and cannabis, however, now each issue is plotted in a separate plot, and thus each geometric object in each plot represents a unique subset of ballots/voters. Why should we care about whether our representations of the data are disjoint or not? I argue that, for many types of plots, disjointness presents a fundamentally better mental model: each geometric object encodes one unique set of observational units. Conversely, if the objects in our plots do not represent disjoint subsets, then we need to keep an additional model of how they are related in our head. This issue is particularly important in interactive visualization. The natural correspondence between geometric objects and disjoint subsets of the data makes certain interactions more intuitive, and conversely, overlapping subsets induce surprising behavior. For instance, when a user clicks on a bar in a linked barplot, they might be surprised if parts of the other bars within the same plot get highlighted as well: they intended to highlight that bar, not the others. Likewise, when querying, if our objects do not represent disjoint subsets of the data, we have to think about what querying means: are we querying the objects or the cases corresponding to the objects? Disjoint partitions simplify our mental model, and this may be the reason why some authors discuss interactive features in the context of partitions (see e.g. Buja, Cook, and Swayne 1996; Keim 2002). SQL aggregation queries (GROUP BY) are based on partitions (Hellerstein et al. 1999). 3.2.1.3 Plots as partitions In the two preceding sections, I have argued the plots in our interactive data visualization system should have have two fundamental properties: Plots should show the full data (surjective mapping) Geometric objects within these plots should represent distinct subsets (disjoint parts) These two properties suggest a fundamental model for plots: that of partitions. Specifically, I propose the following definition of a regular plot: Definition 3.1 (Regular plot) Regular plot is a plot where the geometric objects within one layer represent a partition of the data, such that there is a natural bijection between these objects and (possibly aggregated) parts of the original data. While, I have not been able to find references explicitly conceptualizing plots as partitions, some data visualization researchers have used the language of bijections when discussing graphics. For example, Dastani (2002) discusses plots as bijections (homomorphisms) between data tables and visual attribute tables, however, this is already in the context of aggregated data (e.g. rows of the data table for barplot already being sums/counts). Similarly, Ziemkiewicz and Kosara (2009) and Vickers, Faith, and Rossiter (2012) argued that, in order to be visually unambiguous, plots should represent bijections of the underlying data. However, under this strict view, only one-to-one representations of the data such as scatterplots and parallel coordinate plots would be permitted (and the authors do admit that aggregation can at times present an acceptable trade-off, despite the information loss, Ziemkiewicz and Kosara 2009). The approach I take is slightly different. Instead of modeling plots as bijections between cases and the geometric objects, I model them as bijections between parts of data and the geometric objects. In other words, the bijection is not between rows of the original data table and the geometric objects, but between subtables of the original data table and the geometric objects. This approach has the advantage that aggregation can be considered as part of the bijection. Finally, Wilkinson (2012, pp 210) and Keim (2002) have linked stacked plots to (hierarchical) partitions. 3.2.1.4 Partitions and products In a typical interactive plot, the data will be partitioned across multiple dimensions. To give a concrete example, suppose we want to draw the following barplot: We start with the following data, which includes a categorical variable (group) that we will plot along the x-axis, a variable representing selection status (selection) that we will color the bar segments with, and a continuous variable that we want to summarize (value): group selection value 1 A 1 12 2 A 1 21 3 A 2 10 4 B 1 9 5 B 2 15 6 C 1 15 7 C 2 12 8 C 2 13 To draw the individual bar segments, we need to sum the value variable across the cases corresponding to each segment. To do this, we first need to split our data into multiple small disjoint subsets according to the product of group and selection variables: # Using paste0() here to simulate a product of two factors product_factor &lt;- paste0(df$group, df$selection) split_dfs &lt;- split(df, product_factor) render_tables(split_dfs) group selection value A 1 12 A 1 21 group selection value 3 A 2 10 group selection value 4 B 1 9 group selection value 5 B 2 15 group selection value 6 C 1 15 group selection value 7 C 2 12 8 C 2 13 We could then summarize each small data set by summing value: summarized_dfs &lt;- lapply(split_dfs, function(x) { aggregate(value ~ ., data = x, sum) }) render_tables(summarized_dfs) group selection value A 1 33 group selection value A 2 10 group selection value B 1 9 group selection value B 2 15 group selection value C 1 15 group selection value C 2 25 Finally, to “stack” the segments on top of each other, we need to combine the summaries back together, according to the levels of group variable, and take the cumulative sum: grouped_dfs &lt;- split(summarized_dfs, sapply(summarized_dfs, function(x) x$group)) stacked_dfs &lt;- lapply(grouped_dfs, function(x) { x &lt;- do.call(rbind, x) x$value &lt;- cumsum(x$value) rownames(x) &lt;- NULL x }) render_tables(stacked_dfs) group selection value A 1 33 A 2 43 group selection value B 1 9 B 2 24 group selection value C 1 15 C 2 40 Now, we can combine the tables into one data set and render: combined_df &lt;- do.call(rbind, stacked_dfs) combined_df$selection &lt;- factor(combined_df$selection, levels = c(2, 1)) # Need to reverse data order for ggplot2 to layer segments appropriately combined_df &lt;- combined_df[6:1, ] ggplot(combined_df, aes(x = group, y = value, fill = selection)) + geom_col(position = position_identity(), col = &quot;white&quot;) Now, we have shown how we can compute summary statistics for a stacked barplot using a simple split-apply-combine pipeline (Wickham 2011). This is in fact what happens implicitly in most ggplot2 plots: ggplot(data, aes(x, y, fill = fill)) + geom_bar() In the call above, we partition the data set by the Cartesian product of the x, y, and fill variables. That is, we break the data into parts based on the unique combinations of these variables, and then compute whatever statistical transformation we need. See the following comment from the ggplot2 documentation (Wickham 2016): # If the `group` variable is not present, then a new group # variable is generated from the interaction of all discrete (factor or # character) vectors, excluding `label`. 3.2.1.5 Limits of simple product partitions For many types of plots, the simple strategy of taking products of factors to form a single “flat” partition of the data works reasonably well. However, for other types of plots, this flat model is not enough. To give a concrete example, let’s turn back to the barplot from the section 3.2.1.3. To draw the barplot, we first split our data into smaller tables, summarized each table by summing up the values, stacked the summaries by taking their cumulative sum, and finally used these to draw the bar segments. This gave us a good visualization for comparing absolute counts across the categories. However, what if we wanted to compare proportions? It turns out there is another type of visualization, called spineplot, which can be used to represent the same underlying as a barplot, however, is much more useful for comparing proportions: Figure 3.8: The same underlying data represented as a barplot (left) and a spineplot (right). A spineplot represents the same underlying statistic as a barplot (usually sums of counts). However, unlike in barplot, where the underlying statistic gets mapped to the y-axis position/bar height, in spineplot, the underlying summary statistic gets mapped to two aesthetics: the y-axis position and the bar width. Further, the y-axis position gets normalized, such that the heights of the different segments add up to one. The result is a visualization that makes it easy to compare relative proportions across the categories. The fact that the spineplot makes it easy to see relative proportions makes it a very useful visualization. Notice how, in Figure 3.8, the spineplot makes it much easier to see that the proportions of the red cases are same across the B and C groups. Thus, spineplot is a definitely desirable type of representation for categorical data, especially if we can use interactiveity easily switch between it and the barplot. However, despite the fact that barplot and spineplot are closely related visualizations, turning one into the other is not always a trivial exercise. Specifically, many declarative data visualization systems lack simple syntax for creating spineplots. For example, in ggplot2, there is currently no simple declarative way to define a spineplot. To create the right panel in Figure 3.8, the data had to first be wrangled into the right shape outside of the ggplot2 call, and the entire process took over 10 lines of code (using standard dplyr syntax). Why are spineplots tricky? The reason is that they force us to confront the hierarchical nature of graphics. Specifically, in a spineplot, the x- and y-axes represent the same data, however, this data is summarized and transformed along different levels of aggregation: Along the x-axis, we stack the summaries across the levels of a single factor Along the y-axis, we stack the summaries across the levels of a product of two factors and normalize them by the values within the levels of the parent factor. So, in a spineplot, it is not enough to simply break our data into \\(j \\cdot k\\) tables; instead, we need to break it into \\(j\\) tables on one level, \\(j \\cdot k\\) tables on another level, and preserve this hierarchical relationship in some way, such that 3.2.1.6 Partitions and hierarchy Keim (2002) stacked plots are suited for hierarchically-partitioned data. 3.2.2 Computing summaries After we have split our data into parts, we need a way of summarizing each part via some set of summary statistics. While the computing summaries may seem like a fairly straightforward part of the data visualization pipeline, there is more complexity than meets the eye. This section explores this problem in more depth. 3.2.2.1 Graphics and statistics are not independent One appealing concept in data visualization is that of composing graphics out of independent, modular components. This idea was first introduced in Wilkinson’s Grammar of Graphics (2012), and has gained widespread popularity since, leading numerous implementations (see e.g. McNutt 2022; Kim et al. 2022; Vanderplas, Cook, and Hofmann 2020; Wickham 2010; Satyanarayan, Wongsuphasawat, and Heer 2014; Satyanarayan et al. 2016). One prominent example of such implementation is the popular ggplot2 library (Wickham 2010). In ggplot2, plots are built out of components such as geometric objects (called geoms), statistical summaries (stats), and scales. These components can be combined at will, allowing the user to express a wide range of graphics using just few primitives. The grammar-based model offers many great advantages, including simplicity, flexibility, and expressiveness. However, it also suffers from one fundamental flaw: graphics and statistics are not independent. Let’s illustrate this point on a real-world example. For this example, I will use data from the results of the men’s and women’s sport’s climbing semifinale in the Paris Summer Olympics, 2024 (International Olympic Committee 2024a, 2024b). Twenty men and twenty women from seventeen countries competed within the semifinale, in two different disciplines: bouldering and lead. The climbers could earn up to 100 points from each discipline, and the points were summed up into a total score that was then used to establish ranking and determine who would qualify for the finale. For illustration, here are five random rows of the data set: rank country name boulder lead total qualified gender 7 USA Colin DUFFY 33.8 54.1 87.9 TRUE male 16 GER Lucia DOERFFEL 29.2 51.1 80.3 FALSE female 18 ITA Laura ROGORA 13.2 57.1 70.3 FALSE female 10 JPN Tomoa NARASAKI 54.4 12.1 66.5 FALSE male 3 CZE Adam ONDRA 48.7 68.1 116.8 TRUE male To visualize the number of climbers of each country and gender, we could use the ordinary stacked barplot: library(ggplot2) ggplot(climbers, aes(country, fill = gender)) + geom_bar() The plot above is a perfectly fine representation of the data. However, what if we are interested in another summary statistic, such as the average score across climbers? A data visualization novice might be tempted to do something like this: library(ggplot2) ggplot(climbers, aes(x = country, y = total, fill = gender)) + geom_bar(stat = &quot;summary&quot;, fun = &quot;mean&quot;) At a glance, the plot above looks fine. However, what do the heights of the stacked bars represent? Each bar segment represents a mean of the total variable, grouped by the levels defined by the product of the country and gender variables. Thus, when we stack the segments on top of one another, we are essentially summing up the average score across country and gender. However, herein lies the problem. The sum of group means is not a meaningful summary statistic - it is not something that most consumers of data visualizations would care about or know how to interpret. To someone familiar with data visualization, the problem of stacking invalid statistics is nothing new. Many data visualization researchers have warned about this: “Stacking is useful when the sum of the amounts represented by the individual stacked bars is in itself a meaningful amount” (Wilke 2019, 52). “Because this gives the visual impression of one element that is the sum of several others, it is very important that if the element’s size is used to display a statistic, then that statistic must be summable. Stacking bars that represent counts, sums, or percentages are fine, but a stacked bar chart where bars show average values is generally meaningless.” (Wills 2011, 112). “[…] We do this to ensure that aggregate statistics are always computed over the input data, and so users do not inadvertantly compute e.g., averages of averages, which can easily lead to misinterpretation.” (Wu 2022) So does this mean we can only ever stack counts and sums? Take a look at the following plot: aa 3.2.2.2 Transforming summaries 3.3 Scaling Suppose we have partitioned our data and computed the relevant summary statistics. Now we need a way to to encode these summaries into visual attributes that we can then present on the computer screen. In most data visualization systems, this is done by specialized components called scales or coordinate systems (see e.g. Murrell 2005; Wickham 2016; Wilkinson 2012). 3.3.1 Theory of scales Within the data visualization literature, there exists a fairly large body of research on the theoretical properties of scales. A full treatment is beyond the scope of the present thesis. However, in this section, I will attempt to briefly summarize some important findings and concepts. 3.3.1.1 Philosophy, psychology, and measurement One challenge when discussing scales in data visualization is that the topic unavoidably intersects with a research area that has a particularly long and contentious history: theory of measurement (see e.g. Hand 1996; Michell 1986; Tal 2025). Theory of measurement (not to be confused with measure theory, with which it nevertheless shares some overlap) is the research area which tries to answer the deceptively simple question: what does it mean to measure something? This seemingly trivial problem has inspired long and fiery debates within the fields of mathematics, philosophy, and social science. Particularly, in psychology, where assigning numerical values non-physical phenomena such as moods and mental states is a central concern, the topic has garnered a significant amount of attention, creating a dense body of research (see e.g. Humphry 2013; Michell 2021). Arguably, the most influential work in this field has been that of Stevens (1946). In his fairly concise paper, Stevens defined a scale as method of assigning numbers to values, and introduced a four-fold classification classification, namely: nominal, ordinal, interval, and ratio scales (see Table 3.1). Table 3.1: Types of scales identified by Stevens (1946) Scale Structure Comparison Valid transformations Nominal Isomorphism Are \\(x\\) and \\(y\\) the same? \\(x&#39; = f(x)\\), where \\(f\\) is a bijection Ordinal Monotone map Is \\(x\\) is greater than \\(y\\)? \\(x&#39; = f(x)\\), where \\(f\\) is a monotone bijection Interval Affine transformation How far is \\(x\\) from \\(y\\)? \\(x&#39; = ax + b\\), for \\(a, b \\in \\mathbb{R}\\) Ratio Linear map How many times is \\(x\\) greater than \\(y\\)? \\(x&#39; = ax\\), for \\(a \\in \\mathbb{R}\\) The Steven’s (1946) typology is based on invariance under transformation. Specifically, for each class of scales, we define a set of transformations that preserve valid comparisons. The set of valid transformations shrinks as we move from one class of scales to another. For nominal scales, any kind of bijective transformation is valid. Intuitively, we can think of the scale as assigning labels to values, and any kind re-labeling is valid, as long as it preserves equality of the underlying values. For instance, given a nominal scale with three values, we can assign the labels \\(\\{ \\text{red}, \\text{green}, \\text{blue} \\}\\) or \\(\\{ \\text{monday}, \\text{tuesday}, \\text{wednesday} \\}\\) in any way we like, as long as each value maps to a unique label. This identifies the underlying mathematical structure as an isomorphism. Ordinal scales are more restrictive, since, on top of preserving equality, transformations also need to preserve order. For example, if we want to assign the labels \\(\\{ \\text{monday}, \\text{tuesday}, \\text{wednesday} \\}\\) to an ordinal scale with three values, there is only one way to do it that preserves the underlying order: assign the least values to \\(\\text{monday}\\), the middle value to \\(\\text{tuesday}\\), and the greatest value to \\(\\text{wednesday}\\) (assuming we order the labels/days in the usual day-of-week order). However, there is no notion of distance between the labels: we could just as well assign the values labels in \\(\\mathbb{N}\\) such as \\(\\{ 10, 20, 30 \\}\\), \\(\\{1, 2, 9999 \\}\\), and so on. Thus, the fundamental mathematical structure is that of a monotone map. Interval scales need to additionally preserve equality of intervals. This means that, for any three values \\(a, b,\\) and \\(c\\), if the distances between \\(a\\) and \\(b\\) and \\(b\\) and \\(c\\) are equal, \\(d(a, b) = d(b, c)\\), then so should be the distances between the scaled labels, \\(d^*(f(a), f(b)) = d^*(f(b), f(c)\\). For most real applications, this limits interval scales to the class of affine transformations of the form \\(f(x) = ax + b\\). A canonical example of an interval scale is the conversion formula of degrees Celsius to Fahrenheit: \\(f(c) = 9/5 \\cdot c + 32\\) (Stevens 1946). This example also highlights an important property of interval scales: the zero point can be arbitrary and ratios are not meaningful. Specifically, since the zero points of both Celsius and Fahrenheit scales were chosen based on arbitrary metrics (freezing temperatures of water and brine, respectively), it does not make sense to say that, e.g. 20°C is “twice as hot” as 10°C, in the same way that it does not make sense to say that 2000 CE is “twice as late” as 1000 CE. Finally, ratio scales also need to preserve the equality of ratios. Specifically, if \\(a/b = b/c\\) then \\(f(a)/f(b) = f(b) / f(c)\\). As a consequence, this also means that the scale must have a well-defined zero-point. Examples of ratio scales include physical magnitudes such as height and weight, which have a well-defined zero point (Stevens 1946). Steven’s (1946) typology sparked a considerable debate, on multiple fronts. First, since the original publication, many authors have sought to either expand upon or criticize Steven’s typology. However, despite some monumental efforts towards a unified theory, such as that of Krantz et al. (1971), measurement has remained a hotly debated topic to this day (see e.g. Michell 2021; Tal 2025). Second, more relevant to statistics, some authors such as Stevens (1951) and Luce (1959) used the theory to define come up with prescriptive rules for statistical transformations. For example, some have suggested that taking the mean of an ordinal variable is not permissible, since the meaning of the average is not preserved under monotone transformation. However, this issue was hotly contested by others such as Lord (1953), Tukey (1986), and Velleman and Wilkinson (1993), who argued that many well-established statistical practices, such as rank-based tests and coefficients of variations, rely on “impermissible” statistics but can nevertheless yield valuable insights. More broadly, these authors also argued that data is not really meaningful on its own, but instead derives its meaning from the statistical questions it is being used to answer (see also Wilkinson 2012). At this point, the discussion around measurement has become far too dense and theoretical, and most data visualization researchers seem to avoid delving into it too deeply (see e.g. Wilkinson 2012). Nevertheless, there are still some areas where the issues of measurement and Steven’s typology do crop up. For instance, when scaling area based on a continuous variable, a common recommendation is to start the scale at zero to ensure accurate representations of ratios (see e.g. Hadley Wickham 2024), aligning with Steven’s definition of a ratio scale. Likewise, the long-standing debate around whether the base of a barplot should always start at zero (see e.g. Cleveland 1985; Wilkinson 2012) also carries echoes of the measurement debate. Ultimately, it may yet require long time to settle the issues around measurement, however, there are definitely some ideas in the literature that data visualization can benefit from. 3.3.1.2 Visual perception and encodings Another important research area is how scales apply to visual perception. Specifically, given that we use visual attributes such as position, color, length, or area to encode various aspects of our data, researchers have tried to answer the question of how to use these attributes in a way that best leverages the human visual system. Fortunately, this research has been quite fruitful, yielding precise and actionable guidelines (for a review, see Franconeri et al. 2021; Quadri and Rosen 2021). A landmark work in this area has been that of Cleveland and McGill (1984). In this study, the authors conducted a series of empirical experiments in which they investigated people’s ability to accurately judge quantities based on different visual encodings. They found that judgments based on position along a common scale were the most accurate, followed by length-based comparisons, and then angle-based comparisons. The findings were later corroborated by other authors. Heer and Bostock (2010) replicated the Cleveland and McGill (1984) study, and included judgements of circular and rectangular areas which were found to be less accurate than position, length, or angle. Other authors have extended these experiments to other visual encodings, such as color or density (e.g. Demiralp, Bernstein, and Heer 2014; Saket et al. 2017; Reda, Nalawade, and Ansah-Koi 2018). Together, these findings have been used to create rankings of visual encodings, with researchers generally agreeing on the following ordered list: position, length, area, angle, and intensity (from most effective to least, Mackinlay 1986; Franconeri et al. 2021; Quadri and Rosen 2021). 3.3.2 Applied scales 3.3.2.1 Scale transformation “Transformation is a critical tool for visualization or for any other mode of data analysis because it can substantially simplify the structure of a set of data.” Cleveland (1993), pp. 48 References Abadi, Daniel, Peter Boncz, Stavros Harizopoulos, Stratos Idreos, Samuel Madden, et al. 2013. “The Design and Implementation of Modern Column-Oriented Database Systems.” Foundations and Trends in Databases 5 (3): 197–280. Acton, Mike. 2014. “Data-Oriented Design and c++.” Luento. CppCon. https://www.youtube.com/watch?v=rX0ItVEVjHc. Alsallakh, Bilal, Wolfgang Aigner, Silvia Miksch, and Helwig Hauser. 2013. “Radial Sets: Interactive Visual Analysis of Large Overlapping Sets.” IEEE Transactions on Visualization and Computer Graphics 19 (12): 2496–2505. Alsallakh, Bilal, Luana Micallef, Wolfgang Aigner, Helwig Hauser, Silvia Miksch, and Peter Rodgers. 2014. “Visualizing Sets and Set-Typed Data: State-of-the-Art and Future Challenges.” Eurographics Conference on Visualization (EuroVis). Bertin, Jacques. 1983. Semiology of Graphics. University of Wisconsin press. Bostock, Mike. 2022. “D3.js - Data-Driven Documents.” https://d3js.org. Bouchet-Valat, Milan, and Bogumił Kamiński. 2023. “DataFrames.jl: Flexible and Fast Tabular Data in Julia.” Journal of Statistical Software 107 (September): 1–32. https://doi.org/10.18637/jss.v107.i04. Buja, Andreas, Dianne Cook, and Deborah F Swayne. 1996. “Interactive High-Dimensional Data Visualization.” Journal of Computational and Graphical Statistics 5 (1): 78–99. Cairo, Alberto. 2016. The Truthful Art: Data, Charts, and Maps for Communication. New Riders. ———. 2019. How Charts Lie: Getting Smarter about Visual Information. WW Norton &amp; Company. Cleveland, William S. 1985. The Elements of Graphing Data. Wadsworth Publ. Co. ———. 1993. Visualizing Data. Hobart press. Cleveland, William S, and Robert McGill. 1984. “Graphical Perception: Theory, Experimentation, and Application to the Development of Graphical Methods.” Journal of the American Statistical Association 79 (387): 531–54. Dastani, Mehdi. 2002. “The Role of Visual Perception in Data Visualization.” Journal of Visual Languages &amp; Computing 13 (6): 601–22. Demiralp, Çağatay, Michael S Bernstein, and Jeffrey Heer. 2014. “Learning Perceptual Kernels for Visualization Design.” IEEE Transactions on Visualization and Computer Graphics 20 (12): 1933–42. Electoral Commission New Zealand. 2020. “Official Referendum Results Released.” https://elections.nz/media-and-news/2020/official-referendum-results-released. ———. 2023. “E9 Statistics - Overall Results.” https://www.electionresults.govt.nz/electionresults_2023/index.html. Franconeri, Steven L, Lace M Padilla, Priti Shah, Jeffrey M Zacks, and Jessica Hullman. 2021. “The Science of Visual Data Communication: What Works.” Psychological Science in the Public Interest 22 (3): 110–61. Hadley Wickham, Danielle Navarro. 2024. Ggplot2: Elegant Graphics for Data Analysis (3e). https://ggplot2-book.org. Hand, David J. 1996. “Statistics and the Theory of Measurement.” Journal of the Royal Statistical Society: Series A (Statistics in Society) 159 (3): 445–73. Head, Megan L, Luke Holman, Rob Lanfear, Andrew T Kahn, and Michael D Jennions. 2015. “The Extent and Consequences of p-Hacking in Science.” PLoS Biology 13 (3): e1002106. Heer, Jeffrey, and Michael Bostock. 2010. “Crowdsourcing Graphical Perception: Using Mechanical Turk to Assess Visualization Design.” In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, 203–12. Hellerstein, J. M., R. Avnur, A. Chou, C. Hidber, C. Olston, and V. Raman. 1999. “Interactive data analysis: the Control project.” Computer 32 (8): 51–59. https://doi.org/10.1109/2.781635. Humphry, Stephen. 2013. “Understanding Measurement in Light of Its Origins.” Frontiers in Psychology 4: 113. International Olympic Committee. 2024a. “Men’s Boulder &amp; Lead, Semifinal Lead - Sport Climbing \\(\\vert\\) Paris 2024 Olympics.” https://olympics.com/en/paris-2024/results/sport-climbing/men-s-boulder-and-lead/sfnl000200--. ———. 2024b. “Women’s Boulder &amp; Lead, Semifinal Lead - Sport Climbing \\(\\vert\\) Paris 2024 Olympics.” https://olympics.com/en/paris-2024/results/sport-climbing/women-s-boulder-and-lead/sfnl000200--. Keim, Daniel A. 2002. “Information Visualization and Visual Data Mining.” IEEE Transactions on Visualization and Computer Graphics 8 (1): 1–8. Kelley, Andew. 2023. “A Practical Guide to Applying Data Oriented Design (DoD).” Handmade Seattle. https://www.youtube.com/watch?v=IroPQ150F6c. Kerr, Norbert L. 1998. “HARKing: Hypothesizing After the Results Are Known.” Personality and Social Psychology Review 2 (3): 196–217. Kim, Hyeok, Ryan Rossi, Fan Du, Eunyee Koh, Shunan Guo, Jessica Hullman, and Jane Hoffswell. 2022. “Cicero: A Declarative Grammar for Responsive Visualization.” In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems, 1–15. Krantz, David H, Patrick Suppes, Duncan R Luce, and Amos Tversky. 1971. Foundations of Measurement Volume 1: Additive and Polynomial Representations. New York: Academic Press. Lisnic, Maxim, Zach Cutler, Marina Kogan, and Alexander Lex. 2024. “Visualization Guardrails: Designing Interventions Against Cherry-Picking in Interactive Data Explorers.” https://osf.io/preprints/osf/4j9nr. Lord, Frederic M. 1953. “On the Statistical Treatment of Football Numbers.” Luce, R Duncan. 1959. “On the Possible Psychophysical Laws.” Psychological Review 66 (2): 81. Mackinlay, Jock. 1986. “Automating the Design of Graphical Presentations of Relational Information.” Acm Transactions On Graphics (Tog) 5 (2): 110–41. McNutt, Andrew M. 2022. “No Grammar to Rule Them All: A Survey of Json-Style Dsls for Visualization.” IEEE Transactions on Visualization and Computer Graphics 29 (1): 160–70. Michell, Joel. 1986. “Measurement Scales and Statistics: A Clash of Paradigms.” Psychological Bulletin 100 (3): 398. ———. 2021. “Representational Measurement Theory: Is Its Number Up?” Theory &amp; Psychology 31 (1): 3–23. Müller, Kirill, and Hadley Wickham. 2023. Tibble: Simple Data Frames. https://CRAN.R-project.org/package=tibble. Murrell, Paul. 2005. R Graphics. Chapman; Hall/CRC. Pandas Core Team. 2024. “DataFrame — Pandas 2.2.3 Documentation.” https://pandas.pydata.org/docs/reference/frame.html. Quadri, Ghulam Jilani, and Paul Rosen. 2021. “A Survey of Perception-Based Visualization Studies by Task.” IEEE Transactions on Visualization and Computer Graphics. R Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/. Reda, Khairi, Pratik Nalawade, and Kate Ansah-Koi. 2018. “Graphical Perception of Continuous Quantitative Maps: The Effects of Spatial Frequency and Colormap Design.” In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems, 1–12. Rentzsch, Jonathan. 2005. “Data Alignment: Straighten up and Fly Right.” IBM Developer. https://developer.ibm.com/articles/pa-dalign. Saket, Bahador, Arjun Srinivasan, Eric D Ragan, and Alex Endert. 2017. “Evaluating Interactive Graphical Encodings for Data Visualization.” IEEE Transactions on Visualization and Computer Graphics 24 (3): 1316–30. Satyanarayan, Arvind, Dominik Moritz, Kanit Wongsuphasawat, and Jeffrey Heer. 2016. “Vega-Lite: A Grammar of Interactive Graphics.” IEEE Transactions on Visualization and Computer Graphics 23 (1): 341–50. Satyanarayan, Arvind, Kanit Wongsuphasawat, and Jeffrey Heer. 2014. “Declarative Interaction Design for Data Visualization.” In Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology, 669–78. Stevens, Stanley Smith. 1946. “On the Theory of Scales of Measurement.” Science 103 (2684): 677–80. ———. 1951. “Mathematics, Measurement, and Psychophysics.” Tal, Eran. 2025. “Models and Measurement.” The Routledge Handbook of Philosophy of Scientific Modeling, 256–69. Team, Polars Core. 2024. “Index - Polars User Guide.” https://docs.pola.rs. Tierney, Nicholas, and Dianne Cook. 2023. “Expanding Tidy Data Principles to Facilitate Missing Data Exploration, Visualization and Assessment of Imputations.” J. Stat. Soft. 105 (February): 1–31. https://doi.org/10.18637/jss.v105.i07. Tufte, Edward R. 2001. The Visual Display of Quantitative Information. Cheshire, Connecticut: Graphics Press LLC. Tukey, John W. 1986. “Data Analysis and Behavioral Science or Learning to Bear the Quantitative Man’s Burden by Shunning Badmandments.” The Collected Works of John W. Tukey 3: 391–484. ———. 1993. “Graphic Comparisons of Several Linked Aspects: Alternatives and Suggested Principles.” Journal of Computational and Graphical Statistics 2 (1): 1–33. Unwin, Antony, George Hawkins, Heike Hofmann, and Bernd Siegl. 1996. “Interactive Graphics for Data Sets with Missing Values—MANET.” J. Comput. Graph. Stat., June. https://www.tandfonline.com/doi/abs/10.1080/10618600.1996.10474700. Vanderplas, Susan, Dianne Cook, and Heike Hofmann. 2020. “Testing Statistical Charts: What Makes a Good Graph?” Annual Review of Statistics and Its Application 7: 61–88. Velleman, Paul F, and Leland Wilkinson. 1993. “Nominal, Ordinal, Interval, and Ratio Typologies Are Misleading.” The American Statistician 47 (1): 65–72. Vickers, Paul, Joe Faith, and Nick Rossiter. 2012. “Understanding Visualization: A Formal Approach Using Category Theory and Semiotics.” IEEE Transactions on Visualization and Computer Graphics 19 (6): 1048–61. Wickham, Hadley. 2010. “A Layered Grammar of Graphics.” Journal of Computational and Graphical Statistics 19 (1): 3–28. ———. 2011. “The Split-Apply-Combine Strategy for Data Analysis.” Journal of Statistical Software 40: 1–29. ———. 2016. Ggplot2: Elegant Graphics for Data Analysis (2e). Springer-Verlag New York. https://ggplot2.tidyverse.org. Wilke, Claus O. 2019. Fundamentals of Data Visualization: A Primer on Making Informative and Compelling Figures. O’Reilly Media. Wilkinson, Leland. 2012. The Grammar of Graphics. Springer. Wills, Graham. 2011. Visualizing Time: Designing Graphical Representations for Statistical Data. Springer Science &amp; Business Media. Wu, Eugene. 2022. “View Composition Algebra for Ad Hoc Comparison.” IEEE Transactions on Visualization and Computer Graphics 28 (6): 2470–85. Ziemkiewicz, Caroline, and Robert Kosara. 2009. “Embedding Information Visualization Within Visual Representation.” In Advances in Information and Intelligent Systems, 307–26. Springer. Zig Software Foundation. 2024. “Documentation - the Zig Programming Language.” https://ziglang.org/documentation/master. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]

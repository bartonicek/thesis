[["index.html", "Fluent Graphics 1 Abstract", " Fluent Graphics Adam Bartonicek 1 Abstract An abstract will go here. "],["introduction.html", "2 Introduction 2.1 Brief history of interactive data visualization 2.2 What even is interactive data visualization? 2.3 The highlighting problem 2.4 Mathematical theory 2.5 Theory of data visualization systems", " 2 Introduction 2.1 Brief history of interactive data visualization 2.1.1 Early interactive data visualization: By statisticians for statisticians Static data visualization has a rich and intricate history, and a full treatment is beyond the scope of the present thesis (but see e.g. Dix and Ellis 1998; Friendly 2006; Friendly and Wainer 2021; Young, Valero-Mora, and Friendly 2011). Briefly, prior to the mid-20th century, data visualization was often considered as at best secondary to “serious” statistical analysis (although there were also some counter-examples, see e.g. Friendly 2006; Young, Valero-Mora, and Friendly 2011). However, beginning in the late 1950’s, a series of developments took place which lead to a rise in the prominence and accessibility of data visualization. Firstly, at the theoretical level, the work of Tukey (1962; 1977) and Bertin (1967) established data visualization as valuable discipline in its own right. Secondly, at the practical level, the development of personal computers (see e.g. Abbate 1999) and high-level programming languages such as FORTRAN in 1954 (Backus 1978), made the process of rendering production-grade figures near-effortless in comparison to the earlier hand-drawn techniques. Combined, these developments lead to a surge in the use and dissemination of data visualization. With static data visualization on the rise in the 1950’s, interactive data visualization would not be left far behind. The very early interactive data visualization systems tended to be designed for niche, specialized tasks. For example, Fowlkes (1969) designed a system which allowed the users to view probability plots under different configurations of parameters and transformations, whereas Kruskal (1965) created a tool for visualizing multidimensional scaling. However, soon, researchers began exploring interactive data visualization as a general-purpose tool for data exploration. The first such general-purpose system was PRIM-9 (Fisherkeller, Friedman, and Tukey 1974). PRIM-9 allowed for exploration of multivariate data via interactive features such as projection, rotation, masking, and filtering. Subsequent systems strove to provide an even wider range of features. For example, MacSpin (Donoho, Donoho, and Gasko 1988) and XGobi (Swayne, Cook, and Buja 1998) implemented features such as interactive scaling, rotation, linked selection (or “brushing”), and interactive plotting of smooth fits in scatterplots, as well as interactive parallel coordinate plots and grand tours (excellent video-documentaries of some of these early interactive data visualization systems are available at ASA Statistical Graphics Video Library). Later, with the proliferation of open-source, general-purpose statistical computing software such as S and R, . The successor system to XGobi, GGobi (Swayne et al. 2003), expanded on XGobi and made it directly embeddable in the R runtime. Mondrian (Theus 2002) allowed for sophisticated linked interaction between many different types of plots including scatteplots, histograms, barplots, scatterplot, mosaic plots, parallel coordinates plots, and maps. Finally, iPlots (Urbanek and Theus 2003) implemented a general framework for interactive plotting that was not only embedded in R but could be directly programmatically manipulated, and was later further expanded and made performant for big data in iPlots eXtreme (Urbanek 2011). What all of these interactive data visualization systems had in common is that they were designed by statisticians and with interesting, often ambitious interactive features in mind. High-level analytic features such as linked selection/brushing, rotation and projection, and interactive manipulation of model parameters made frequent appearances. While being a clear strength, the more complex nature of the systems may have also slowed their adoption, as they often demanded an expert user to take advantage of most fully. 2.1.2 Interactive data visualization and the Web: World-wide interactivity The tail end of the millennium would mark the arrival of a whole new class of technologies that had a significant impact on interactive data visualization, just as it had on almost every other field of human endeavor. The arrival of the internet in the mid 1990’s, and the development of JavaScript in 1995 as a high-level programming language for the Web (for a thorough description of the history, see e.g. Wirfs-Brock and Eich 2020), saw the rise of interactive applications that could be accessed by anyone, from anywhere. This was aided by the dissemination of robust and standardized Web browsers. Soon, interactive data visualization became just one of many interactive technologies highly sought after within the fledgling Web ecosystem. Early interactive data visualization systems for the Web, such as Prefuse (Heer, Card, and Landay 2005) and Flare (developed around 2008, Blokt 2020) tended to relied on external plugins (Java and Adobe Flash Player, respectively). However, as browsers got faster at interpreting JavaScript, thanks to advances in compiler technologies, specifically Just In Time (JIT) compilation, it became possible to write entire data visualization libraries in JavaScript. In the late 2000’s and early 2010’s, several true Web-native interactive data visualization systems emerged. Currently, the most prominent framework for data visualization on the Web, interactive or otherwise, is D3.js (Mike Bostock 2022). D3 is a fairly general and low-level JavaScript framework for visualizing data, and consists of a suite of specialized modules designed for various aspects of the data visualization workflow, including parsing data, transformation, defining scales, interfacing with the DOM and handling interaction events, and even physics simulation and animation. Importantly, while D3 does provide methods for handling interactive events, it does not provide a system for dispatching and coordinating these events - it instead delegates this responsibility to the user and encourages the use of reactive Web frameworks such as React (Meta 2024), Vue (Evan You and the Vue Core Team 2024), or Svelte (Rich Harris and the Svelte Core Team 2024). Finally, D3.js visualizations are rendered as Scalable Vector Graphics (SVG) by default, meaning that they can be scaled without loss of quality. However, a price to pay for this robustness is in performance, as visualizations can become slow to render at high data volumes. The modular nature of D3 means that the visualizations can be rendered by other, more performant devices, such as the HTML 5 Canvas element or WebGL, however, as of this date, the implementation of such an alternative rendering framework is left to the user and there are no official modules. Building upon the fairly low level framework provided by D3, many packages have been developed to provide a more high-level and opinionated interfaces. Prominent two among these are plotly.js (Plotly Inc. 2022) and Highcharts (Highsoft 2024). While D3 provides low-level utilities such as data transformations, scales, and geometric objects, these packages provide more high-level utilities such as functions for rendering complete plots and registering reactive events, which are under the hood automatically handled via systems based on the native DOM Event Target interface (MDN 2024a). Like D3, both plotly.js and Highcharts also render the graphics in SVG by default, however, unlike D3, they both also provide alternative rendering engines based on WebGL (Highsoft 2022; Plotly Inc. 2024). A somewhat different approach is taken by another popular visualization package built partially on D3 - Vega (Satyanarayan et al. 2015; Vega Project 2024b). Vega provides a declarative framework for defining (interactive) data visualizations using a static JSON schema. Compared to plotly.js or Highcharts, Vega is significantly more expressive, allowing for fine-grained customization of graphics and interactive behavior, standing essentially just one level above D3. However, as a consequence, it is also significantly more verbose. For example, a full specification of a scatterplot matrix with linked brushing takes over 300 lines of code (JSON, not including the data and using default formatting such as would be created by calling JSON.stringify(schema, null, \"\\t\"), Vega Project 2024a). While these contemporary Web-based interactive data visualization systems offer great deal of flexibility and customizability, I argue that this comes at the cost of making them practical for applied researchers and data scientists. Most importantly, it seems that there is some ambiguity about what counts as interactive features. For example, in the R Graph Gallery entry on Interactive Charts (Holtz 2022), which features several examples of interactive visualization derived from the above-mentioned JavaScript interactive data visualization libraries, the visualizations feature interactions such zooming, panning, hovering, 3D rotation, and repositioning a node within a network graph. However, in all of these examples, the user only manipulates surface-level graphical attributes of a single plot. In contrast, the Plotly Dash documentation page on Interactive Visualizations (Plotly Inc. 2022) does feature two examples of linked hovering and cross-filtering, i.e. examples of linked interactivity. However, it should be noted that vast majority of visualizations in the Plotly R Open Source Graphing Library documentation page (Plotly Inc. 2022) allow for only surface-level interactions. Similarly, VegaLite Gallery pages on Interactive Charts and Interactive Multiview Displays (Vega Project 2022) feature many examples, however, only a few show limited examples of linked or parametric interactivity. Finally, the Highcharter Showcase Page (Kunst 2022) does not feature any examples of linking or parametric interactivity. What all of the packages listed above have in common is that most featured interaction is typically surface-level and takes place within a single plot, and the few examples that feature interesting types of interactivity (linked or parametric) often require a complicated setup. The main reason for this is most likely that all of these packages have been designed to be very general-purpose and flexible, and the price to pay for this flexibility is that complex types of interactivity require complex code. Another reason is that these packages have been built for static visualizations first, and interactivity second. Further, since all of these packages are native to JavaScript, the expectation may be that if more interesting types of interactivity are desired, the interactive “back-end” may be written separately, outside of the package. Finally, the typical use case for these packages seems to be presentation, not EDA. Be it as it may, there is a fairly high barrier for entry for creating interesting types of interactivity (i.e. linked or parametric) with these packages. This may not be an issue for large organizations which can afford to hire computer science specialists to work on complex interactive dashboards and visualizations full-time. However, to the average applied scientist or data scientist, the upfront cost of producing a useful interactive data visualization may be too high, especially if one is only interested in exploratory data analysis for one’s own benefit. This may be the reason why interactive visualizations are nowadays mainly used for data communication, not data exploration (Batch and Elmqvist 2017). On a higher level, the current options for interactive data visualization may reflect a broader cultural differences between Computer Science and Statistics, where Computer Science may be more oriented towards business and large-team collaboration, whereas Statistics may be more focused on applied research and individual/small-team workflow. 2.2 What even is interactive data visualization? If it looks like a duck, swims like a duck, and quacks like a duck, then it probably is a duck. […] The irony is that while the phrase is often cited as proof of abductive reasoning, it is not proof, as the mechanical duck is still not a living duck Duck Test entry, (Wikipedia 2022) What is interactive data visualization? Surprisingly, despite the widespread popularity of interactive visualizations, if you ask researchers, you may get many different and at times even incongruent answers (see e.g. Dimara and Perin 2019; Elmqvist et al. 2011; Pike et al. 2009). Within the literature, the terms “interactive” and “interaction” are used in many different ways and across a wide variety of contexts, with an explicit definition being rarely given. The lack of a clear consensus about what “interactive data visualization” is makes the task of discussing existing work challenging. On one hand, ignoring the issue might leave the reader confused about the relevant concepts. On the other, a comprehensive account of the terminology surrounding interactive data visualization would almost surely become too dense; entire research papers have been dedicated to this topic (see e.g. Dimara and Perin 2019; Elmqvist et al. 2011). Therefore, in the following section, I have tried to strike a balance by providing a concise yet informative-enough account of how interactivity has been conceptualized within the existing literature. Ultimately, the goal is to provide the reader with context and define what the terms “interactive” and “interaction” should mean for the scope of the present thesis. The following section is laid out as follows. I start with a brief overview of the history of the field, of data visualization more generally and interactive data visualization more specifically. Then I discuss the different ways the term “interactive data visualization” has been used throughout the literature, and finally I arrive at a working definition of interactivity for the scope of the present thesis. 2.2.1 Interactive vs. interacting with First, when we say “interactive data visualization”, are we referring to a concrete figure or chart, or are we referring to the process of interacting with such a figure? In other words, are we using the term “visualization” as a noun or a verb? Here already we can see a significant overloading of the term (Dimara and Perin 2019; Pike et al. 2009; see also Yi et al. 2007). The split between these two meanings is quite noticeable within the interactive data visualization literature. On one hand, there are some papers which focus on the mathematical and computational aspects of interactive data visualization, discussing specific systems and implementations (see e.g. Buja, Cook, and Swayne 1996; Kelleher and Levkowitz 2015; Leman et al. 2013; Wills 2008). On the other hand, there are papers which approach the topic from a more cognitive or human-computer interaction (HCI) point of view: exploring what impact different kinds of visualization and interaction styles have on the user’s ability to derive insights from the data (see e.g. Dimara and Perin 2019; Dix and Ellis 1998; Pike et al. 2009; Quadri and Rosen 2021; Yi et al. 2007). There is of course a significant overlap between these two uses of the term “interactive data visualization”: most papers discuss both concrete implementations of interactive data visualization systems and the user’s actions and experiences while using those systems. Nevertheless, the fact that the term is used to refer to both the user’s actions and experiences as well as the object of these actions and experiences can make can make literature search complicated - whenever searching for any subtopic within one of the two fields, one will inevitably find hits from the other. It also highlights an important fact about interactive data visualization as a research area: rather than being a single field, it is actually an intersection of several different fields, including statistics, computer science, applied mathematics, business analytics, human-computer interaction, and cognitive psychology (Dimara and Perin 2019). While I do plan to discuss some elementary features of the psychology of interacting with visualizations, when used throughout this thesis, the term “interactive data visualization” will refer to concrete charts or figures, typically displayed on a computer screen. When referring to the practice of interactive data visualization, I will attempt to use more active phrasing such as “interacting with a visualization” or “user’s interaction with a visualization”, to indicate that what is being referred to is the activity or process of visualization, rather than any concrete object or implementation. 2.2.2 What counts as “interactive enough”? But even when we use the term “interactive data visualization” to refer to concrete charts or figures, the meaning still remains ambiguous. What is the bar for calling a figure “interactive”? What features should an interactive figure have? Among data visualization researchers, there are considerable differences of opinion, such that the same figure may be considered interactive by some but not by others. And these differences are important - they are not just a matter of opinion or aesthetic taste. When building interactive data visualization systems, what we consider “interactive” has a profound impact on the implementation details and requirements of the system. Consider a scatterplot with a color palette widget that can be used to select the color of the points. Does such a feature justify the scatterplot being called an “interactive data visualization”? There are some researchers who will answer affirmatively - for them, interactivity is something fundamental, and if the user is able to manipulate some visual aspect of the figure, that’s enough to call the visualization interactive. To some, almost any user manipulation qualifies (Brodbeck, Mazza, and Lalanne 2009). Other researchers emphasize speed of the computer’s responses to user interaction, with faster updates translating to greater interactivity (Becker and Cleveland 1987; Buja, Cook, and Swayne 1996). Complicating matters further, some of these researchers also make the distinction between “interactive” and “dynamic” manipulation, where interactive manipulation involves discrete actions such as pressing a button or selecting an item from a drop-down menu, whereas dynamic manipulation involves continuous actions, like moving a slider or clicking-and-dragging to highlight a rectangular area (Rheingans 2002; Jankun-Kelly, Ma, and Gertz 2007; see also Dimara and Perin 2019). Yet, for other researchers, simple features such as changing the color of points in a scatterplot are far too low of a bar. For many, true interactivity hinges on high-level analytic features which allow the practitioner to derive insights from the data which would be much harder or time-intensive to derive from static visualizations. These features include the ability to generate different views of the data (by e.g. zooming, panning, sorting, and filtering), and the reactive propagation of changes between connected or “linked” parts of a figure (Kehrer et al. 2012; Buja, Cook, and Swayne 1996; Keim 2002; Unwin 1999). Similarly, in visual analytics research, a distinction is made between “surface-level” (or “low-level”) and “parametric” (or “high-level”) interactions, where surface-level interactions manipulate attributes of the visual domain only (e.g. zooming and panning), whereas parametric interactions manipulate attributes of mathematical models or algorithms underlying the visualization (Leman et al. 2013; Pike et al. 2009). Table 2.1 summarizes the several ways of defining interactivity as discussed above. Note that the list is not supposed to be exhaustive; more complete taxonomies of interactive visualization systems and features have been described before (see e.g. Dimara and Perin 2019; Yi et al. 2007). Instead, I want to use the list to broadly summarize the ways researchers have thought about interactivity, and to have a place to refer the reader to when discussing these ideas later on in the text. Table 2.1: Table 2.2: Definitions of Interactivity Feature Details User interaction The user can interactively manipulate the visualization in some way Real-time updates The user’s interactions propagate into the visualization with little to no lag Plot- and data-space manipulation The user can interactively explore different parts of the data set by doing actions which effectively amount to “subsetting” rows of the data (e.g. zooming, panning, and filtering) Linked views The visualization consists of connected or “linked” parts and the user’s interactions with one part propagate to the other parts (e.g. linked highlighting) Parametric updates The user can manipulate the parameters of some underlying mathematical model or algorithm (e.g. histogram bins, grand tour projections, etc…) 2.2.3 Complexity of interactive features The different definitions of interactivity are not just differences of opinion or taste - they also have a significant impact on implementation requirements. To start with a perhaps slightly over-exaggerated example, many programming languages come equipped with a read-evaluate-print loop (REPL) which can be used to interactively execute code from the command line. The user writes code, presses ENTER, and the language interpreter evaluates the code, returns any output, and waits for more input from the user. Now, if the language in question supports plotting, then, under the permissive “user interaction” definition, it could be argued that even the act of running code from a command line to produce new plots could be considered an “interactive data visualization system”, since the user’s interaction with the REPL produces changes to the visual output. And, hypothetically, if the user could type fast enough, they would see the updates appear almost instantly, satisfying the “real-time update” definition. Does this mean that every programming language which has a REPL and supports plotting automatically ships with an interactive data visualization system? I would argue that no: most people nowadays probably do not consider the command line to be an interactive data visualization system. But perhaps it has not always been this way. Several decades ago, the command line played a much bigger role as an interactive user interface (see e.g. Foley 1990; Howard and MacEachren 1995). Compared to waiting seconds or minutes for code to compile, a REPL is indeed a much more interactive experience. However, with the rise in processor speed and the proliferation of highly interactive graphical user interfaces (GUIs), users have come to expect visualizations that can be interacted with directly (Dimara and Perin 2019). As such, our perceptions of what is “interactive” are not constant but change over time; as technologies improve, we come to expect more direct and responsive user interfaces. Now, let’s set the somewhat exaggerated example of the REPL aside, and focus on what today would be considered more “typical” examples interactive data visualization systems. That is, systems in which the user can interact with the visualizations directly, by pressing keys or mouse buttons. Then, there still are considerable differences in what different features imply for implementation requirements. There are features which manipulate visual attributes of the plot only, independent of the data. These include, for example, changing the size, color, or opacity of points in a scatterplot. Features like this are usually fairly simple to implement because they do not affect the underlying data representation: a point displays the same data (as indicated by its xy-coordinates) no matter whether it is green or orange. Also, these graphical-only features typically do not require specialized data structures, and have low time- and space-complexity: for example, when interactively changing the opacity of points in a scatterplot, we only need to update one scalar value - the points’ opacity - and as such most of the user-experienced time will be spent re-rendering, rather than on any computation. In contrast, some interactive features require specialized data structures and complex algorithms, above and beyond those that are required for static plots. For instance, each time the user engages in interactive features such as filtering, linked highlighting, or parametric interaction, new summaries of the underlying data may need to be computed. When a user selects several points in a linked scatterplot, we first have to find the ids of all the corresponding cases, recompute the statistics underlying all other linked plots (such as counts/sums in barplots or histograms), train all of the relevant scales, and only then can we re-render the plot. Likewise, if we interactively manipulate a histogram’s binwidth, we need to recompute the number of cases in each bin each time the binwidth changes. To maintain the illusion of smooth, “continuous” interaction (Dimara and Perin 2019), these computations need to happen fast, and as such, computational efficiency becomes imperative. 2.2.4 Working definition Clearly, when building an interactive data visualization system, what we choose to call “interactive” has profound implications on our implementation. So how do we decide what we should consider “interactive”? There are essentially two modes of visualizing data. In statistics, the goal of data visualization is to facilitate rapid, accurate, and effective data exploration. However, it is not always the case that more complex visuals necessarily translate to better statistical insights. In static visualization, it is a well-established fact that plots can include sophisticated-looking and seemingly appealing features which do not promote the acquisition of statistical insights in any way (Cairo 2014, 2019; Gelman and Unwin 2013; Tufte 2001). Similarly, adding interactivity to a visualization does not always improve its statistical legibility (see e.g. Abukhodair et al. 2013; Franconeri et al. 2021). I propose to approach interactive features the same way we treat visual features in static visualization. Specifically, I propose the following working definition: To justify being called an “interactive data visualization”, the interactive features in a visualization should promote statistical understanding. If we accept this proposition, then there are several important consequences that follow. First, we must favour high-level, data-dependent, parametric interactions over the purely graphical ones. That is not to say that purely graphical interactive features are not useful. For example, in the presence of overplotting, manipulating size or alpha of objects can help us features (areas of high density) that would otherwise remain hidden. Likewise, zooming and panning, while often being counted among the more high-level features, require manipulation of existing axis limits only and can be done without reference to the original data. Still, I argue that the ability to see new summaries of the data is what makes some interactive data visualizations systems ultimately more powerful (and also more challenging to implement). The interactive features that enable this, such as filtering, linked highlighting, and parameter manipulation, go beyond aesthetics, and empower the users to explore the data dynamically, uncovering hidden patterns and relationships that may otherwise remain hidden. 2.2.5 List of common interactive features This section describes several common types of interactive features that facilitate interactive data exploration. 2.3 The highlighting problem As was discussed in the previous section, linked selection is one of the most highly ranked interactive features in interactive data visualization. It allows us to mark specific cases in one plot, and see the corresponding summary statistics in all the other plots. In this way, it allows us to quickly explore different dynamically-generated subsets of our data, within the context of the whole data set. 2.3.1 Linked selection and stacking When we engage in linked selection, we want to highlight parts of objects corresponding to the selected cases. Thus, fundamentally, we need to break each object into parts. We then need some way of representing those parts, in a way that visually preserves the hierarchical nature of the relationship (it needs to be clear that the parts are still parts of the whole object). In data visualization, there are three common methods for dealing with objects that have been split into parts: stacking, dodging, and layering. Let’s briefly illustrate these on the example of the barplot. We start with the whole bars and then break each bar into multiple segments based on the levels of another variable (such as selection status). Now, what we do with these segments will differ based on the technique we use. When stacking, we plot the bar segments vertically on top of each other. When dodging, we plot the bars side-by-side, as “clusters”. Finally, when layering, we plot bar segments in separate graphical layers and use partial transparency to mitigate overplotting. Figure 2.1: Examples of the three methods for dealing with objects that have been split into parts: stacking, dodging, and layering. Much has been written about the relative merits of stacking, dodging, and layering. For example, layering is only useful with few categories, as blending many colors can make it difficult to tell the categories apart (Franconeri et al. 2021; Wilke 2019). Further, in a landmark study, Cleveland and McGill (1984) showed that people tend to be less accurate when reading information from stacked bar charts as opposed to dodged bar charts. Specifically, since the lower y-axis coordinate of a stacked segment is pushed up by the cumulative height of the segments below, it becomes difficult to accurately compare segments’ length, both within and across bars (Cleveland and McGill 1984). Subsequent research has independently validated these findings and expanded upon them (see e.g. Heer and Bostock 2010; Thudt et al. 2016; Quadri and Rosen 2021). Due to this suboptimal statistical legibility, many data visualization researchers have urged caution about stacking (see e.g. Byron and Wattenberg 2008; Cairo 2014; Franconeri et al. 2021), and some have even discouraged its use altogether (Kosara 2016; Wilke 2019). However, much of this research has been done with on static visualizations. I argue that, in interactive data visualization, there are additional constraints that affect the calculus of which method is the best. First, if we want to support the ability to interactively manipulate the alpha channel and do linked selection with multiple (3+) selection groups, we can effectively eliminate layering as an option. Having two different sources of alpha channel (layering and user input) would make the interaction confusing and limit its range (e.g. if we use 75% alpha to do layering, then the effective range of values the user can manipulate is 0-75%). Further, with multiple selection groups, it would become hard to tell the selection status apart. It is much better to keep the two interactive features (selection and alpha manipulation) orthogonal and use a separate visual attribute (color and transparency) for each. Thus, the choice is between stacking and dodging. While, as was discussed above, dodging is the preferred choice in static data visualization, I propose that First, I propose that stacking is the superior method for displaying selection. Thanks to the four Gestalt principles of proximity, similarity, closure, and common region, in a stacked barplot, each bar presents itself as a single visual entity. In other words, since the stacked segments are placed right on top of each other, and share width and a closed border, they are perceived as part of a unified whole (the bar). This remains true throughout selection - no matter how the heights of the highlighted segments change, the height of the whole stacked bar remains constant, and so does the overall outline of the plot. To address the highlighting problem, we need to first discuss some key mathematical concepts. 2.4 Mathematical theory This section starts with some foundational mathematical concepts, such as functions, relations, and orders, and slowly builds up to more advanced topics such as categories, monoids, and functors. Readers familiar these concepts may feel free to skip ahead. However, even the fundamental concepts will be used throughout the thesis, so a refresher might be beneficial. The material covers topics from category theory and abstract algebra and follows primarily from Fong and Spivak (2019), Lawvere and Schanuel (2009), Baez (2023), Pinter (2010), and Milewski (2018). For an accessible introduction to the topic, interested readers are encouraged to consult these references, particularly Fong and Spivak (2019) and Lawvere and Schanuel (2009). 2.4.0.1 Note on past applications of category theory to data visualization There have been some applications of category theory to data visualization in the past. Specifically, several authors have used category theory to describe the more abstract aspects of data visualization, with some trying to lay down the theoretical foundations for what it means to “visualize” (Beckmann 1995) or for which visualizations can be considered well-formed perceptual representations of the data (Kindlmann and Scheidegger 2014). Handful of authors have also jointly discussed category theory and data visualization in the context of functional programming, introducing new functional programming libraries and domain-specific languages for data visualization (see e.g. Yorgey 2012; Petricek 2021; Smeltzer, Erwig, and Metoyer 2014; Smeltzer and Erwig 2018). The present thesis seeks to do neither. Instead, the goal is to use category theory to pin down the algebraic properties of the statistics underlying our visualizations, to determine which combinations of graphics, statistical summaries, and interactive features will work well together. Thus, the target is somewhere in the middle: provide a theoretical framework which will guide application. 2.4.1 Relations A relation is one of the simplest mathematical structures. Given two sets \\(X\\) and \\(Y\\), a relation \\(R\\) between \\(X\\) and \\(Y\\) is a subset of the Cartesian product of the two sets, \\(R \\subseteq X \\times Y\\). In other words, a relation can be thought of as the subset of pairs \\((x, y) \\in X \\times Y\\) for which the condition “\\(x\\) and \\(y\\) relate” holds. There are many different types of relations. One of the most fundamental relations is equality; in this case, “\\(x\\) and \\(y\\) relate” means that, for our purposes, \\(x\\) and \\(y\\) are the same, i.e. \\(x = y\\). Other examples of relations include the usual order relations \\(&lt;\\), \\(\\leq\\), \\(&gt;\\), \\(\\geq\\), or the divides operator \\(\\mid\\) (\\(x \\mid y\\) means “\\(x\\) divides \\(y\\) without remainder”). Finally, note that \\(X\\) and \\(Y\\) can be the same set, such that \\(R \\subseteq X \\times X\\). Since a relation is a subset of the product set \\(X \\times Y\\), we can visualize it as a matrix, with values of \\(X\\) as rows, values of \\(Y\\) as columns, and the related pairs \\((x, y)\\) marked out in some specific way. For example, here’s how we can display the order relation \\(\\leq\\) on the set \\(X = \\{ 1, 2, 3 \\}\\): Figure 2.2: A relation is a subset of the Cartesian product of two sets. The diagram shows the usual order relation \\(\\leq\\). We can see that 1 is less than or equal to every other element, 2 is less than or equal to 2 and 3, and 3 is less than or equal to 3 only. Note the symmetry between rows and columns - this is due to the fact that the same set (\\(X\\)) is display on both dimensions. As the examples above show, some relations \\(R\\) are marked with an infix symbol (such as \\(\\star\\)), such that, if \\(x\\) and \\(y\\) relate, i.e. \\((x, y) \\in R\\), then we write \\(x \\star_R y\\) or \\(x \\star y\\) (\\(R\\) implicit), for example, \\(x = y\\), \\(x \\leq y\\), and so on. In less common types of relations, \\(R\\) is also sometimes used as the infix symbol, e.g. \\(x R y\\). If two elements do not relate, i.e. \\((x, y) \\not \\in R\\), we typically do not write this out explicitly - the lack of relation is indicated by its absence. Relations can have properties. For example, some types of relations are reflexive, meaning that \\(x \\star x\\) for all \\(x \\in X\\) (every element relates to itself). This is the case for equivalence relations. In fact, we can define equivalence relations using just three properties: Definition 2.1 (Equivalence relation) A relation \\(\\sim\\) on \\(X\\) is called an equivalence relation if it is: Reflexive: \\(x \\sim x\\) for all \\(x \\in X\\) Symmetric: \\(x \\sim y\\) if and only if \\(y \\sim x\\) for all \\(x, y \\in X\\) Transitive: if \\(x \\sim y\\) and \\(y \\sim z\\), then \\(x \\sim z\\) Equivalence relations encode the notion that two things are the same, for our purposes. We can further use them to assign objects in \\(X\\) to equivalence classes, which divide \\(X\\) into groups of equivalent objects. That is, for some element \\(a \\in X\\), its corresponding equivalence class is: \\[[a] = \\{ x \\in X : x \\sim a \\}\\] While relations might seem simple, they are very versatile. The next few sections will discuss three important examples of relations: functions, partitions, and preorders. 2.4.2 Functions A function is a special kind of relation, one that encodes a mapping between two sets. More specifically, let \\(S\\) be the set of sources (also called the domain) and \\(T\\) be the set of possible targets (also called the codomain). Then, we can think of a function as a relation \\(F \\subseteq S \\times T\\) of valid source-target pairs \\((s, t)\\), such that for every \\(s \\in S\\) in there exists a unique \\(t \\in T\\) with \\((s, t) \\in F\\) (see Figure 2.3). In other words, every source relates to exactly one target: Figure 2.3: A function is a type of relation. Specifically, it is a subset of the Cartesian product of its domain (\\(S\\)) and codomain (\\(T\\)), such that element in the domain marks out exactly one element in the codomain (shown in red). The depicted function has the following characteristics: \\(F: \\{ 1, 2, 3 \\} \\to \\{ 1, 2, 3 \\}\\), such that \\(F(1) = 1\\), \\(F(2) = 1\\), and \\(F(3) = 2\\). One possible example of this function might be \\(f(x) = \\lfloor x / 2 \\rceil\\) (divide \\(x\\) by two and round to the nearest whole number). Note that, for any function, there must be exactly one red dot in each column (each source maps to one and only one target), however, there may be zero or many red dots in any row (some targets may not be reached from any source, and others may be reachable from many sources). We can classify functions based on how their domains and codomains relate to each other (see Figure 2.4). If every target in the function’s codomain has a path leading to it from some source, such that no target remains unreachable, then we call it a surjective or onto function. More formally: Definition 2.2 (Surjectivity) A function \\(f\\) is surjective if, for all \\(t \\in T\\), there exists a \\(s \\in S\\) such that \\(f(s) = t\\). Alternatively, if each source in the function’s domain leads to a unique target, such that no two sources map to the same target, then we call such a function injective or one-to-one. That is: Definition 2.3 (Injectivity) A function is injective if for all \\(s_1, s_2 \\in S\\), if \\(f(s_1) = t\\) and \\(f(s_2) = t\\), then \\(s_1 = s_2\\). Finally, if a function is both surjective and injective, meaning that every target can be reached from, and only from, a unique source, then we call such a function bijective or a bijection. Definition 2.4 (Bijectivity) A function is a bijection and only if it is both surjective and injective, which is also the case if and only if it is invertible. Figure 2.4: Types of functions. Left: in a surjective function, each target can be reached from some source. Middle: in an injective function, there is a unique source for each target. Right: in a bijection, each target can be reached from, and only from, a unique source. Bijections are special since they encode the idea of reversible transformations. Any bijective function \\(f\\) has an associated inverse function \\(f^{-1}\\) such that \\(f^{-1}(f(x)) = x\\) and \\(f(f^{-1}(y)) = y\\) for all \\(x\\) and \\(y\\) in the function’s domain and codomain, respectively. In other words, we can keep translating the value from the domain to codomain and back without losing any information. We will generalize this idea later when we discuss isomorphisms. As an example, suppose I have a group of friends \\(f \\in F\\) that each went to one city \\(c \\in C\\) in Europe during the holiday. I can construct a function \\(f: F \\to C\\) that sends each friend to his or her holiday destination. If every city in \\(C\\) was visited by at least one friend, then the function is surjective. If each friend went to a different destination, then the function is injective. If both are true - that is, if every city on our list was visited by exactly one friend - then the function is bijective. In that case, we could just as well use the names of cities \\(c \\in C\\) when we speak of friends \\(f \\in F\\) - instead of “Sam”, we could say “the person who went to Rome”, and it will be clear who are we talking about. An important property of functions is that they can be composed. Specifically, if the domain of one function matches the codomain of another, the functions can be composed by piping the output of the first function is used as the input of the second. Then we end up with a new, composite function: Definition 2.5 (Function composition) If we have two functions \\(f: X \\to Y\\) and \\(g: Y \\to Z\\), we can form a new function \\(h: X \\to Z\\) such that: \\[h(x) = g(f(x))\\] In terms of notation, we can omit the explicit reference to the variable \\(x\\) and write the composition in several different ways: \\(h = g \\circ f\\) (read: “apply \\(g\\) after \\(f\\)”) \\(h = gf\\) (same as above) \\(h = f ⨾ g\\) (read “apply \\(f\\) then \\(g\\)”) I will use the bracket notation (\\(h(x) = g(f(x))\\)) when explicitly referring to the variable, and the postfix/fat semicolon notation (\\(h = f ⨾ g\\)) otherwise. There are other things we can do with functions. For example, given a subset of sources, we can ask about the image - the set of targets we can reach from those sources: Definition 2.6 (Image) For some subset \\(S_i \\subseteq S\\), its image under \\(f\\) is defined as \\(f_!(S_i) = \\{ f(s) \\in T \\lvert s \\in S_i \\}\\). Likewise, given a subset of targets, we can ask about the pre-image - the set of sources that could have produced those targets. That is: Definition 2.7 (Pre-image) For some subset \\(T_i \\subseteq T\\), its pre-image under \\(f\\) is defined as \\(f^*(T_i) = \\{ s \\in S \\lvert f(s) \\in T_i \\}\\). An important fact to note is that, although the pre-image \\(f^*\\) is also sometimes called “inverse image”, it is not the inverse of the image \\(f_!\\) for most functions (unless they are bijections). That is, by applying the pre-image after image or vice versa, we cannot expect to come up with the same set as we started with. Specifically, if we have a non-injective function and apply the pre-image after the image, we may come up with more sources that we started with, \\(S_i \\subseteq f^*(f_!(S_i))\\) (equality if injective), and similarly, if we have a non-surjective function and apply the image after the pre-image, we might end up with fewer targets than we started with, \\(f_!(f^*(T_i)) \\subseteq T_i\\) (again, equality if surjective). As an example, suppose again I have the function \\(f\\) which maps each friend to a holiday destination. The image of that function, \\(f_!\\), maps a set of friends to the set of all cities that at least one of them went to, and similarly, the pre-image, \\(f^*\\), maps a set of cities to the set of friends that went to them. Now, suppose that Sam and Dominic went to Rome, and I ask: “who went to [the city that Sam went to]?” I will get both Sam and Dominic back, since: \\[f^*(f_!(\\{ Sam \\})) = f^*(\\{ Rome \\}) = \\{ Sam, Dominic \\}\\] That is, I will get back Sam and Dominic even though I had initially only asked about Sam. Similarly, if no friends had visited Paris and I ask: “what are the cities that [people who went to Paris or Rome] went to?” then I will get Rome only, since \\[f_!(f^*(\\{Paris, Rome \\})) = f_!(\\{ Sam, Dominic \\}) = \\{ Rome \\}\\] This weird relationship between the the image and the pre-image is due to the fact that the image is actually something called left adjoint (Baez 2023; Fong and Spivak 2019). Adjoints can be thought of as the “best approximate answer to a problem that has no solution” (no inverse, Baez 2023), and they come in pairs - a left and a right adjoint - with the left adjoint being more permissive or “liberal” and the right adjoint being more strict or “conservative” (Baez 2023). Proper treatment of adjoint is beyond the scope of this thesis, however. 2.4.3 Partitions One useful thing we can construct with functions (or equivalently, relations) are partitions. Partitions encode the idea of splitting elements of some some into distinct groups. Definition 2.8 (Function definition of a partition) Given some set \\(X\\), a set of part labels \\(P\\), and a surjective function \\(f: X \\to P\\), we can partition \\(A\\) by assigning every element \\(x \\in X\\) a part label \\(p \\in P\\), by simply applying the function: \\(f(x) = p\\). Above we used a function to define a partition, however, we can achieve the same with a relations, specifically equivalence classes. By taking any part label \\(p \\in P\\), we can recover the corresponding subset of \\(X\\) by pulling out its pre-image: \\(f^*(\\{p\\}) = X_p \\subseteq X\\). We can then define a partition without reference to \\(f\\): Definition 2.9 (Equivalence class definition of a partition) A partition of \\(A\\) consists of a set of part labels \\(P\\), such that, for all \\(p \\in P\\), there is a non-empty subset \\(A_p \\subseteq A\\) which forms an equivalence class on \\(A\\) and: \\[X = \\bigcup_{p \\in P} X_p \\qquad \\text{and} \\qquad \\text{if } p \\neq q, \\text{ then } X_p \\cap X_q = \\varnothing\\] I.e. the parts \\(X_p\\) jointly cover the entirety of \\(X\\) and parts cannot share any elements. We can rank partitions by their coarseness. That is, for any set \\(X\\), the coarsest partition is one with only one part label \\(P = \\{ 1 \\}\\), such that each element of \\(X\\) gets assigned \\(1\\) as label. Conversely, the finest partition is one where each element gets assigned its own unique part label, such that \\(\\lvert X \\lvert = \\lvert P \\lvert\\). Given two partitions, we can form a finer (or at least as fine) partition by taking their intersection, i.e. by taking the set of all unique pairs of labels that co-occur for any \\(x \\in X\\) as the new part labels. For example, suppose \\(X = \\{ 1, 2, 3 \\}\\) and partition 1 assigns part labels: \\[p_1(x) = \\begin{cases} a &amp; \\text{if } x = 1 \\text{ or } x = 2 \\\\ b &amp; \\text{if } x = 3 \\end{cases}\\] and partition 2 assigns part labels the following way: \\[ p_2(a) = \\begin{cases} s &amp; \\text{if } x = 1 \\\\ t &amp; \\text{if } x = 2 \\text{ or } x = 3 \\end{cases} \\] Then the intersection partition will have the following part labels \\(P_3 = \\{ (a, s), (a, t), (b, t) \\}\\) such that: \\[ p_3(a) = \\begin{cases} (a, s) &amp; \\text{if } x = 1 \\\\ (b, s) &amp; \\text{if } x = 2 \\\\ (b, t) &amp; \\text{if } x = 3 \\end{cases} \\] 2.4.4 Preorders Definition 2.10 (Preorder) A preorder is a set \\(X\\) equipped with a binary relation \\(\\leq\\) that conforms to two simple properties: Reflexivity: \\(x \\leq x\\) for all \\(x \\in X\\) Transitivity: if \\(x \\leq y\\) and \\(y \\leq z\\), then \\(x \\leq z\\), for all \\(x, y, z \\in X\\) Simply speaking, this means that between any two elements in \\(X\\), there either is a relation and the elements relate (one element is somehow “less than or equal” to the other), or the two elements do not relate. An example of a preorder is the family tree, with the underlying set being the set of family members: \\(X = \\{ \\textbf{daughter}, \\textbf{son}, \\textbf{mother}, \\textbf{father}, \\textbf{grandmother}, ... \\}\\) and the binary relation being ancestry or familial relation. Thus, for example, \\(\\textbf{daughter} \\leq \\textbf{father}\\), since the daughter is related to the father, and \\(\\textbf{father} \\leq \\textbf{father}\\), since a person is related to themselves. However, there is no relation (\\(\\leq\\)) between \\(\\textbf{father}\\) and \\(\\textbf{mother}\\) since they are not related. Finally, since \\(\\textbf{daughter} \\leq \\textbf{father}\\) and \\(\\textbf{father} \\leq \\textbf{grandmother}\\), then, by reflexivity, \\(\\textbf{daughter} \\leq \\textbf{grandmother}\\). We can further restrict preorders by imposing additional properties, such as: If \\(x \\leq y\\) and \\(y \\leq x\\), then \\(x = y\\) (anti-symmetry) Either \\(x \\leq y\\) or \\(y \\leq x\\) (comparability) If a preorder conforms to 3., we speak of a partially ordered set or poset. If it conforms to both 3. and 4., then it is a total order. 2.4.5 Monoids A monoid is a tuple \\((M, e, \\otimes)\\) consisting of: A set of objects \\(M\\) A neutral element \\(e\\) called the monoidal unit A binary function \\(\\otimes: M \\times M \\to M\\) called the monoidal product Such that: \\(m \\otimes e = e \\otimes m = m\\) for all \\(m \\in M\\) (unitality) \\(m_1 \\otimes (m_2 \\otimes m_3) = (m_1 \\otimes m_2) \\otimes m_3 = m_1 \\otimes m_2 \\otimes m_3\\) for all \\(m_1, m_2, m_3 \\in M\\) (associativity) In simple terms, monoids encapsulate the idea that the whole is exactly the “sum” of its parts (where “sum” can be replaced by the monoidal product). Specifically, we have some elements and a way to combine them, and when we combine the same elements, no matter where we put the brackets we always get the same result (i.e. something like “the order does not matter”, although that is not precisely right, more on that later). Finally, we have some neutral element that when combined with an element yields back the same element. For example, take summation on natural numbers, \\((\\mathbb{N}, 0, +)\\): \\[1 + 0 = 0 + 1 = 1 \\qquad \\text{(unitality)}\\] \\[1 + (2 + 3) = (1 + 2) + 3 = 1 + 2 + 3 \\qquad \\text{(associativity)}\\] Likewise, products of real numbers \\((\\mathbb{R}, 1, \\times)\\) are also a monoid, and so is multiplication of \\(n \\times n\\) square matrices \\((\\mathbf{M}_{n \\in \\mathbb{Z}}, \\mathbf{I}, \\cdot)\\), where \\(\\mathbf{I}\\) is the identity matrix and \\(\\cdot\\) stands for an infix operator that is usually omitted. As a counterexample, exponentiation does not meet the definition of a monoid, since it is not associative: \\(x^{(y^z)} \\neq (x^y)^z\\). There are more exotic forms of monoids as well. For example, the operation of appending a value to a vector and taking the Euclidean norm can also be recast as a monoid: \\[||(||(x, y)||_2, z)||_2 = \\sqrt{\\bigg(\\sqrt{(x^2 + y^2)}\\bigg)^2 + z^2} = \\sqrt{(x^2 + y^2) + z^2} = ||(x, y, z)||_2\\] We may want to impose further restrictions on monoids, for example: \\(m_1 \\otimes m_2 = m_2 \\otimes m_1\\) for all \\(m_1, m_2 \\in M\\) (commutativity) Both commutativity and associativity can both be viewed as a kind of “order does not matter” rule, however, they are fundamentally different. Let’s imagine our set of objects consists of three wires of different colours \\(\\{ \\textbf{red}, \\textbf{green}, \\textbf{blue} \\}\\) and the monoidal product consists of connecting wires. Let’s also imagine that the \\(\\textbf{red}\\) wire is connected to a power source and the \\(\\textbf{blue}\\) wire is connected to a lightbulb, and the blue wire amplifies the current from the power source such that it is enough to power the light bulb. To turn on the lightbulb, we need to connect \\(\\textbf{red} \\to \\textbf{green}\\) and \\(\\textbf{green} \\to \\textbf{blue}\\). The time order in which we connect the three wires does not matter: we can connect \\(\\textbf{green} \\to \\textbf{blue}\\) first and \\(\\textbf{red} \\to \\textbf{green}\\) second or vice versa, either way we get the same result (lightbulb turns on). However, the spatial order in which we connect the wires does matter: if we connect \\(\\textbf{red} \\to \\textbf{blue}\\), then the current will not be enough to power the lightbulb. Hence, the operation is associative (temporal order does not matter) but not commutative (spatial order does matter). If \\(M\\) is a preorder, another restriction we may want to impose is that the monoidal product is strictly increasing: \\(m_1 \\leq m_1 \\otimes m_2\\) and \\(m_2 \\leq m_1 \\otimes m_2\\) for all \\(m_1, m_2 \\in M\\) (monotonicity) This means that when we combine two things, we get back something that’s at least as big as the bigger of the two things. Summation of natural numbers \\((\\mathbb{N}, 0, +)\\) again works, but for example summation of integers \\((\\mathbb{Z}, 0, +)\\) or multiplication of reals \\((\\mathbb{R}, 1, \\times)\\) does not. 2.5 Theory of data visualization systems 2.5.1 Scales Every data visualization system needs some way of translating abstract data values into concrete graphical attributes such as position, size, or colour. Given the ubiquitous need for scales, one might expect them to be a “solved issue”, within the relevant literature. However, this is far from the truth. Specifically, the issues of scales and measurement are still being grappled with in the areas of mathematics and philosophy of science to this day (for a gentle yet thorough introduction, see Tal 2015). Scales are another area of data visualization in which there has been considerable debate, with many terms being overloaded and relating to concepts from different fields. This is due to the fact that the issue of how to compare, rank, and translate values has a long and complicated history. In particular, the issue of measurement has been hotly debated in the field of psychometrics and mathematical psychology, leading to the development of the theory of measurement (which has some overlap with, but is not the same as, measurement theory in mathematics). One paper that has been key to the debate around scales and measurement has been the seminal work of Stevens (1946). In this paper, Stevens defined a scale as a method of assigning numbers to values, allowing for various kinds of comparisons. Further, by considering transformations which preserve the comparisons, Stevens identified 4 types of scales: nominal, ordinal, interval, and ratio (see also Michell 1986; Velleman and Wilkinson 1993). Table 2.3: Types of scales identified by Stevens (1946) Scale Structure Comparison Valid transformations Nominal Equivalence relation Are \\(x\\) and \\(y\\) the same? \\(x&#39; = f(x)\\), where \\(f\\) is a bijection Ordinal Total order Is \\(x\\) is greater than \\(y\\)? \\(x&#39; = f(x)\\), where \\(f\\) is a monotonic bijection Interval Lebesque measure How far is \\(x\\) from \\(y\\)? \\(x&#39; = ax + b\\), for \\(a, b\\) real Ratio How many times is \\(x\\) greater than \\(y\\)? \\(x&#39; = ax\\), for \\(a\\) real Table 2.3 shows a loose reproduction of Table 1 from Stevens (1946). Note that the family of valid transformations gets smaller in each row, meaning that the scales carry more information (Velleman and Wilkinson 1993). Let’s discuss the scales individually. 2.5.1.0.1 Nominal scales Nominal scales correspond to equivalence relations. An equivalence relation is a binary relation \\(\\sim\\) on some set \\(X\\) which, for all \\(x, y, z \\in X\\), has the following properties: Reflexivity: \\(x \\sim x\\) Symmetry: \\(x \\sim y \\text{ if and only if } y \\sim x\\) Transitivity: \\(x \\sim y \\text{ and } y \\sim z \\text{ then } x \\sim z\\) Intuitively, we can think of the numbers on a nominal scale as “labels”. Thus, the only question which we can ask a nominal scale is whether two labels are the same or different. Examples of variables with nominal scale include variables of which we typically think of as categorical, such as color, species, or political party. It does not make sense to say “blue is more than green” or “cat is more than dog” without specifying some other axis of comparison. It does make sense, however, to say “Daisy and Molly are the same species of animal (cat)” or “these two glasses are of different colors”. For nominal scales, any permutation is a valid transformation (Stevens 1946). For example, if we use the numbers \\(\\{ 1, 2, 3 \\}\\) to represent the species \\(S = \\{ \\text{cat}, \\text{dog}, \\text{hamster} \\}\\), respectively, we can re-assign the numbers in any order we want and the properties of the scale are preserved. It is arguable whether any nominal quantities exist in and of themselves or whether they only ever exists as abstract social constructions over underlying continuous reality. Color is a discretization of the visible light spectrum (frequency of electromagnetic radiation), and the pre-Darwinian concept of a species is likewise an abstraction over continuously varying distribution of genes (although there have been some attempts to ground the definition of a discrete species in the theory of genetics, e.g. as a population of individuals which can produce viable offsprings, see Mayr 1999). Further, even many subjective concepts which are typically described as discrete such as emotions may be abstractions over underlying continuous phenomena (Barrett 2013). However, even if nominal quantities are entirely socially constructed, this does not mean they are arbitrary or useless. SEARLE 2.5.1.0.2 Ordinal scales Ordinal scales correspond to total orders. A total order is a relation \\(\\leq\\) on \\(X\\) which, for all \\(x, y, z \\in X\\), has the following properties: Reflexivity: \\(x \\leq x\\) Antisymmetry: \\(\\text{if } x \\leq y \\leq x \\text{ then } x = y\\) Transitivity: \\(x \\leq y \\text{ and } y \\leq z \\text{ then } x \\leq z\\) Totality or comparability or strong connectedness: \\(\\text{for all } x, y, \\text{ either } x \\leq y \\text{ or } y \\leq x\\) Examples of total orders include the usual ordering \\(\\leq\\) on natural numbers \\(\\mathbb{N}\\): \\(1 \\leq 2 \\leq 3 \\leq \\ldots\\) or the alphabetical order on letters: \\(A \\leq B \\leq C \\ldots \\leq Z\\). As total orders, ordinal scales allow us to rank quantities. A good example of an ordinal variable is placement in a race or competition. If Emma and Charlotte ran a marathon, and Emma placed 2nd and Charlotte 3rd, we can say that Charlotte ran finished the race earlier than Emma. However, we do not know whether she crossed the finish line 15 minutes or 2 hours earlier, or whether or not her average pace was less than half of that of Emma. Some authors have related ordinal scales to weak orders (see Michell 1986). Weak orders (also known as total preorders, see nLab 2024a) generalize total orders by allowing for ties (properties 2. and 4. above do not need hold). While this seems like a useful property, I opted to relate ordinal scales to total orders here instead, since there is currently ambuiguity in the way the term weak order is used in the literature (see e.g. nLab 2024b, 2024c; Abrams 2024), and, for practical data analysis, the distinction is fairly inconsequential. For instance, in the marathon example above, if Emma and Charlotte both placed second, after Lucy and before Lily, we could frame the outcome of the race as the following weak order on the set \\(M\\) of marathoners: \\(\\text{Lucy} \\leq \\text{Emma, Charlotte} \\leq \\text{Lily} \\leq ...\\). However, the underlying set of ranks \\(R \\subset \\mathbb{N}\\) still retains a total ordering: \\(1 \\leq 2 \\leq 3 \\leq \\ldots\\) and we can specify a surjective monotonically increasing function \\(r: M \\to R\\) which maps each marathoner to her rank. Clearly, we can map any weak order to a total order by applying a functor which enforces anti-symmetry in this way (Fong and Spivak 2019; nLab 2024a). The only transformations which are permissible for ordinal scales are those which preserve order, that is, monotonic increasing transformations (Stevens 1946; Michell 1986). For example, transforming our set of ranks \\(R\\) by taking the log or square root of each rank leaves the order relations between them unchanged. 2.5.1.0.3 Interval scales Interval scales. Interval scales allow us to calculate a distance between two points. However, they do not have a natural “zero point” or intercept. As such we cannot use them to determine the ratio between two quantities. Examples of interval scales include the calendar date and geographical position. It does not make sense to say that the year 1000 CE is “twice as much” as 500 CE, since the birth of Jesus Christ is (religious beliefs aside) an arbitrary zero point: we could set 0 CE at any other point in time, such as the founding of Athens or the release of Taylor Swift’s first album, and all of the properties we care about when tracking historical time would be preserved. Likewise, it does not make sense to say that 90° longitude is “three times” that of 30° longitude: the location of the prime meridian is also the product of arbitrary historical cirumstances. 2.5.1.0.3.1 Ratio scales Unlike interval scales, ratio scale have a well-defined natural zero point. For example, 2.5.1.0.4 Criticism of On the Theory of Scales of Measurement In the original paper, Stevens had also made the claim that the type of scale determined which statistical tests and summaries were “permissible” for the data. For example, according to Stevens, while mean is an appropriate summary of an interval scale (since expectation is linear), it would not be a permissible summary of ordinal data. This claim was later disputed by researchers knitr::opts_chunk$set( fig.align = &quot;center&quot;, dpi = 300 ) References Abbate, J. 1999. “Getting small: a short history of the personal computer.” Proc. IEEE 87 (9): 1695–98. https://doi.org/10.1109/5.784256. Abrams, Dave. 2024. “Total Weak Order Vs Total Order.” Mathematics Stack Exchange. https://math.stackexchange.com/questions/3793222/total-weak-order-vs-total-order. Abukhodair, Felwa A, Bernhard E Riecke, Halil I Erhan, and Chris D Shaw. 2013. “Does Interactive Animation Control Improve Exploratory Data Analysis of Animated Trend Visualization?” In Visualization and Data Analysis 2013, 8654:211–23. SPIE. Backus, John. 1978. “The History of Fortran i, II, and III.” ACM Sigplan Notices 13 (8): 165–80. Baez, John. 2023. “Applied Category Theory Course.” https://math.ucr.edu/home/baez/act_course. Barrett, Lisa Feldman. 2013. “Psychological Construction: The Darwinian Approach to the Science of Emotion.” Emotion Review 5 (4): 379–89. Batch, Andrea, and Niklas Elmqvist. 2017. “The Interactive Visualization Gap in Initial Exploratory Data Analysis.” IEEE Transactions on Visualization and Computer Graphics 24 (1): 278–87. Becker, Richard A, and William S Cleveland. 1987. “Brushing Scatterplots.” Technometrics 29 (2): 127–42. Beckmann, Peter E. 1995. “On the Problem of Visualizing Point Distributions in High Dimensional Spaces.” Computers &amp; Graphics 19 (4): 617–29. Bertin, Jacques. 1967. Sémiologie Graphique: Les diagrammes, les réseaux, les cartes. Gauthier-Villars. Blokt. 2020. “Flare \\(\\vert\\) Data Visualization for the Web.” Blokt - Privacy, Tech, Bitcoin, Blockchain &amp; Cryptocurrency. https://blokt.com/tool/prefuse-flare. Bostock, Mike. 2022. “D3.js - Data-Driven Documents.” https://d3js.org. Brodbeck, Dominique, Riccardo Mazza, and Denis Lalanne. 2009. “Interactive Visualization - A Survey.” In Human Machine Interaction, 27–46. Berlin, Germany: Springer. https://doi.org/10.1007/978-3-642-00437-7_2. Buja, Andreas, Dianne Cook, and Deborah F Swayne. 1996. “Interactive High-Dimensional Data Visualization.” Journal of Computational and Graphical Statistics 5 (1): 78–99. Byron, Lee, and Martin Wattenberg. 2008. “Stacked Graphs–Geometry &amp; Aesthetics.” IEEE Transactions on Visualization and Computer Graphics 14 (6): 1245–52. Cairo, Alberto. 2014. “Graphics Lies, Misleading Visuals: Reflections on the Challenges and Pitfalls of Evidence-Driven Visual Communication.” In New Challenges for Data Design, 103–16. Springer. ———. 2019. How Charts Lie: Getting Smarter about Visual Information. WW Norton &amp; Company. Cleveland, William S, and Robert McGill. 1984. “Graphical Perception: Theory, Experimentation, and Application to the Development of Graphical Methods.” Journal of the American Statistical Association 79 (387): 531–54. Dimara, Evanthia, and Charles Perin. 2019. “What Is Interaction for Data Visualization?” IEEE Transactions on Visualization and Computer Graphics 26 (1): 119–29. Dix, Alan, and Geoffrey Ellis. 1998. “Starting simple: adding value to static visualisation through simple interaction.” In AVI ’98: Proceedings of the working conference on Advanced visual interfaces, 124–34. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/948496.948514. Donoho, Andrew W, David L Donoho, and Miriam Gasko. 1988. “MacSpin: Dynamic Graphics on a Desktop Computer.” IEEE Computer Graphics and Applications 8 (4): 51–58. Elmqvist, Niklas, Andrew Vande Moere, Hans-Christian Jetter, Daniel Cernea, Harald Reiterer, and TJ Jankun-Kelly. 2011. “Fluid Interaction for Information Visualization.” Information Visualization 10 (4): 327–40. Evan You and the Vue Core Team. 2024. “Vue.js.” https://vuejs.org. Fisherkeller, Mary Anne, Jerome H Friedman, and John W Tukey. 1974. “An Interactive Multidimensional Data Display and Analysis System.” SLAC National Accelerator Lab., Menlo Park, CA (United States). Foley, James D. 1990. “Scientific Data Visualization Software: Trends and Directions.” The International Journal of Supercomputing Applications 4 (2): 154–57. Fong, Brendan, and David I Spivak. 2019. An Invitation to Applied Category Theory: Seven Sketches in Compositionality. Cambridge University Press. Fowlkes, EB. 1969. “User’s Manual for a System Fo Active Probability Plotting on Graphic-2.” Tech-Nical Memorandum, AT&amp;T Bell Labs, Murray Hill, NJ. Franconeri, Steven L, Lace M Padilla, Priti Shah, Jeffrey M Zacks, and Jessica Hullman. 2021. “The Science of Visual Data Communication: What Works.” Psychological Science in the Public Interest 22 (3): 110–61. Friendly, Michael. 2006. “A Brief History of Data Visualization.” In Handbook of Computational Statistics: Data Visualization, edited by C. Chen, W. Härdle, and A Unwin, III???–. Heidelberg: Springer-Verlag. Friendly, Michael, and Howard Wainer. 2021. A History of Data Visualization and Graphic Communication. Harvard University Press. Gelman, Andrew, and Antony Unwin. 2013. “Infovis and Statistical Graphics: Different Goals, Different Looks.” Journal of Computational and Graphical Statistics 22 (1): 2–28. Heer, Jeffrey, and Michael Bostock. 2010. “Crowdsourcing Graphical Perception: Using Mechanical Turk to Assess Visualization Design.” In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, 203–12. Heer, Jeffrey, Stuart K. Card, and James A. Landay. 2005. “prefuse: a toolkit for interactive information visualization.” In CHI ’05: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, 421–30. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/1054972.1055031. Highsoft. 2022. “Render Millions of Chart Points with the Boost Module Highcharts.” Highcharts. https://www.highcharts.com/blog/tutorials/highcharts-high-performance-boost-module. ———. 2024. “Highcharts - Interactive Charting Library for Developers.” Highcharts Blog \\(\\vert\\) Highcharts. https://www.highcharts.com. Holtz, Yan. 2022. “Interactive charts \\(\\vert\\) the R Graph Gallery.” https://r-graph-gallery.com/interactive-charts.html. Howard, David, and Alan M MacEachren. 1995. “Constructing and Evaluating an Interactive Interface for Visualizing Reliability.” In Congresso Da Associação Cartográfica Internacional–ICA, 17:321–29. Jankun-Kelly, TJ, Kwan-Liu Ma, and Michael Gertz. 2007. “A Model and Framework for Visualization Exploration.” IEEE Transactions on Visualization and Computer Graphics 13 (2): 357–69. Kehrer, Johannes, Roland N Boubela, Peter Filzmoser, and Harald Piringer. 2012. “A Generic Model for the Integration of Interactive Visualization and Statistical Computing Using r.” In 2012 IEEE Conference on Visual Analytics Science and Technology (VAST), 233–34. IEEE. Keim, Daniel A. 2002. “Information Visualization and Visual Data Mining.” IEEE Transactions on Visualization and Computer Graphics 8 (1): 1–8. Kelleher, Curran, and Haim Levkowitz. 2015. “Reactive Data Visualizations.” In Visualization and Data Analysis 2015, 9397:263–69. SPIE. Kindlmann, Gordon, and Carlos Scheidegger. 2014. “An Algebraic Process for Visualization Design.” IEEE Transactions on Visualization and Computer Graphics 20 (12): 2181–90. Kosara, Robert. 2016. “Stacked Bars Are the Worst.” https://eagereyes.org/blog/2016/stacked-bars-are-the-worst. Kruskal, J. B. 1965. “Multidimensional Scaling.” https://community.amstat.org/jointscsg-section/media/videos. Kunst, Joshua. 2022. Highcharter: A Wrapper for the ’Highcharts’ Library. Lawvere, F William, and Stephen H Schanuel. 2009. Conceptual Mathematics: A First Introduction to Categories. Cambridge University Press. Leman, Scotland C, Leanna House, Dipayan Maiti, Alex Endert, and Chris North. 2013. “Visual to Parametric Interaction (V2pi).” PloS One 8 (3): e50474. Mayr, Ernst. 1999. Systematics and the Origin of Species, from the Viewpoint of a Zoologist. Harvard University Press. MDN. 2024a. “EventTarget - Web APIs \\(\\vert\\) MDN.” MDN Web Docs. https://developer.mozilla.org/en-US/docs/Web/API/EventTarget. Meta. 2024. “React.” https://react.dev. Michell, Joel. 1986. “Measurement Scales and Statistics: A Clash of Paradigms.” Psychological Bulletin 100 (3): 398. Milewski, Bartosz. 2018. Category Theory for Programmers. Blurb. nLab. 2024a. “Posetal Reflection in nLab.” https://ncatlab.org/nlab/show/posetal+reflection. ———. 2024b. “Strict Weak Order in nLab.” https://ncatlab.org/nlab/show/strict+weak+order. ———. 2024c. “Weak Order in nLab.” https://ncatlab.org/nlab/show/weak+order. Petricek, Tomas. 2021. “Composable Data Visualizations.” Journal of Functional Programming 31: e13. Pike, William A, John Stasko, Remco Chang, and Theresa A O’connell. 2009. “The Science of Interaction.” Information Visualization 8 (4): 263–74. Pinter, Charles C. 2010. A Book of Abstract Algebra. Courier Corporation. Plotly Inc. 2022. “Part 4. Interactive Graphing and Crossfiltering \\(\\vert\\) Dash for Python Documentation \\(\\vert\\) Plotly.” https://dash.plotly.com/interactive-graphing. ———. 2024. “Webgl.” https://plotly.com/python/webgl-vs-svg. Quadri, Ghulam Jilani, and Paul Rosen. 2021. “A Survey of Perception-Based Visualization Studies by Task.” IEEE Transactions on Visualization and Computer Graphics. Rheingans, Penny. 2002. “Are We There yet? Exploring with Dynamic Visualization.” IEEE Computer Graphics and Applications 22 (1): 6–10. Rich Harris and the Svelte Core Team. 2024. “Svelte.” https://svelte.dev. Satyanarayan, Arvind, Ryan Russell, Jane Hoffswell, and Jeffrey Heer. 2015. “Reactive Vega: A Streaming Dataflow Architecture for Declarative Interactive Visualization.” IEEE Transactions on Visualization and Computer Graphics 22 (1): 659–68. Smeltzer, Karl, and Martin Erwig. 2018. “A Domain-Specific Language for Exploratory Data Visualization.” In Proceedings of the 17th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences, 1–13. Smeltzer, Karl, Martin Erwig, and Ronald Metoyer. 2014. “A Transformational Approach to Data Visualization.” In Proceedings of the 2014 International Conference on Generative Programming: Concepts and Experiences, 53–62. Stevens, Stanley Smith. 1946. “On the Theory of Scales of Measurement.” Science 103 (2684): 677–80. Swayne, Deborah F., Dianne Cook, and Andreas Buja. 1998. “XGobi: Interactive Dynamic Data Visualization in the X Window System.” J. Comput. Graph. Stat. 7 (1): 113–30. https://doi.org/10.1080/10618600.1998.10474764. Swayne, Deborah F., Duncan Temple Lang, Andreas Buja, and Dianne Cook. 2003. “GGobi: evolving from XGobi into an extensible framework for interactive data visualization.” Comput. Statist. Data Anal. 43 (4): 423–44. https://doi.org/10.1016/S0167-9473(02)00286-4. Tal, Eran. 2015. “Measurement in Science.” Theus, Martin. 2002. “Interactive Data Visualization using Mondrian.” J. Stat. Soft. 7 (November): 1–9. https://doi.org/10.18637/jss.v007.i11. Thudt, Alice, Jagoda Walny, Charles Perin, Fateme Rajabiyazdi, Lindsay MacDonald, Diane Vardeleon, Saul Greenberg, and Sheelagh Carpendale. 2016. “Assessing the Readability of Stacked Graphs.” In Proceedings of Graphics Interface Conference (GI). Tufte, Edward R. 2001. The Visual Display of Quantitative Information. Cheshire, Connecticut: Graphics Press LLC. Tukey, John W. 1962. “The Future of Data Analysis.” The Annals of Mathematical Statistics 33 (1): 1–67. Tukey, John W et al. 1977. Exploratory Data Analysis. Vol. 2. Reading, MA. Unwin, Antony. 1999. “Requirements for interactive graphics software for exploratory data analysis.” Comput. Statist. 14 (1): 7–22. https://doi.org/10.1007/PL00022706. Urbanek, Simon. 2011. “iPlots eXtreme: Next-Generation Interactive Graphics Design and Implementation of Modern Interactive Graphics.” Computational Statistics 26 (3): 381–93. Urbanek, Simon, and Martin Theus. 2003. “iPlots: High Interaction Graphics for r.” In Proceedings of the 3rd International Workshop on Distributed Statistical Computing. Citeseer. Vega Project. 2022. “Example Gallery: Interactive.” https://vega.github.io/vega-lite/examples/#interactive. ———. 2024a. “Brushing Scatter Plots Example.” Vega. https://vega.github.io/vega/examples/brushing-scatter-plots. ———. 2024b. “Vega and D3.” Vega. https://vega.github.io/vega/about/vega-and-d3. Velleman, Paul F, and Leland Wilkinson. 1993. “Nominal, Ordinal, Interval, and Ratio Typologies Are Misleading.” The American Statistician 47 (1): 65–72. Wikipedia. 2022. “Duck test - Wikipedia.” https://en.wikipedia.org/w/index.php?title=Duck_test&amp;oldid=1110781513. Wilke, Claus O. 2019. Fundamentals of Data Visualization: A Primer on Making Informative and Compelling Figures. O’Reilly Media. Wills, Graham. 2008. “Linked Data Views.” In Handbook of Data Visualization, 217–41. ch. II. 9. Springer Berlin/Heidelberg, Germany. Wirfs-Brock, Allen, and Brendan Eich. 2020. “JavaScript: the first 20 years.” Proc. ACM Program. Lang. 4 (HOPL): 1–189. https://doi.org/10.1145/3386327. Yi, Ji Soo, Youn ah Kang, John Stasko, and Julie A Jacko. 2007. “Toward a Deeper Understanding of the Role of Interaction in Information Visualization.” IEEE Transactions on Visualization and Computer Graphics 13 (6): 1224–31. Yorgey, Brent A. 2012. “Monoids: Theme and Variations (Functional Pearl).” ACM SIGPLAN Notices 47 (12): 105–16. Young, Forrest W, Pedro M Valero-Mora, and Michael Friendly. 2011. Visual Statistics: Seeing Data with Dynamic Interactive Graphics. John Wiley &amp; Sons. "],["problem-set.html", "3 Problem Set 3.1 Data representation 3.2 Data transformation 3.3 Scaling", " 3 Problem Set Designing an interactive data visualization system presents a unique set of challenges which need to be addressed. Some of these have been already mentioned in the Introduction. This section discusses these inherent challenges in greater depth, and begins exploring avenues for possible solutions. 3.1 Data representation There is no data visualization without data. However, data can come to use in different shapes and sizes. 3.2 Data transformation When visualizing data, it is rarely the case that we can just draw the raw data as is. Often, the quantities underlying a specific plot type are instead summaries or aggregates of some kind. Take for example a typical barplot. To draw a barplot, we first need to divide the data into disjoint parts, each corresponding to one bar, and then summarize each part by some metric, usually either the number of cases (count) or the sum of some continuous variable. Similarly, in a histogram, we first need to divide the data into bins and then summarize them (typically by count). Thus, there are two fundamental operations involved in every visualization: splitting the data into parts and computing the summaries on these parts. 3.2.1 Partitioning the data 3.2.1.1 Leave nothing out One simple principle is that a faithful visualization should not arbitrarily leave some parts of the data out. 3.2.1.2 Disjoint parts The essence of data visualization is comparison. A seemingly trivial point is that, when making relative judgements, we often compare entities which are distinct or independent rather than ones which overlap. It may be useful to dwell on this a bit. In data visualization, a fact that is so common that it is often overlooked is that the geometric objects we draw typically tend to represent disjoint subsets of the data (although there are counter-examples, see e.g. Alsallakh et al. 2014). Take, for example, the following barplot representing vote share of the top 3 parties in the 2023 New Zealand general election (Electoral Commission New Zealand 2023): Each bar represents a unique set of voters and so the parts of the data represents by the bars are disjoint. Technically speaking, there is no physical law forcing us to do things this way. We could just as easily represent the same underlying data the following way: However, this way of representing data is quite unnatural. Part of it has to do with visualization goals. When visualizing election data, we are typically interested in the relative number of votes each party received. The second barplot makes this difficult. Specifically, in the second barplot, since the bar for National represents a subset of the bar for National OR Labour, it becomes difficult to compare absolute counts (Cleveland 1985). Further, the second barplot needlessly duplicates information; the number of votes the Blue party received is counted twice, once in the left bar and again in the right. This goes against the general principle of representing our data in the most parsimonious way (Tufte 2001). One might object that the real question we may want to answer is the proportion of the total number of votes that the parties received. However, even here, a better representation is available: By stacking the bars on top of each other, we can easily compare proportion of the total number of votes while retaining a parsimonious representation of our data. A more challenging situation arises when there are multiple attributes of the data which can be simultaneously present or absent for each case. For example, in 2020, a joint referendum was held in New Zealand on the question of legalization of euthanasia and cannabis. The two issues were simultaneously included on the same ballot. The legalization of euthanasia was accepted by the voters, with 65.1% of votes supporting the decision, whereas the legalization of cannabis was rejected, with 50.7% of voters rejecting the decision (Electoral Commission New Zealand 2020). The referendum data could be visualized in the following way: By definition, both bars include votes which were cast by the same voter (ignoring the votes where no clear indication was given, Electoral Commission New Zealand 2020). This type of display is fine for static data visualization, however, in interactive data visualization, it becomes problematic. Specifically, if we want to support linked selection, selecting an object in one plot should not highlight parts of other objects in the same plot. Thus, the fundamental model for interactive data visualization is that of a partition. In other words, plots should adhere to the common heuristic that each geometric object in the plot represents a disjoint part of the data set, and the objects should jointly cover the entirety of the data. (Wilkinson 2012) Stacked bar is a partitioned bar 3.3 Scaling Information needs to be encoded in visual attributes (Cleveland 1985) References Alsallakh, Bilal, Luana Micallef, Wolfgang Aigner, Helwig Hauser, Silvia Miksch, and Peter Rodgers. 2014. “Visualizing Sets and Set-Typed Data: State-of-the-Art and Future Challenges.” Eurographics Conference on Visualization (EuroVis). Cleveland, William S. 1985. The Elements of Graphing Data. Wadsworth Publ. Co. Electoral Commission New Zealand. 2020. “Official Referendum Results Released.” https://elections.nz/media-and-news/2020/official-referendum-results-released. ———. 2023. “E9 Statistics - Overall Results.” https://www.electionresults.govt.nz/electionresults_2023/index.html. Tufte, Edward R. 2001. The Visual Display of Quantitative Information. Cheshire, Connecticut: Graphics Press LLC. Wilkinson, Leland. 2012. The Grammar of Graphics. Springer. "],["high-level-design.html", "4 High-level design 4.1 Programming paradigm 4.2 Reactivity 4.3 Row-based or column-based 4.4 Rendering engine", " 4 High-level design “Designing is fundamentally about taking things apart […] in such a way that they can be put back together. Separating things into things that can be composed.” Rich Hickey (2013) This section contains a description of some of the high-level design decisions that went into making the system. The example code chunks in this section are written in both R and TypeScript. While I would prefer to use R for all code examples, due to its tight integration with RMarkdown, some of the concepts are much easier to explain in a language with static typing like TypeScript (particularly, type annotations and interfaces). However, since some examples can also be greatly enhanced by graphical output, I also wanted to use R. So, where graphical output is important, the code examples are written in R, and, where the code itself is the main focus, they are written in TypeScript. I hope this bilingualism is not too confusing. I have tried to use only the core features of each language to make the examples clear. 4.0.1 User profile When designing an interactive data visualization system, one important consideration is the profile of the average user. Among interactive data visualization systems, there can be significant differences in what the user is expected to know, across various implementations and research areas. For example, some general interactive data visualization systems make almost no assumptions about the user’s level of experience or motivation, whereas others assume a highly motivated “expert” user with a sufficient level of domain knowledge (Dimara and Perin 2019). The goal of the project was to design a fairly versatile system for general data exploration. As such, the system was designed in such a way to make it possible for someone with fairly modest data science skills to pick up and make decent use of its basic features. At the same time, I also wanted to provide some more advanced features that experienced users could make use of. [Give examples]. 4.1 Programming paradigm “Apex predator of Grug is complexity. Complexity bad. Say again: complexity very bad. You say now: complexity very, very bad. Given choice between complexity or one on one against T-rex, Grug take T-rex: at least grug see T-rex.” The Grug Brained Developer, Gross (2024) It is often the case that single programming task can often be solved in many different ways, each with its own set of trade-offs. These trade-offs affect factors like performance, readability, reliability, and maintainability. With regards to these trade-offs, one of the most fundamental decisions is the choice of a programming paradigm. Programming paradigms provide a set of high-level guidelines for thinking about and solving programming problems. They offer distinct approaches to issues such as data representation, code organization, and control flow. Underlying these are philosophical differences on topics such as data mutability, the fundamental unit of computation. These differences can produce programs of radically different shapes and styles. However, all programming paradigms ultimately share one fundamental concern: complexity (Booch et al. 2008). Every programming problem comes with some inherent level of complexity. Without careful thought and constant effort, software has the tendency to grow out of scope and becomes unmanageable (Moseley and Marks 2006). Thus, all programming paradigms provide answers to the fundamental problem of complexity, each in their own idiosyncratic way (Chambers 2014; Jordan et al. 2015; Moseley and Marks 2006; Van Roy et al. 2009). Most programming languages are geared towards one specific programming paradigm, and typically support only one or two to a reasonable capacity (Van Roy et al. 2009). Fortunately, this is not the case for either JavaScript/TypeScript or R, since both are multiparadigm programming languages (Chambers 2014; MDN 2024f). Both languages support object-oriented programming, via prototype inheritance in the case of JavaScript (MDN 2024b) and the S3, S4, and R6 systems in the case of R (Wickham 2019), and treat functions as first class citizens, allowing for functional programming style (Chambers 2014; MDN 2024d). Further, as C based languages, both also support classical imperative/procedural programming style, and also provide some utilities for reflective metaprogramming. The flexibility of JavaScript and R had allowed me to experiment with different programming paradigms while developing my interactive data visualization package. I have rewritten the JavaScript side of the package multiple times from scratch, testing out several different programming paradigms and styles in the process. Below, I provide a rough sketch of the paradigms I have sampled, as well as an account of my experience of using each paradigm and some thoughts on its suitability for designing interactive data visualization systems. 4.1.1 Imperative programming Imperative programming is one of the oldest and most classical programming paradigms. It conceptualizes the program as a sequence of discrete steps that manipulate some mutable state (Frame and Coffey 2014). In this way, it closely resembles the way computer programs get executed on the underlying hardware (barring some advanced techniques such as branch prediction and speculative execution, the CPU executes instructions sequentially, see e.g. Parihar 2015; Raghavan, Shachnai, and Yaniv 1998). 4.1.2 Functional programming 4.1.3 Object oriented programming Object oriented programming (OOP) is a widely used programming paradigm. It first appeared in the late 1950’s and early 1960’s, with languages like Simula and Smalltalk (Black 2013). It then grew to prominence in the 1980’s and 1990’s, eventually becoming an industry standard. While there are may different interpretations of OOP, there are several concepts which tend to be shared by most implementations. The core idea of OOP is that programs should be modeled as networks of objects: independent units of computation which bundle related data (properties, members) and code (methods, virtual functions) together (Booch et al. 2008). Objects are self-contained, and own hidden, private state; they expose only a limited public interface. To communicate with each other, objects send each other messages (Meyer 1997). In this way, they behave similarly to networks of biological cells (this was the inspiration for Alan Kay, one of the creators of Smalltalk, who also coined the term “object oriented,” Kay 1996). Each object is instantiated based on a class, a sort of a blueprint which provides the object with a type as well as implementation of its methods. Classes can be associated with each other in various ways. For example, a class may inherit methods and properties from a parent class (superclass), such that it can make use of the parent’s implementation. Alternatively, a class may be composed of other classes, by having their instances as properties. Here’s a basic example one might see in an introductory OOP text-book (for some reasons, animals are a popular metaphor in introductory OOP texts). We start with a base class Animal: // Animal.ts type Food = { nutrition: number }; export class Animal { // Class properties with default values x = 0; y = 0; nutrition = 100; // A basic method shared by all Animals move(dx: number, dy: number) { this.x += dx; this.y += dy; } // Default eat method eat(food: Food) { this.nutrition += food.nutrition; } } Next, we subclass the Animal class and create a Dog class: import { Animal } from &quot;./Animal.ts&quot; type Meat = { type: `meat`; nutrition: number }; class Dog extends Animal { // Overwrite superclass method - dogs only eat meat eat(food: Meat) { super.eat(food); // Call superclass method } // Dogs also makes a specific dog-like sound getSound() { return `Woof!`; } } const dog = new Dog(); // Dog inherits all of the methods and properties of Animal dog.move(-10, 10); dog.eat({ type: `meat`, nutrition: 10 }); // This is fine // dog.eat({ type: `vegatable`, nutrition: 10 }); // This would result in a compiler error console.log([`x`, `y`, `nutrition`].map(e =&gt; `${e}: ${dog[e]}`).join(`, `)) console.log(dog.getSound()); ## x: -10, y: 10, nutrition: 110 ## Woof! This is only a toy example of OOP code. In real applications, when writing OOP code, one may also rely on more advanced language features such as interfaces, abstract classes, and generics, however, a detailed discussion of these features is outside of the scope of the present thesis. Nevertheless, what is important to discuss are some fundamental theoretical ideas behind OOP. 4.1.3.1 Abstraction Abstraction is a fundamental concept in OOP. It refers to the idea that, after having built a system (class) out of components (values and other classes), one should be able to “forget” all of the messy internal details and only rely on the exposed public interface. For instance, in the example above, the Dog class inherits from the Animal class: we should be able to use the methods inherited from Animal (such as move) without having to remember how these methods were implemented on the Animal class. Ultimately, the goal of abstraction is to make it possible to reason about complex systems (Booch et al. 2008). By hiding away details, good abstractions can free us to think about higher-level concepts. Fundamentally, this also comes with a shift of perspective: instead of thinking about the data objects are composed of, we should only consider their behavior (Black 2013; Meyer 1997). 4.1.3.2 Encapsulation An idea closely related to abstraction is encapsulation. Encapsulation refers to the practice of controlling access to class internals. Specifically, to properly encapsulate its data, a class should expose only a limited number of properties/methods and the rest should be kept private (Booch et al. 2008). Note that this does not mean that the hidden parts of a class have to be entirely invisible to the user, merely that the user should not be able to access the hidden data and rely on it in their applications (Meyer 1997). The main goal of encapsulation is continuity. By encapsulating private properties, the developer is free to modify them without affecting the public interface (Booch et al. 2008; Meyer 1997). In other words, users can continue using the class the same way, even while the developer is actively modifying the internals. Encapsulation is also closely related to the ideas of the open-closed principle and small interfaces (Meyer 1997). The open-closed principles states that modules should be open to extension but closed to modification. The idea of small interfaces suggests that, when communicating via messages (method calls), objects should exchange as little information as possible (Meyer 1997). This is meant to prevent coupling. 4.1.3.3 Polymorphism Polymorphism refers to the idea that we should be able to swap out components of our system for other components that conform to the same public interface. 4.1.3.4 Inheritance “programming without inheritance is distinctly not object-oriented; that would merely be programming with abstract data types” (Booch et al. 2008). 4.1.3.5 Domain-driven design One final concept that is not uniquely object-oriented but has a strong tradition in OOP is domain-drive design. This refers to the idea that components of a program (classes) should model things in the business domain or the real world (Hadar 2013; Meyer 1997). The goal of this strategy is to make it easier to discover objects/classes and their relationships by exploiting the mind’s capacity for thinking about the natural world (Booch et al. 2008; Hadar 2013; Meyer 1997). 4.1.3.6 Criticism of OOP Another OOP principle that is not always adhered to is the idea of small interfaces (Meyer 1997). In practice, it is common for class methods to receive pointers to other objects as arguments. However, this results in effectively sending all of the information contained in the argument object(s), defeating the principle of small interfaces (Will 2016). Another issue is that, while elegant abstractions are undeniably powerful, it usually takes a long time to come up with them. Poor and/or complicated abstractions tend to appear first (Meyer 1997). The problem with OOP is that it tends to introduce abstraction early (essentially treating all components of a system as abstract data types, Van Eerd 2024). 4.1.4 Data oriented programming My second remark is that our intellectual powers are rather geared to master static relations and that our powers to visualize processes evolving in time are relatively poorly developed. For that reason we should do (as wise programmers aware of our limitations) our utmost to shorten the conceptual gap between the static program and the dynamic process, to make the correspondence between the program (spread out in text space) and the process (spread out in time) as trivial as possible. Edgar Dijkstra -Dijkstra (1968) Data-oriented programming (DOP) is a relatively new programming paradigm that has been gaining attention in certain programming communities over the recent years. Unfortunately, due to its novelty, there is some confusion in the terminology surrounding this paradigm. The term “DOP” is often used interchangeably with Data Oriented Design (DOD), which shares a number of similarities. However, there are also important differences: whereas DOP is inspired by the Clojure style of programming and focuses on high-level principles such as structure and organization of code (Hickey 2011, 2018; Sharvit 2022; Nicolai Parlog 2024), DOD originates in the world of video-game development and primarily concerns itself with low-level optimization details such as memory layout and CPU cache utilization (Acton 2014; Bayliss 2022; Kelley 2023; Nikolov 2018; Fabian 2018). Also, both paradigms share a connection to Value Oriented Design (VOD, Van Eerd 2024) which emphasizes values and value semantics and has its roots in the work of Alexander Stepanov (see e.g. Stepanov and McJones 2009; Stepanov 2013). Nevertheless, despite their differences, there is also a large common set of ideas that the above-mentioned paradigms share. Specifically, the high-level principles and guidelines they provide are remarkably similar, and there are even examples of explicit cross-polination (see e.g. Van Eerd 2024). For this reason, I have decided to use DOP as an umbrella term in this section and use it to refer to all three paradigms, unless specified otherwise. 4.1.4.1 Data first The main idea of DOP is a data-first perspective. Programs should be viewed as transformations of data, nothing less, nothing more (Acton 2014; Fabian 2018; Sharvit 2022). This view has several important consequences for design. First and foremost, programs should be partitioned into two classes of independent components: data and code (Fabian 2018). Data should be represented by generic data structures and code should live inside modules composed of stateless functions (Fabian 2018; Sharvit 2022). The main benefit of this approach is that, by keeping data and code separate, we can use and reason about both in isolation, without entanglement (Van Eerd 2024). More specifically, data becomes easy to pass around and serialize, and, since the code is composed of pure functions, it becomes easy to test and mock (Sharvit 2022). In this way, programs can be thought of as being split between what is and what does. 4.1.4.2 The data In the DOP view, data should be represented by plain data structures. These are any structures that can formed by combining generic components: primitives, arrays, and dictionaries (aka objects, maps, or structs, Hickey 2011, 2018; Sharvit 2022). A good example of such plain data is JSON. The data should be organized in a way that makes logical sense and avoids duplicating information. It may even be desirable that the data adheres to the relational model (Codd 1970; Moseley and Marks 2006; Fabian 2018). This does not mean that the data has to actually live inside a relational database, just that its shape should resemble that of normalized database tables, with columns represented by generic arrays (Fabian 2018). Since data represents just itself - data - there is no obligation for it to model the real world or any kind of abstract entity. This can bring significant performance benefits (Acton 2014; Fabian 2018). An example typically discussed in DOD is the Structure of Arrays (SoA) data structure (Acton 2014, 2019; Kelley 2023). Storing a list of records as a single record of homogeneous arrays can dramatically reduce memory footprint and improve cache line utilization, resulting in better performance (Acton 2014; Fabian 2018; Kelley 2023). Another example of alternative data representation that can lead to improved performance are Entity Component Systems in videogames (Härkönen 2019). Outside of performance, another benefit of plain data is that it allows us introduce abstraction gradually. When starting a new project, we should rely on generic data manipulation functions as much as possible (Fabian 2018; Sharvit 2022). Only once we have settled on a good abstraction should we reify it as code. 4.1.4.3 The code Separate calculating from doing in methods (Van Eerd 2024). It may seem that many of the DOP principles directly contradict many popular OOP principles, specifically encapsulation, inheritance, polymorphism, and domain driven design. However, many of these principles can either be reconciled with DOP, or DOP in fact provides better alternatives. Below, I go over these principles and provide code examples that further illustrate how DOP works. 4.1.4.3.1 Encapsulation When it comes to encapsulation in DOP, we have to differentiate between encapsulating data and encapsulating code. Encapsulating code is easy in DOP - we can simply not export certain functions from the code modules. We are then free to modify the signature of these functions without affecting the public interface (Fabian 2018). Encapsulating data may require a bit more work. Depending on the language, generic data structures may not have property access modifiers (although there does seem to be a trend in recent languages to support property access modifiers more generically, see e.g. Rust Foundation 2024; Zig Software Foundation 2024). For instance, in JavaScript, private properties can only be declared as part of a class declaration (MDN 2024d). However, in most languages, it is still possible to use other language features and metaprogramming to achieve data encapsulation - for example, in JavaScript, we can use the Proxy class to emulate private property access (see Appendix). Thus, encapsulation of data is certainly possible in DOP. However, a question still remains whether it is a good idea. While in OOP, encapsulation is generally seen as a net positive, in DOP it is thought to come with trade-offs. It does provide an additional layer of security, however, it also makes systems more complex and harder to debug (Fabian 2018; Sharvit 2022). And, even with full encapsulation, users may still come to rely on hidden features of the system (Fabian 2018). Ultimately, it is necessary to weigh the pros and cons of encapsulating data within the context of the specific use-case. Some languages also have features which allow for a weak form encapsulation which is compatible with DOP. In JavaScript, this can be implemented by using symbol keys for object properties (MDN 2024c). Symbols are builtin primitive in JavaScript and are guaranteed to be unique. If we assign a property to an object using an unexported symbol as the key, the user will still be able to inspect the object and see the property, however, they will not be able to access it without using reflection. This is actually in line with the data hiding concept as laid out by Meyer (1997). I actually found this form a weak encapsulation a good fit for plotscape. For example, here is how I implemented the Meta mixin which allows use to store arbitrary metadata on objects: // Meta.ts const METADATA = Symbol(&quot;metadata&quot;); type METADATA = typeof METADATA; export interface Meta&lt;T extends Record&lt;string, any&gt;&gt; { [METADATA]: T; } export namespace Meta { export function of&lt;T extends Object&gt;(object: T) { return { ...object, [METADATA]: {} }; } export function get&lt;T extends Meta&gt;(object: T, key: keyof T[METADATA]) { return object[METADATA][key]; } export function set&lt;T extends Meta, K extends keyof T[METADATA]&gt;( object: T, key: K, value: T[METADATA][K] ) { object[METADATA][key] = value; } } Now we can import the module and use it to add secret metadata to arbitrary data objects: import { Meta } from &quot;./Meta.ts&quot; interface User extends Meta&lt;{ id: number }&gt; { name: string; } const user: User = Meta.of({ name: &quot;Adam&quot; }); Meta.set(user, `id`, 1337); console.log(user) console.log(Meta.get(user, `id`)); ## { ## name: &quot;Adam&quot;, ## [Symbol(metadata)]: { ## id: 1337, ## }, ## } ## 1337 4.1.4.3.2 Inheritance In OOP, primary mechanisms for code reuse are inheritance and composition. In DOP, since data is generic and separate from behavior, we can call functions from arbitrary modules with any compatible data, and this makes code reuse trivial. For example, here’s a simplified version of the Reactive interface (Observer pattern) from plotscape: const LISTENERS = Symbol(`listeners`); // A unique symbol, to avoid namespace clashes type Dict = Record&lt;string, any&gt;; // Generic dictionary type type Callback = (data: Dict) =&gt; void; // Generic callback function type interface Reactive { [LISTENERS]: Record&lt;string, Callback[]&gt;; } namespace Reactive { export function of&lt;T extends Object&gt;(object: T): T &amp; Reactive { return { ...object, [LISTENERS]: {} }; } export function listen(object: Reactive, event: string, cb: Callback) { if (!object[LISTENERS][event]) object[LISTENERS][event] = []; object[LISTENERS][event].push(cb); } export function dispatch(object: Reactive, event: string, data: Dict) { for (const cb of object[LISTENERS][event] ?? []) cb(data); } } interface Dog extends Reactive { name: string } namespace Dog { export function of(name: string) { return Reactive.of({ name }) } } const dog = Dog.of(`Terry`) Reactive.listen(dog, `car goes by`, () =&gt; console.log(`Woof!`)) Reactive.dispatch(dog, `car goes by`) ## Woof! 4.2 Reactivity 4.3 Row-based or column-based 4.4 Rendering engine References Acton, Mike. 2014. “Data-Oriented Design and c++.” Luento. CppCon. https://www.youtube.com/watch?v=rX0ItVEVjHc. ———. 2019. “Building a Data-Oriented Future.” WeAreDevelopers. https://www.youtube.com/watch?v=u8B3j8rqYMw. Bayliss, Jessica D. 2022. “The Data-Oriented Design Process for Game Development.” Computer 55 (5): 31–38. Black, Andrew P. 2013. “Object-Oriented Programming: Some History, and Challenges for the Next Fifty Years.” Information and Computation 231: 3–20. Booch, Grady, Robert A Maksimchuk, Michael W Engle, Bobbi J Young, Jim Connallen, and Kelli A Houston. 2008. “Object-Oriented Analysis and Design with Applications.” ACM SIGSOFT Software Engineering Notes 33 (5): 29–29. Chambers, John M. 2014. “Object-Oriented Programming, Functional Programming and r.” Statistical Science 29 (2): 167–80. https://doi.org/10.1214/13-STS452. Codd, Edgar F. 1970. “A Relational Model of Data for Large Shared Data Banks.” Communications of the ACM 13 (6): 377–87. Dijkstra, Edsger W. 1968. “Letters to the Editor: Go to Statement Considered Harmful.” Communications of the ACM 11 (3): 147–48. Dimara, Evanthia, and Charles Perin. 2019. “What Is Interaction for Data Visualization?” IEEE Transactions on Visualization and Computer Graphics 26 (1): 119–29. Fabian, Richard. 2018. “Data-Oriented Design.” Framework 21: 1–7. Frame, Scott, and John W Coffey. 2014. “A Comparison of Functional and Imperative Programming Techniques for Mathematical Software Development.” Journal of Systemics, Cybernetics and Informatics 12 (2): 1–10. Gross, Carson. 2024. “The Grug Brained Developer.” https://grugbrain.dev. Hadar, Irit. 2013. “When Intuition and Logic Clash: The Case of the Object-Oriented Paradigm.” Science of Computer Programming 78 (9): 1407–26. Härkönen, Toni. 2019. “Advantages and Implementation of Entity-Component-Systems.” Hickey, Rich. 2011. “Simple Made Easy.” Strange Loop. https://www.youtube.com/watch?v=LKtk3HCgTa8. ———. 2013. “Design, Composition, and Performance.” ETE Conference. https://www.youtube.com/watch?v=QCwqnjxqfmY. ———. 2018. “Maybe Not.” Clojure Conj. https://www.youtube.com/watch?v=YR5WdGrpoug. Jordan, Howell, Goetz Botterweck, John Noll, Andrew Butterfield, and Rem Collier. 2015. “A Feature Model of Actor, Agent, Functional, Object, and Procedural Programming Languages.” Science of Computer Programming 98: 120–39. Kay, Alan C. 1996. “The Early History of Smalltalk.” In History of Programming Languages—II, 511–98. Kelley, Andew. 2023. “A Practical Guide to Applying Data Oriented Design (DoD).” Handmade Seattle. https://www.youtube.com/watch?v=IroPQ150F6c. ———. 2024b. “Classes - JavaScript \\(\\vert\\) MDN.” MDN Web Docs. https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Classes. ———. 2024c. “Symbol - JavaScript \\(\\vert\\) MDN.” MDN Web Docs. https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Symbol. ———. 2024d. “Functions - JavaScript \\(\\vert\\) MDN.” MDN Web Docs. https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Functions. ———. 2024f. “JavaScript Language Overview - JavaScript \\(\\vert\\) MDN.” MDN Web Docs. https://developer.mozilla.org/en-US/docs/Web/JavaScript/Language_overview. Meyer, Bertrand. 1997. Object-Oriented Software Construction. Vol. 2. Prentice hall Englewood Cliffs. Moseley, Ben, and Peter Marks. 2006. “Out of the Tar Pit.” Software Practice Advancement (SPA) 2006. Nicolai Parlog. 2024. “Data Oriented Programming in Java 21.” Devoxx. https://www.youtube.com/watch?v=8FRU_aGY4mY. Nikolov, Stoyan. 2018. “OOP Is Dead, Long Live Data-Oriented Design.” CppCon. https://www.youtube.com/watch?v=yy8jQgmhbAU&amp;t=2810s. Parihar, Raj. 2015. “Branch Prediction Techniques and Optimizations.” University of Rochester, NY, USA. Raghavan, P., H. Shachnai, and M. Yaniv. 1998. “Dynamic Schemes for Speculative Execution of Code.” In Proceedings. Sixth International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems (Cat. No.98TB100247), 24. IEEE. https://doi.org/10.1109/MASCOT.1998.693711. Rust Foundation. 2024. “Pub - Rust.” https://doc.rust-lang.org/std/keyword.pub.html. Sharvit, Yehonathan. 2022. Data-Oriented Programming: Reduce Software Complexity. Simon; Schuster. Stepanov, Alexander A. 2013. “Efficient Programming with Components.” A9. Youtube. https://www.youtube.com/playlist?list=PLHxtyCq_WDLXryyw91lahwdtpZsmo4BGD. Stepanov, Alexander A, and Paul McJones. 2009. Elements of Programming. Addison-Wesley Professional. Van Eerd, Tony. 2024. “Value Oriented Programming Part v - Return of the Values.” C++ Now. Youtube. https://www.youtube.com/watch?v=sc1guyo5Rso. Van Roy, Peter et al. 2009. “Programming Paradigms for Dummies: What Every Programmer Should Know.” New Computational Paradigms for Computer Music 104: 616–21. ———. 2019. Advanced r. Chapman; Hall/CRC. Will, Brian. 2016. “Object-Oriented Programming Is Bad.” Youtube. https://www.youtube.com/watch?v=QM1iUe6IofM. Zig Software Foundation. 2024. “Documentation - the Zig Programming Language.” https://ziglang.org/documentation/master. "],["system-description.html", "5 System description 5.1 Core requirements 5.2 Components", " 5 System description This section contains a description of the concrete system implementation (plotscape and plotscaper). 5.1 Core requirements The system needs to be able to: Split the raw data into a hierarchy of parts Compute summary statistics on the parts Further transform these summaries in a way that respects the hierarchy (e.g. stacking, normalizing by parent values) Translate these summaries into visual attributes such as x- and y-position, width, height, area,… Render geometric objects based on the visual attributes Do all of the above reactively, in response to user input 5.2 Components This section contains a detailed listing of the system’s components. 5.2.1 Indexable As discussed in Section 4.3, when it comes to representing data, the column-based model offer several advantages over the row-based model. In this model, data is stored in a dictionary of contiguous arrays (as in, for example, a CSV file). Thus, I chose to represent the fundamental unit of data as a column, a fixed-length array of values. However, at times, it may be useful to have additional flexibility. Specifically, it may be useful to to extend our definition of a “column” to non-array-like things, such as derived values and values repeated across all rows. This is where Indexable&lt;T&gt; comes in. An Indexable&lt;T&gt; represents a single “column” of data, and is just a union of three basic types: Indexable&lt;T&gt; = T | T[] | ((index: number) =&gt; T) In plain words, an Indexable&lt;T&gt; is one of three things: A variable of type T An array of Ts A callback that takes an index and returns a T. To extract a value from Indexable&lt;T&gt;, we rely on a generalized form of indexed access. The way how this indexed access works depends on the type of the indexable. First, if the indexable is an array, we subset it using the usual square bracket notation. Second, if the indexable is a non-array-like variable (but not a function), we always return it, regardless of the index (we can think of it as the value as being repeated across all rows). Third and finally, if the indexable is a callback, then we call it with the index and take the returned value. A uniform interface for this generalization of indexed access is provided by Getters. Altogether, this is similar to Leland Wilkinson’s idea of a variable function (2012). The main advantage of Indexable&lt;T&gt; is that, while the raw data will typically come in the form of arrays, there are many places further down the data visualization pipeline where constants and callbacks are useful. For example, in a typical barplot, the base of the y-axis is set to a constant value, typically zero. While we could hypothetically append an array filled with zeros to the rendering data, it is more convenient and memory efficient to instead use a constant (0), or a thunk (() =&gt; 0). As another example, often, if we have an array of several repeated values, it may be convenient to instead represent it as two arrays: a (short) underlying array of unique values or labels and an array of indices (similar to base R’s factor class). When we need the actual values, we can use a callback to subset the array of labels. 5.2.2 Getter A Getter&lt;T&gt; is simply a function which takes an index and returns a value of type T. To construct a Getter&lt;T&gt;, we take an Indexable&lt;T&gt; and dispatch on its underlying type. For illustration, here is a (slightly) simplified implementation: // Getter.ts export type Getter&lt;T&gt; = (index: number) =&gt; T; export namespace Getter { // Constructor export function of&lt;T&gt;(x: Indexable&lt;T&gt;): Getter&lt;T&gt; { if (typeof x === `function`) return x; else if (Array.isArray(x)) return (index: number) =&gt; x[index]; else return () =&gt; x } } we can then create and use Getters like so: import { Getter } from &quot;./Getter&quot; const getter1 = Getter.of([1, 2, 3]) const getter2 = Getter.of(99); const getter3 = Getter.of((index: number) =&gt; index - 1); console.log(getter1(0)); console.log(getter2(0)); console.log(getter3(0)); ## 1 ## 99 ## -1 Note that, by definition, every Getter&lt;T&gt; is also automatically an Indexable&lt;T&gt; (since it is a callback (index: number) =&gt; T). This means that we can create new getters out of other getters. The Getter namespace also includes several other utility functions. One example is Getter.constant which takes in a value T and returns a thunk which always returns T (i.e. () =&gt; T). This is useful, for example, when T is an array and we always want to return the whole array (not just a single element): import { Getter } from &quot;./Getter&quot; const getter4 = Getter.constant([`A`, `B`, `C`]) console.log(getter4(0)) console.log(getter4(1)) ## [ &quot;A&quot;, &quot;B&quot;, &quot;C&quot; ] ## [ &quot;A&quot;, &quot;B&quot;, &quot;C&quot; ] Another utility function is Getter.proxy, which takes a Getter and an array of indices, and returns a new Getter which proxies the access to the original values through the array of indices: import { Getter } from &quot;./Getter&quot; const proxyGetter = Getter.proxy([`A`, `B`, `C`], [2, 1, 1, 0, 0, 0]); console.log([0, 1, 2, 3, 4, 5].map(proxyGetter)) This function becomes particularly useful when implementing Factors. 5.2.3 Dataframe Another fundamental data structure is a Dataframe. A Dataframe is just a record of Indexable values: interface Dataframe { [key: string | symbol]: Indexable } In this way, a Dataframe is essentially just a SoA with a bit of extra flexibility. Specifically, while in typical SoA data structures, all properties are usually arrays, in Dataframe they are instances of the Indexable type, so they may also be constants or functions. For example, the following is a valid instance of a Dataframe: const data: Dataframe = { name: [`foo`, `bar`, `baz`], age: 99, canDrive: (index: number) =&gt; index &lt; 1 } The fact that the “columns” of a Dataframe can be constants and functions is useful, for example, when want every row to contain the same value (e.g. 0 for the base of a barplot), or when we want the value be lazily computed based on other values. This is also where the SoA representation offers a unique advantage: to achieve the same behavior in AoS layout, we would have to have a copy of the value or function pointer in every row. Dataframe should always contain at least one array and all arrays in a Dataframe should have the same length. This is because some operations are impossible if we do not know the length of the Dataframe (the number of rows). For example, when rendering a scatterplot, how do we decide how many points to draw if the x- and y-positions have length 19 and 20, or if they are both constants? Thus, at least of one the dataframe’s columns needs to have a fixed length (i.e. have an underlying array) and there should not be multiple different lengths. In the current version of the system, these fixed-length constraints are not enforced via a static check (such as during a constructor call), but are instead checked dynamically during runtime, whenever the integrity of a dataframe’s length becomes a key concern (using utility functions such as Dataframe.checkLength). This is the case, for example, when initializing a Scene or when rendering. I found the dynamic fixed-length checks to be the better option, for several reasons. First, they allow us to represent data as a plain JavaScript object (POJO) rather than having to instantiate a class. Second, due to JavaScript’s dynamic nature, this approach is also safer: if, during runtime, the user adds a property to a Dataframe which violates the fixed-length constraints, this approach will catch the error. Third, and finally, for any data sets with typical dimensionality (more rows than columns, \\(p &lt;&lt; n\\)), the tiny performance hit that may be incurred due to having to loop through the columns to find the length dynamically will be minuscule compared with the computational cost of looping through the data set’s rows and doing work such as rendering or computing statistics. For high-dimensional datasets (\\(p &gt;&gt; n\\)), we could always extend the system to memoize the length/number of rows on the Dataframe object (although then we may lose the security of the dynamic runtime checks). 5.2.4 Reactive By definition, an interactive data visualization system needs to be able to respond to user input and propagate this information wherever it needs to go. Reactive is a fundamental mixin that provides this utility. It is essentially just custom implementation of the Observer/EventEmitter pattern. Any object can be made Reactive by passing it into the Reactive constructor, and then calling it with functions from the Reactive namespace. Here is a simplified implementation: // Reactive.ts const LISTENERS = Symbol(`listeners`); // A symbol key to emulate private property type Callback = (data: Record&lt;string, unknown&gt;) =&gt; void; export interface Reactive { [LISTENERS]: Record&lt;string, Callback[]&gt;; } export namespace Reactive { export function of&lt;T extends Object&gt;(object: T): T &amp; Reactive { return { ...object, [LISTENERS]: {} }; } export function listen(object: Reactive, event: string, cb: Callback) { if (!object[LISTENERS][event]) object[LISTENERS][event] = []; object[LISTENERS][event].push(cb); } export function dispatch( object: Reactive, event: string, data: Record&lt;string, unknown&gt; = {} ) { if (!object[LISTENERS][event]) return; for (const cb of object[LISTENERS][event]) cb(data); } } We can use Reactive like so: import { Reactive } from &quot;./Reactive&quot; const dog = Reactive.of({ name: `Terry the Terrier` }) Reactive.listen(dog, `car goes by`, () =&gt; console.log(`Woof`)) Reactive.dispatch(dog, `car goes by`) ## Woof The actual Reactive implementation includes more features, such as the ability to propagate events, throttle them and set their priority (determining the order in which event callbacks execute), remove listeners, and fire only once multiple events have been dispatched. However, the underlying model is the same. 5.2.5 Factors When visualizing data, we often need to split our data into several parts. As was discussed in Introduction [ADD REFERENCE], these parts together forms a partition of the data, and there may be multiple partitions organized in a hierarchy, such that one or more parts in a child partition “add up” to a part in the parent partition. A Factor provides a way to represent such data partitions and the associated metadata. In this way, it is similar to base R’s factor S3 class, although there are some important differences which will be discussed below. Factor has the following interface: interface Factor&lt;T extends Dataframe&gt; extends Reactive { cardinality: number; indices: number[]; data: T parent?: Factor; } cardinality records the number of unique parts (indices) that form the partition represented by the Factor. For example, if the factor represents boolean partitioning of the data into two parts, the cardinality will be 2, if it represents partitioning into three parts, the cardinality will be 3, if it represents a partitionining into all countries in the world, the cardinality will be 195, and so on. indices represent the actual assignment of cases (rows of the data) to the parts. For example, the array of indices [1, 0, 1, 1, 2] represents the following partitioning: the second case (row) is assigned to the first part, the first, third, and fourth case are assigned to the second part, and the fifth case to the third part (keeping in mind JavaScript’s zero-based indexing). As was mentioned above, the number of unique values in indices has to match the factor’s cardinality, and the length of indices has to match the number of rows of the data set that the factor partitions. Technically, cardinality represents the same information as is contained in indices (the number of unique values). However, for some operations, it is useful to be able to access cardinality directly, in \\(O(1)\\) time, instead of having to loop through the entire array of indices (\\(O(n)\\) time). Such is the case, for example, when constructing product factors or when initializing arrays of summaries. A factor may have associated metadata stored in the data property. The data property is a Dataframe with one row for each part (i.e. the number of rows is equal to cardinality). Representing metadata as a dataframe represents a departure from base R’s factor class, which represents all metadata as a flat vector of levels. For instance: cut(1:10, breaks = c(0, 5, 10)) ## [1] (0,5] (0,5] (0,5] (0,5] (0,5] (5,10] (5,10] (5,10] (5,10] (5,10] ## Levels: (0,5] (5,10] With Factor, the same information would be represented as: const factor: Factor = { cardinality: 2, indices: [0, 0, 0, 0, 0, 1, 1, 1, 1, 1], data: { binMin: [0, 5], binMax: [5, 10], }, }; There are several advantages to storing Factor metadata in a Dataframe as opposed to a flat vector/array. First, when partitioning data, we often want to associate several pieces of metadata with each part. For example, if we cut or bin some numeric variable, like in the example above, we want to store both the lower and upper bound of each part’s bin. We could store both pieces of information as a single element (tuple) in an array/vector, the way that cut does it, however, this works well with only few pieces of metadata: once we start storing longer tuples, it becomes hard to tell what each tuple element represents. We could alleviate the problem by naming the tuple elements, but then we are essentially storing the metadata in an array of dictionaries, i.e. the AoS data structure. We can do better by storing the metadata in a table. Second, if we store metadata in a Dataframe, it is easier to combine it when we take a product of two or more factors. Since taking the product of multiple factors is a fundamental operation in an interactive data visualization system, underpinning operations such as linked brushing, it makes sense to use this representation. There are multiple types of factors which differ semantic meaning as well as the associated metadata. However, all are represented by the same underlying Factor data type. To construct these Factor subtypes, we use different constructor functions which are all exported by the Factor namespace. These will be discussed in the following subsections. 5.2.5.1 Bijection factors Factor.bijection is the first of two trivial factor constructors. It assigns each case its individual part, such that the cardinality of the resulting factor is simply equal to the number of cases in the data. The function signature of Factor.bijection is as follows: function bijection&lt;T extends Dataframe&gt;(n: number, data?: T): Factor&lt;T&gt; Notice that, when we create a bijection factor, we need to specify the length of the data n (the number of cases). This is used to create indices. Technically speaking, explicitly specifying n is not necessary: we could implement indices as a callback which takes an index and simply returns it back (an identity function). However, since the factors are primarily used to summarize data, and we need to know how many cases to summarize, we do need to store the length of the data somewhere. I found it easier to simply create a dummy array of indices, rather than defining a separate length property on Factor. There is little computational cost associated with this, since, by definition, the partitioning represented by a bijection factor does not change1. Factor.bijection can also have associated metadata. This is just a Dataframe of length n. On an abstract level, a bijection factor represents a terminal object in the Factor category (a category of partitions with intersections as morphism). If we take the product of a bijection factor and any other factor, the result will always be another bijection. Typical use case of Factor.bijection is the scatterplot. When constructing a scatterplot, we simply take two arrays of equal length (three, if representing size as well) and draw one point per each element of the arrays. Thus, the partitioning Factor has the same cardinality as the length of the arrays and the arrays represent the factor’s metadata. Another use case is the parallel coordinates plot. 5.2.5.2 Constant factors Constant factors, created with the Factor.mono constructor, are the second trivial factor subtype. They represent the dual of bijection factors, by assigning all cases of the data to a single part. Thus, the labeling function is a constant function (hence the name). The constructor signature is the same as for bijection factors: function mono&lt;T extends Dataframe&gt;(n: number, data?: T): Factor&lt;T&gt; The metadata associated with constant factors is, intuitively, a Dataframe of length 1, i.e. a dictionary of arrays with one element. On an abstract level, Factor.mono represents an initial object in the Factor category. That is, if we take the product of a constant factor and any other factor, the result will simply be the other factor (with the constant factor’s metadata repeated in all of the other factor’s partitions’ metadata). The use cases for constant factors are a bit more obscure. One example is the spinogram. In a spinogram, the upper x-axis limit represents the cumulative sum (count) of all cases in the data. A convenient way to model this is by a constant factor. Another potential use case for constant factors might be plots with a single geometric object, such as a radar plot2. 5.2.5.3 String factors When we have array of values which can be coerced to strings (using the .toString() method), we can easily turn this into a factor by treating two values as the same level if their string representations match. This is the basis of string factors constructed by Factor.from: type Stringable = { toString(): string }; function from(array: Stringable[]): Factor&lt;{ labels: string[] }&gt; The metadata associated with Factor.from is the array of labels, which are simply all of the unique values produced by applying the toString() method to each element in array. 5.2.5.4 Binned factors When we have an array of numeric values, we can turn this into a factor by assigning the values to (typically equal-sized) bins, as in a histogram. This is what Factor.bin is for. It has the following constructor signature: function bin( array: number[], options: { breaks?: number[]; nBins?: number; width?: number; anchor?: number; }, ): Factor&lt;{ binMin: number[]; binMax: number[] }&gt;; Notice that the constructor comes with a list of options which control how the bins are created. If breaks are provided, the constructor uses those as the bins directly. Otherwise it uses nBins, width, and anchor to determine the breaks, in decreasing order of importance. The metadata that Factor.bin produces are the limits of each bin (binMin and binMax). 5.2.5.5 Product factors A fundamental operation that we need to be able to do with factors is to take their Cartesian product. That is, when we have two factors of same length, each with its corresponding array of indices, we need to be able to take the indices, combine them element-wise, and create a new factor that will have as its cardinality the number of unique pairwise combinations of indices. For example, take two factors represented by the following data (omitting the data property for conciseness): { cardinality: 2, indices: [0, 0, 1, 0, 1, 1] }; { cardinality: 3, indices: [0, 1, 2, 0, 1, 2] }; If we take their product, we should end up with the following factor: { cardinality: 4, indices: [0, 1, 3, 0, 2, 3] }; Notice that the cardinality of the resulting factor (\\(4\\)) is greater than that of either of the two constituent factors (\\(2\\), \\(3\\)) but less than than the combined product of their cardinalities (\\(2 \\cdot 3 = 6\\)). This will generally be the case: if the first factor has cardinality \\(a\\) and the second cardinality \\(b\\), the product of the two factors will have cardinality \\(c\\), such that: \\(c \\geq a\\) and \\(c \\geq a\\) (equality only if one or both of the factors are constant or there is only one unique element-wise index combination) \\(c \\leq a \\cdot b\\) (equality only if all element-wise index combinations are unique) That is, the product factor will have will be at least as many parts (but most likely more) as either of its constituent, and at most as many parts as the product of the numbers of the constituents’ parts (but most likely fewer). This is a simple fact based on the logic of partitions. 5.2.5.5.1 Computing product indices A subtle challenge is how to actually compute indices of a product factor. One naive idea might be to simply sum the constituent factors’ indices element-wise. However, this approach does not work: the sum of two different pairs of indices may produce the same value. For instance, in a product of two factors with cardinality \\(4\\), there are three different ways to get indices which sum to \\(4\\): \\(1 + 3\\), \\(3 + 1\\), and \\(2 + 2\\). Additionally, this approach does not preserve order: intuitively, we would want the cases associated with lower values of the first factor’s indices to come first (have lower index). The way out of this predicament is to use the following formula (similar to one discussed in Wickham 2013) for computing product indices: \\[i_{\\text{product}} = \\max(a, b) \\cdot i_{\\text{factor 1}} + i_{\\text{factor 2}}\\] We multiply the first factor’s index by the greater of the two factors’ cardinalities and add the index of the second factor. Intuitively then, the first factor gets assigned a greater “weight” (represented by the cardinality), and taking the maximum of the two cardinalities ensures that unique pairs of indices always produce a unique product index (since, e.g. with factors of cardinalities 2 and 3, an index pair (0, 2) will produce a product index 2 and index pair (1, 0) will produce a product index 3). This method works well but there are two issues with it that need to be addressed. First, computing the indices does not tell us the resulting cardinality. Second, the computed indices are not dense. For example, multiplying computing the product indices using the factors above gets us the following: { cardinality: 2, indices: [0, 0, 1, 0, 1, 1] }; { cardinality: 3, indices: [0, 1, 2, 0, 1, 2] }; // Product { cardinality: ?, indices: [0, 1, 5, 0, 4, 5] } Notice that the cardinality is unknown (?) and there are gaps in the indices (for example, we are missing indices 2 and 3). The issue of cardinality is unavoidable - we cannot infer the cardinality of the product factor from the cardinalities of the constituent factors alone, because we do not know how many unique combinations of indices there will be. However, what we can do is keep track of how many unique product indices we encounter while computing them (using, for example, an array or a hash set), and then use that number after we have finished looping through the indices. The issue of the gaps in the indices is a bit more complicated. While looping through the indices, we do not know how many unique index combinations we will encounter. There are several options for dealing with this problem. First, hypothetically, we could implement the downstream parts of our system in such a way that the factor indices would only represent relative position, not absolute. I have actually tried this out but found it rather inconvenient. Second, in a language with first-class support for pointers, another option would be to store the product indices as pointers to a shorter underlying array of “dirty” indices (with gaps), and only clean up this dirty array of indices (by removing the gaps) once we compute the product factor. Unfortunately, in JavaScript, there is no way to represent pointers to primitive types, such as numbers, strings, or booleans. The third option, and the one I ended up going with, is to simply loop through the product indices again and clean up the gaps this way. That is, the concrete steps taken in my system are: Compute product indices, keeping track of unique ones Sort the unique product indices Loop through product indices again, replacing them by position in the unique indices array Steps 1. and 3. require looping through the array of same length as the data, i.e. an \\(O(n)\\) complexity. Step 2. has hypothetically \\(O(n \\cdot \\log n)\\) complexity (using the usual quicksort algorithm), however, the actual required work is likely to be much less, since, unless either of the two factors is a bijection or all of the product indices are unique (which is unlikely), the length of the array of unique indices will only be a fraction of the length of the data. Thus, the combined complexity of all three steps is likely to be \\(O(n)\\), with \\(O(n \\cdot \\log n)\\) worst case performance. Since the operation of computing product factors represents a “hot” code path (it is required, for example, every time linked selection happens), having to loop through an array of the length of the data twice is not ideal. However, in JavaScript, there is likely no better alternative. While we could hypothetically wrap indices in objects, and store the pointers to those, the memory and data access overhead of this approach would most likely result in performance characteristic many times worse. One of the data types JavaScript engines such as V8 are best optimized to handle are packed arrays of small integers (called “smi’s,” V8 Core Team 2017), and so, heuristically, it makes sense to keep the indices in this format. Additionally, compared to the overhead of other operations (such as rendering), the cost of looping through an array of small integers twice is likely to be miniscule. Personally, this is what I have seen while profiling the system. 5.2.6 Reducers 5.2.6.1 Motivation When we visualize, we draw summary statistics computed on different parts of the data. For example, when drawing a typical barplot, we split the data based on the levels of some categorical variable, count up the number of cases in each part, and then draw bars of corresponding heights. In the preceding section, I discussed the component used to represent partitioning of the data into parts: factors. Now it is time to discuss the process of actually computing the statistical summaries representing these parts. Interestingly, while all visualizations rely on this process in one way or another, designing a generic pipeline for doing this presents some surprising challenges. One such important challenge is displaying coherent visualization. This topic has been discussed in the introduction, in Section [ADD REFERENCE]. Briefly, in order to be a valid representation of the data, an interactive data visualization system should compute and render statistics in such a way that the resulting visualization has correct algebraic properties. As was discussed previously, monoids and groups present a framework for ensuring this, and as such should serve as the backbone of our system. Another challenge is the hierarchical nature of graphics. In interactive data visualization, it is often desirable to be able to convert one specific plot type into a different representation. For example, a typical barplot represents counts along the y-axis. This is useful for comparing absolute counts, however, it is less useful for comparing proportions. As such, some interactive data visualization systems offer the feature of turning a barplot into a spineplot, a normalized version of the plot where the counts are instead presented along the x-axis, in a stacked order, and the y-axis represents proportion of counts, see Figure 5.1. ## ## Attaching package: &#39;dplyr&#39; ## The following object is masked from &#39;package:kableExtra&#39;: ## ## group_rows ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union Figure 5.1: While barplot and spineplot represent the same underlying summaries, each makes it easier to see different aspects of our data. Barplot (left) makes it easier to compare absolute counts, whereas spineplot (right) makes it easier to compare proportions. Notice that the spineplot makes it much easier to see that the proportions of blue cases in categories B and C are exactly the same. However, despite the fact that both barplots and spineplots represent the same underlying summaries (counts), turning one into the other is not always a trivial exercise. For example, in ggplot2, it is easy to create a barplot using simple declarative syntax, however, there is no such simple recipe for spineplots - creating the right plot in Figure 5.1 took over 10 lines of external data wrangling code (using standard dplyr syntax). What is so complicated about spineplots? First of all, both the x- and y-axes represent the same variable: counts. However, the way the variable is used is different: Along the x-axis, we stack counts within the levels of a single factor Along the y-axis, we stack counts within the levels of a product of two factors and normalize them by the counts within the levels of the parent factor. In other words, the spineplot forces us to confront the fact that the summaries in our plots form a hierarchy. When we compute the summaries underlying a stacked barplot or spineplot, we are not merely computing a matrix of values where the rows and the columns have no underlying meaning - instead, we are implicitly saying that objects along the x-axis (whole bars) represent a coarser level of partitioning compared with the objects (stacked segments) along the y-axis. The only difference between a barplot and spineplot is, in the barplot, we can get away with treating the two factors as if they were independent (had the same “weight”), whereas in the spineplot this is no longer possible. This is why in declarative data visualization systems such as ggplot2, certain types of plots like spineplots are difficult to express. In these systems, the data is implicitly partitioned as a “flat” product of the factor variables. This representation is convenient (e.g. for defining aesthetics via a single call to aes()) but makes it impossible to express hierarchical structures such as the one encoded in spineplot. Thus, ideally, our system should make it easy specify a pipeline where we compute monoidal summaries of our data within a hierarchy of partitions represented by one or more factor variables, apply some transformations to these summaries (possibly across the levels of the hierarchy), and finally map the summaries to some visual attributes. 5.2.7 Scales To visualize data, we need to be able to translate values from the space of the data to the space of the graphical device (computer screen). In most data visualization systems, this is done by specialized components called scales or coordinate systems (see e.g. Murrell 2005; Wickham 2016; Wilkinson 2012). Scales serve as a bridge between what we have (data) and what we see (visual attributes), allowing us to cross from one domain to the other. There exists is a fair research on the theoretical properties of scales and how they relate to the mechanisms of visual perception (see e.g. Krzywinski 2013; Michell 1986; Wilkinson 2012; Stevens 1946). However, when it comes to applying this knowledge and implementing scales in concrete data visualization systems, a lot less information is available. And, even when such information is available, it is it is often quite high-level of abstract (for some rare counter-examples, see e.g. Murrell 2005; Ziemkiewicz and Kosara 2009). Thus, the following section is based largely on how scales have been implemented in existing data visualization codebases, such as the ggplot2 R package (Wickham 2016) or d3-scale module of D3 (Observable 2024; also used by Vega Satyanarayan et al. 2015), as well as on personal insights gained while implementing the package. 5.2.7.1 Overview From a high-level perspective, a scale is just a function \\(s: D \\to V\\) which translates values of the data \\(d \\in D\\) to values of some visual attribute \\(v \\in V\\), such as the x- and y-position, length, area, radius, or color (Wilkinson 2012). This function may or may not be invertible, such that, at times, each value of the visual attribute may be identified with a unique data value (but this is not always the case). One of the most common and typical cases is a scale where both \\(D\\) and \\(V\\) are subsets of the real numbers: \\[s: [d_{min}, d_{max}] \\to [v_{min}, v_{max}] \\qquad d_{min}, d_{max}, v_{min}, v_{max} \\in \\mathbb{R}\\] For example, suppose our data takes values in the range from 1 to 10 and we want to plot it along the x-axis, within a 800 pixels wide plotting region. Then, our scale is simply: \\[s_x: [1, 10] \\to [0, 800]\\] Now, there is an infinite number of functions that fit this signature. However, one particularly nice and simple candidate is the following function: Definition 5.1 (Simple linear mapping) \\[s(d) = v_{max} + \\frac{d - d_{min}}{d_{max} - d_{min}} \\cdot (v_{max} - v_{min})\\] if we substitute the concrete values into the formula, this becomes: \\[s_x(d) = 0 + \\frac{d - 1}{10 - 1} \\cdot (800 - 0) = [(d - 1) / 9] \\cdot 800\\] The function acts on the data in the following way: \\(s_x(1) = (1 - 1) / 9 \\cdot 800 = 0\\) \\(s_x(10) = (10 - 1) / 9 \\cdot 800 = 800\\) \\(s_x(d) \\in (0, 800)\\) for any \\(d \\in (1, 10)\\) That is, the function maps the data value 1 to pixel 0 (left border of the plotting region), value 10 to to pixel 800 (right border of the plotting region), and any value in between 1 and 10 inside the interval 0 to 800, proportionally to where in the data range it is located. It is relatively simple to translate the formula in 5.1 to code: // simpleScale.ts export function simpleScale( d: number, dmin: number, dmax: number, vmin: number, vmax: number, ): number { return vmin + ((d - dmin) / (dmax - dmin)) * (vmax - vmin); } And indeed, this function works the way we would expect: import { simpleScale } from &quot;./simpleScale.ts&quot; console.log(simpleScale(1, 1, 10, 0, 800)) console.log(simpleScale(5.5, 1, 10, 0, 800)) console.log(simpleScale(10, 1, 10, 0, 800)) ## 0 ## 400 ## 800 5.2.7.2 Limits of modeling scales as simple functions Simple scale functions like the one above can work fine for basic data visualization systems. However, once we begin adding more features, this design becomes prohibitive. Consider, for example, what happens if we want to: Expand the scale limits Scale discrete data Apply non-linear transformations Pan, zoom, reverse, reorder, or otherwise modify the scale interactively Let’s take the first point as a motivating example. Consider what happens to data points at the limits of the data range under the simple linear mapping: x &lt;- 1:10 y &lt;- rnorm(10, 0, 5) col &lt;- ifelse(1:10 %in% c(1, 10), &quot;indianred&quot;, &quot;grey80&quot;) plot(x, y, col = col, cex = 3, xaxs = &quot;i&quot;) The plot above shows values scaled using the simple linear mapping along the x-axis, that is, \\(s: [1, 10] \\to [0, 800]\\) (effect of the xaxs = \"i\" argument). Notice that, since the position of the points representing the values 1 and 10 (highlighted in red) gets mapped to pixel values 0 and 800 (the left and right border of the plot), only half of each point is visible. This is quite undesirable - a fundamental principles of graphical integrity is that our graphics should not arbitrarily downplay or hide certain data points (Tufte 2001). The points at the axis limits are represented by only 1/2 of the area (or less, if at the limits of both axes), making them less salient, and this is especially pernicious since they are likely to be outliers. To address this problem, most data visualization systems automatically expand the range of the domain by some pre-specified percentage: # By default, the plot() function automatically expands the x- and y-axis # limits by approximately 4% on each end, see `xaxs` in ?graphics::par plot(x, y, col = col, cex = 3) We could achieve similar effect by modifying the simple linear mapping we have defined above and adding an additional argument: // simpleScale2.ts export function simpleScale2( d: number, dmin: number, dmax: number, vmin: number, vmax: number, exp: number, // Extra argument ): number { return ( vmin + (exp / 2 + ((d - dmin) / (dmax - dmin)) * (1 - exp)) * (vmax - vmin) ); } Now, if we set the exp argument to some positive value, the scaled values get mapped closer to the center of the plotting region. For example, setting exp to 0.2 moves each of the data limits 10% closer to the center of the plotting region: import { simpleScale2 } from &quot;./simpleScale2.ts&quot; console.log(simpleScale2(1, 1, 10, 0, 800, 0.2)); console.log(simpleScale2(5.5, 1, 10, 0, 800, 0.2)); console.log(simpleScale2(10, 1, 10, 0, 800, 0.2)); ## 80 ## 400 ## 720 However, notice that this argument is applied symmetrically. At times, we may want to apply a different margin to each end of the scale. We could solve this by adding two arguments instead of one, e.g. expLeft and expRight, however, at this point, the function signature starts to become unwieldy. If we have to call the function in multiple places, it may become difficult to remember what each individual argument represents. Further, note that by adding arguments, the logic inside the function’s body becomes denser and less readable. Finally, we may want to persist or modify some of the arguments during runtime (such as when panning or zooming). For all of these reasons, it may be a good idea to take a more structured approach and break the function down into several smaller components. 5.2.7.3 Solution: Two-component scales The linear mapping formula in 5.1 can guide us in decomposing the scale function into smaller, more manageable parts. Let’s look at it again: \\[s(d) = v_{min} + \\frac{d - d_{min}}{d_{max} - d_{min}} \\cdot (v_{max} - v_{min})\\] If we look closely, we may be able to see that there are two parts to the function: \\[s(d) = \\color{steelblue}{v_{min} +} \\color{indianred}{\\frac{\\color{black}{d} - d_{min}}{d_{max} - d_{min}}} \\color{steelblue}{\\cdot (v_{max} - v_{min})}\\] That is, the linear mapping is composed of two simpler functions: \\(\\color{indianred}{n(d) = (d - d_{min}) / (d_{max} - d_{min})}\\) takes a data value \\(d \\in D\\) and maps it to the interval \\([0, 1]\\) \\(\\color{steelblue}{u(p) = v_{min} + p \\cdot (v_{max} - v_{min})}\\) takes a value in \\([0, 1]\\) and maps it to a visual attribute value \\(v \\in V\\) This leads us to the following definition of a scale: Definition 5.2 (Scale as composition of two functions) A scale \\(s\\) can be created by composing: A normalize function \\(n: D \\to [0, 1]\\), mapping data to the interval \\([0, 1]\\) An unnormalize function \\(u: [0, 1] \\to V\\), mapping value in \\([0, 1]\\) to the visual attribute codomain Such that: \\[s(d) = u(n(d))\\] Note that the terms normalize and unnormalize are arbitrary, however, I think they make for useful labels. They represent 1-D equivalent of vector normalization, mapping a value in the domain to and from a unit interval \\([0, 1]\\). For the case of the linear mapping, we could rewrite this in code as follows: // LinearMap.ts export namespace LinearMap { export function normalize(d: number, dmin: number, dmax: number) { return (d - dmin) / (dmax - dmin); } export function unnormalize(p: number, vmin: number, vmax: number) { return vmin + p * (vmax - vmin); } } import { LinearMap } from &quot;./LinearMap.ts&quot; console.log(LinearMap.normalize(5.5, 1, 10)) console.log(LinearMap.unnormalize(0.5, 0, 800)) console.log(LinearMap.unnormalize(LinearMap.normalize(5.5, 1, 10), 0, 800)) ## 0.5 ## 400 ## 400 This two component system allows for a clean separation of concerns. Specifically, the normalize function only needs to know how to map the data values to \\([0, 1]\\). It does not need to be aware of where these normalized data values will be mapped to. Conversely, the unnormalize function only needs to understand how to translate values from \\([0, 1]\\) to the space of the visual attribute (such as x-axis position). 5.2.7.3.1 Beyond linear maps Another big advantage of the two-component scale system is that the functions \\(n\\) and \\(u\\) do not need to be a simple linear maps anymore. For example, suppose that our data \\(D\\) takes form of a set of discrete labels, such as \\(D = \\{ Prague, Vienna, Munich, Salzburg \\}\\). We can then replace \\(n\\) with a surjective function \\(n: D \\to [0, 1]\\) such that: \\[n(d) = \\begin{cases} 0.2 &amp; \\text{if } d = Munich \\\\ 0.4 &amp; \\text{if } d = Prague \\\\ 0.6 &amp; \\text{if } d = Salzburg \\\\ 0.8 &amp; \\text{if } d = Vienna \\end{cases}\\] In other words, \\(n\\) will place values of \\(D\\) at equidistant points along \\([0, 1]\\), ordered alphabetically. We can implement this function in code as follows: // PointMap.ts export namespace PointMap { export function normalize(d: string, dlabels: string[]) { return (dlabels.indexOf(d) + 1) / (dlabels.length + 1) } } Since the codomain of \\(n\\) is still \\([0, 1]\\), we can compose it with a simple linear mapping \\(u\\) just as easily as before: import { LinearMap } from &quot;./LinearMap.ts&quot; import { PointMap } from &quot;./PointMap.ts&quot; const labels = [&quot;Munich&quot;, &quot;Prague&quot;, &quot;Salzburg&quot;, &quot;Vienna&quot;]; console.log(PointMap.normalize(&quot;Munich&quot;, labels)); console.log(LinearMap.unnormalize(PointMap.normalize(&quot;Munich&quot;, labels), 0, 800)); console.log(LinearMap.unnormalize(PointMap.normalize(&quot;Prague&quot;, labels), 0, 800)); ## 0.2 ## 160 ## 320 5.2.7.3.2 Inverses Additionally, another property of the two-component scale system that can be useful is that, if both \\(n\\) and \\(u\\) are invertible, then so is \\(s\\). That is, we can easily obtain the inverse scale function by inverting the definition from 5.2: Definition 5.3 (Scale inverse) If a scale \\(s\\) is composed of invertible functions \\(n\\) and \\(u\\), then \\(s\\) is invertible: \\[s^{-1}(v) = n^{-1}(u^{-1}(v))\\] This is the case for the simple linear map: the normalize and unnormalize functions are actually inverses of each other: import { LinearMap } from &quot;./LinearMap.ts&quot; console.log(LinearMap.unnormalize(LinearMap.normalize(300, 0, 500), 0, 500)) ## 300 However, the inverse may not always exist. In practice, this is often the case when the domain of the data \\(D\\) is smaller than the codomain \\([0, 1]\\). Take, for example, the discrete point mapping. Since \\(D\\) is finite but \\([0, 1]\\) has infinitely many values, there will always be some values in \\([0, 1]\\) that no \\(d \\in D\\) maps to. For example, if \\(D = \\{ Munich, Prague, Salzburg, Vienna \\}\\) and \\(Munich\\) maps to 0.2, \\(Prague\\) maps to \\(0.4\\), and \\(Salzburg\\) maps to \\(0.8\\), then there are no cities which map to 0.9, 0.444, or 0.123456789. Conversely, if we get given those numeric values, then there is no obvious way to map them back to the cities. One thing we can do is to replace the inverse/unnormalize function with a weaker form of inverse, called retraction (Lawvere and Schanuel 2009). Specifically, if we have a normalize function \\(n: D \\to [0, 1]\\), then an unnormalize retraction \\(u^*\\) will have the property that: \\[u^*(n(d)) = d \\qquad \\forall d \\in D\\] However, the converse doesn’t necessarily hold: \\[\\neg \\big[ n(u^*(v)) = v \\qquad \\forall v \\in V \\big]\\] For example, for the discrete point mapping, a retraction may map a value in \\([0, 1]\\) to the closest data value \\(d \\in D\\): // PointMap.ts export namespace PointMap { export function normalize(d: string, dlabels: string[]) { return (dlabels.indexOf(d) + 1) / (dlabels.length + 1) } // Retraction - find the closest label export function unnormalize(p: number, dlabels: string[]) { const k = Math.round(p * (dlabels.length + 1) - 1) return dlabels[k] } } const labels = [&quot;Munich&quot;, &quot;Prague&quot;, &quot;Salzburg&quot;, &quot;Vienna&quot;]; const [prague, munich] = [&quot;Prague&quot;, &quot;Munich&quot;].map(x =&gt; PointMap.normalize(x, labels)) const midpoint = (prague + munich) / 2 // Helper function for stripping away floating point error const strip = (x: number) =&gt; parseFloat(x.toPrecision(12)) console.log(`Midpoint between Munich and Prague: `, strip(midpoint)) console.log(`unnormalize(0.2999): `, PointMap.unnormalize(0.2999, labels)) console.log(`unnormalize(3): `, PointMap.unnormalize(0.3, labels)) ## Midpoint between Munich and Prague: 0.3 ## unnormalize(0.2999): Munich ## unnormalize(3): Prague While inverses are always unique (Lawvere and Schanuel 2009; Fong and Spivak 2019), we may be able to come up with many different retractions for any given function. For example, with the discrete point map above, we could use the floor function instead of rounding and assign label to a value in \\([0, 1]\\) if it is less than the value of the normalized label (but more than the preceding labels). The non-uniqueness of retractions presents a bit of a dilemma. How do we decide which retraction to use? And, if a certain retractive implementation of unnormalize returns a value, how do we decide if it is the “correct one”? However, in practice, this is not much of a problem. While developing the package, I found that I’ve only ever had to use the unnormalize function with continuous data (LinearMap), and so the inverse was always well-defined. This is probably also why packages like ggplot2 and D3 can get by without this functionality. However, I still find it helpful to include the unnormalize function as a first class citizen (instead of it being relegated to some special case), both in terms of the mental model and also for debugging. 5.2.7.3.3 Some other remarks about the two-component scale system It is worth noting that there is nothing inherently special about the interval \\([0, 1]\\) as the intermediate domain: any finite subset of \\(\\mathbb{R}\\) would do. However, the interval \\([0, 1]\\) is convenient, both in terms of interpretation as well as for implementation, as we will see later. Finally, so far I have discussed scales as functions: the scale function, the normalize function, and unnormalize function. Framing scales as composition of functions leads to a nice correspondence between the mathematical definition and the code. However, in practice, it may be more convenient to implement the domain and codomain as objects or classes, as we will also see in the following section. The important point is that, no matter how the two components are represented, each is responsible for translating values from/to its domain and the interval \\([0, 1]\\). 5.2.7.4 Past implementations of scales Two-component scale systems such as the one sketched out above are fairly standard across data visualization libraries. For example, in D3 (Michael Bostock, Ogievetsky, and Heer 2011), scales are implemented in a functional style, such that the data domain and the visual attribute codomain are passed as tuples or arrays of values to a higher-order scale* function (such as scaleLinear, scalePoint, or scaleBand), which then returns a new function that can be used for scaling. The domain and codomain can also be changed at a later point, by using the scale*.domain and scale*.range methods respectively (JavaScript functions are objects and can have other functions/methods attached to them). For illustration, here is an example from the official documentation (Observable 2024): const x = d3.scaleLinear([10, 130], [0, 960]); x(20); // 80 const color = d3.scaleLinear([10, 100], [&quot;brown&quot;, &quot;steelblue&quot;]); color(20); // &quot;rgb(154, 52, 57)&quot; // The domain and codomain can be changed after initialization const y = d3.scaleLinear().domain([10, 130]); Internally, the scale* functions rely on other specialized functions to translate from its domain to the codomain (such as the normalize() and scale() functions for continuous and discrete/ordinal domains, respectively, and various interpolate() functions for codomains). Similarly, in ggplot2 (Wickham 2016), scales are built upon the Scale class, with each subtype implementing limits and palette properties. The limits property is a vector which corresponds to the data domain and the palette property is a function which corresponds roughly to the visual codomain (the x- and y-position behave slightly differently, due to being transformed via coordinate systems). Internally, the package uses the rescale function from the scales package (Wickham, Pedersen, and Seidel 2023) to map data values to \\([0, 1]\\) and then the palette function is responsible for mapping these normalized values to the visual attribute. For illustration, here’s the full definition of the map method on the ScaleContinuous class (I’ve added comments for clarity): map = function(self, x, limits = self$get_limits()) { # Limits are just a tuple, rescale maps x to [0, 1] x &lt;- self$rescale(self$oob(x, range = limits), limits) uniq &lt;- unique0(x) # Palette is a function which returns a vector of attribute values pal &lt;- self$palette(uniq) scaled &lt;- pal[match(x, uniq)] ifelse(!is.na(scaled), scaled, self$na.value) } 5.2.7.5 Proposed model of scales One feature that the models of scales that D3 and ggplot2 rely on is that they both treat the data domain and the visual attribute codomain as different types. In D3, fundamentally different functions are used to translate from \\(D \\to [0, 1]\\) and from \\([0, 1] \\to V\\), and in ggplot2, limits is a simple vector/tuple whereas palette is a function. While these approaches may have some benefits, such as perhaps offering greater flexibility, they also add additional complexity. Specifically, we have to use two different mental models: one when considering the domain and another when considering the codomain. Further, these models of scales only work in one direction: mapping values \\(D \\to V\\). For going the the other way, i.e. mapping \\(V \\to D\\), other specialized functions have to be used. I propose a model of scales which implements both the domain and the codomain as components of the same type: Expanse. Fundamentally, this makes it so that the only difference between the data domain and the visual attribute codomain is which property of the scale they are assigned to. Here is a (slightly) simplified version of the Scale interface: interface Scale&lt;D extends Expanse, V extends Expanse&gt; { domain: D codomain: V } D and V represent the data domain and the visual attribute codomain, respectively. The two fundamental functions connected to Scale are: function pushforward&lt;D, V&gt;(scale: Scale&lt;D, V&gt;, value: ValueOf&lt;D&gt;): ValueOf&lt;V&gt; function pullback&lt;D, V&gt;(scale: Scale&lt;D, V&gt;, value: ValueOf&lt;V&gt;): ValueOf&lt;D&gt; The pushforward function pushes values forward through the scale, first through its domain and then its codomain, and the pullback function pulls values back, first through its codomain and then through its domain. The ValueOf type helper just identifies the type associated with the expanse’s data (e.g. number for a continuous Expanse, string for a discrete Expanse, etc…). I’ve omitted the generic type parameter constraint (&lt;D extends Expanse, V extends Expanse&gt;) for brevity. Here is a simplified implementation of the two functions: namespace Scale { function pushforward&lt;D, V&gt;(scale: Scale&lt;D, V&gt;, value: ValueOf&lt;D&gt;): ValueOf&lt;V&gt; { const { domain, codomain } = scale; return Expanse.unnormalize(codomain, Expanse.normalize(domain, value)); } function pullback&lt;D, V&gt;(scale: Scale&lt;D, V&gt;, value: ValueOf&lt;V&gt;): ValueOf&lt;D&gt; { const { domain, codomain } = scale; return Expanse.unnormalize(domain, Expanse.normalize(codomain, value)) } } We can see that most of the work is done by the two Expanse components: we use domain to translates \\(D \\to [0, 1]\\) and codomain to translate \\([0, 1] \\to V\\). Scale only serves as plumbing, connecting the two together. I argue that this model provides several benefits. First of all, it makes the code easier to reason about. Since both the domain and codomain are of the same type, we only need to keep a single mental model in mind. Second, if domain and codomain provide inverse functions (unnormalize), we get the inverse scale function \\(V \\to D\\) for free (this is just the pullback function). However, before we discuss Expanse, there are also some important functionalities that we may want to implement on Scale directly. There are two main reasons for this. First, we may want these functionalities to apply generally, across the various Expanse subtypes. Second, by implementing them on Scale, we can keep the Expanse interface cleaner. These general functionalities will be the subject of the next few sections. 5.2.7.5.1 Zero and one Recall how in Section 5.2.7.2, we discussed the problem of expanding axis limits to display margins. Clearly, this is something that we also want to be able to do with our two-component scales. However, since we are designing an interactive data visualization system, we also want to be able to do more with axis limits: we want to be able to manipulate them dynamically during runtime, to implement features such as zooming and panning. In Section 5.2.7.2, we solved the problem of expanding axis limits by adding an additional argument to the simpleScale function. However, as was discussed previously, this approach does not scale well for more featureful implementations of scales. So how should we go about implementing dynamic axis limits in the context of the two-component scale system? Suppose we want to add margins to a scale where both the domain or codomain are continuous, such as the x-axis in a typical scatterplot. To implement margins, we could either expand the range of the data (the domain) or shrink the range of the visual attribute (the codomain). However, expanding the domain seems like a bad idea - this only works if the domain is continuous, and, clearly, we may want to add margins to discrete scales too, such the the x-axis of a barplot. Shrinking the range of the codomain could work (most visual attributes are continuous), however, we would need to implement some custom logic for when the plot gets resized. Also, by treating codomain differently than the codomain, we would be breaking away from our intention of representing both with the same generic Expanse type. So what can we do? As was foreshadowed at end of the previous section, we can put the functionality for expanding axis limits directly onto Scale. Specifically, notice that any values passing through a scale are first converted to the interval \\([0, 1]\\) and then back to the space of either the domain or codomain: \\[D \\to [0, 1] \\to V\\] If we re-normalize these normalized values in \\([0, 1]\\), we effectively expand or shrink axis limits without having to touch either the domain or codomain. To give a metaphor, if we imagine Scale as a pipe connecting the domain and codomain, we can manipulate axis limits by stretching or squeezing this pipe, allowing more or less water to flow through. To actually implement this, we can add two additional parameters to Scale, zero and one: interface Scale&lt;D extends Expanse, V extends Expanse&gt; { domain: D codomain: V props: { // A dictionary of properties zero: number one: number } } Now, we can use these two parameters to implement a new version of the pushforward function: function pushforward&lt;D, V&gt;(scale: Scale&lt;D, V&gt;, value: D): V { const { domain, codomain, props } = scale; const { zero, one } = props let normalized = Expanse.normalize(domain, value) normalized = zero + normalized * (one - zero) // Re-normalize return Expanse.unnormalize(codomain, normalized) } The new function’s body is a bit more dense, however, the only real change is in the line with the comment. When we re-normalize, we scale the normalized value by the (zero - one) range and increment it by zero. In other words, zero tells us the proportion of the codomain range that the minimum data value gets mapped to, and one tells us the proportion of the codomain range that the maximum data value gets mapped to. For example, suppose we set zero to 0.1 and one to 0.9. Then we have effectively implemented 10% margins on either side of the scale. If our scale has a \\([1, 10]\\) domain and \\([0, 800]\\) codomain, this will result in the following mapping: The “minimum” data value (1) gets mapped to 10% of the codomain range (80) Because zero + 0 * (one - zero) = zero = 0.1 The “maximum” data value (10) gets mapped to 90% of the codomain range (720) Because zero + 1 * (one - zero) = one = 0.9 Note the quotation marks around the words “minimum” and “maximum” - there is no requirement for the data to be continuous. For example, if the domain is a discrete Expanse which maps the string value \"A\" to zero, then the pushforward function will map \"A\" to 10% of the codomain range, just as it did in the case of the continuous domain. Likewise, the codomain could also be discrete - we could use this to implement scales for binned versions of visual attributes such as color or size. Thus, we can use zero and one to implement margins. However, there is much more we can do with these parameters. First, despite the names, zero and one can both take values less than zero and more than one. For example, suppose we increment both zero and one by the same amount, e.g. we set zero to 0.1 and one to 1.1. Then, the minimum data value will get mapped to the 10% of the codomain range, and the maximum data value will get mapped to 110% of the codomain range (which may lie outside the space representable by the graphic device). If the codomain represents the x-axis position, then we have shifted all of the geometric objects 10% to the right. We have effectively implemented panning: function move(scale: Scale, amount: number) { scale.props.zero += amount; scale.props.one += amount; } That’s it. We have implemented a functionality for panning which will work no matter if domain translates numbers, strings, or some other more complex data types. We can also stretch or shrink zero and one in opposite directions. For example, by setting zero to -0.5 and one to 1.5, then the minimum and maximum data values will get mapped 50% below and 50% above the limits of the codomain range, respectively, and the 25 and 75 data percentiles will get mapped to the minimum and maximum of the codomain range. If we apply this to the x- or y-axes, we’ve just implemented zooming. To be perfectly honest, there’s a bit more ceremony involved with zooming. Specifically, if we don’t start from zero = 0 and one = 1 (e.g. if our plot already has margins or if we’re zooming in multiple levels deep), then we need to re-normalize within these values. This took me a bit of time to nail down, however, it’s just (highschool) algebra: function rangeInverse(min: number, max: number) { return 1 / (max - min); } function invertRange(min: number, max: number) { const ri = rangeInverse(min, max); return [-min * ri, ri - min * ri]; } namespace Scale { export function expand( scale: { props: { zero: number; one: number } }, zero: number, one: number ) { const { zero: currentZero, one: currentOne } = scale.props; const currentRange = currentOne - currentZero; // Re-normalize within current values zero = (zero - currentZero) / currentRange; one = (one - currentZero) / currentRange; // Invert [zero, one] = invertRange(zero, one); scale.props.zero = zero; scale.props.one = one; } } const scale1 = { props: { zero: 0, one: 1 } }; // Mock of default scale const scale2 = { props: { zero: 0.1, one: 0.9 } }; // Mock of scale with margins // Zoom into the middle 50% of either scale Scale.expand(scale1, 0.25, 0.75); Scale.expand(scale2, 0.25, 0.75); console.log(`Zoomed in scale with no margins`, scale1.props); console.log(`Zoomed in scale with 10% margins`, scale2.props); ## Zoomed in scale with no margins { ## zero: -0.5, ## one: 1.5, ## } ## Zoomed in scale with 10% margins { ## zero: -0.3, ## one: 1.3, ## } As you can see, zooming into the middle 50% of a scale that already includes margins has a smaller effect on zero and one, since the margins have effectively expand the space we’re zooming into (i.e., a scale with margins is already zoomed out, in a way). 5.2.7.5.2 Direction In the same way we can think about expanding/shrinking axis limits in a way that is not coupled to any particular data representation or visual attribute, it may also be helpful to make direction a property of Scale rather than either of the Expanse components. We could do this by manipulating the zero and one properties. For example, by setting zero to 1 and one to 0, we could effectively reverse the direction of the scale. However, in practice, this would complicate our logic and make it harder for someone to interpret the Scale properties. It is a better idea to add an explicit direction parameter instead: interface Scale&lt;D extends Expanse, V extends Expanse&gt; { domain: D codomain: V props: { zero: number one: number direction: 1 | -1 // Extra parameter } } Like with zero and one, direction acts on the normalized values in \\([0, 1]\\). This means that we need to apply it in any transformations that use these values. For example, here’s an updated version of the move function: export function move(scale: Scale, amount: number) { let { direction, zero, one } = scale.props; zero += direction * amount; one += direction * amount; } Likewise, the pushforward, pullback, and expand functions also need to take direction into account. Either way, with this functionality in place, it becomes trivial to flip or reverse a scale: export function flip(scale: Scale) { scale.props.direction -= 1; } 5.2.7.5.3 Multipliers Finally, it may also be helpful to have the ability to shrink/expand the normalized values by some constant without having to modify properties of either the domain or codomain. Again, this could be done by using the zero and one properties, however, it’s better to define separate properties instead. Specifically, we can add two additional parameters: interface Scale&lt;D extends Expanse, V extends Expanse&gt; { domain: D codomain: V props: { zero: number one: number direction: 1 | -1 scale: number // Extra parameter mult: number // And another one } } The reason it is better to have two multiplier parameters instead of just one is that there are different reasons for why we may want to multiply values by a constant. First, we may want to multiply the values by some constant that remains fairly static throughout the lifetime of the program/visualization. That is the job of the scale parameter. Conversely, we may want to also dynamically manipulate the constant by which the values are multiplied. That is what mult is for. Having two multipliers makes it easier to reason about the scale’s behavior, as well as to apply changes such as restoring to defaults. A good example of this is the barplot. In a typical barplot, all bars share the same width, which is some fraction of the width of the entire plotting region. Clearly, this fraction needs to depend on the number of bars in the plot, such that, with \\(k\\) categories/bars, the bar width will be proportional to \\(k\\). However, we may also want to be able to make the bars wider/narrower interactively, e.g. by pressing the +\\- keys. Thus, the width of the bars is proportional to \\(c \\cdot k\\) where \\(k\\) is the static part of the constant (scale) and \\(c\\) is the dynamic part of the constant (mult). We apply the constant to the normalized value each time we push/pull a value through a scale: // This will be included in the body of pushforward(); see below for full example let normalized = Expanse.normalize(domain, value) normalized = normalized * scale * mult Finally, we could hypothetically extend this idea to an entire array of different multipliers, that we could reduce into a single constant each time we push a value through a scale. This could be useful in some circumstances, however, in my application, I found that having two parameters was enough to solve all of my scaling problems. Additionally, having an array of multipliers might make the scaling functions slightly less performant, if we have to reduce the array each time we pushforward/pullback, or it might make keeping track of the state of the Scale object slightly more complicated, if we roll these multipliers into one constant each time we update the array. We would also lose the semantic distinction that we have with scale and mult. This might be a perfectly fine trade-off if our scales require more multipliers, however, I did not find this to be the case in my implementation. 5.2.7.5.4 The Full Monty With all of the pieces in place, we can put together the full implementation of the pushforward function. It may be helpful to define two helper function for applying the Scale properties to a normalized value. First, the applyDirection function simply applies the direction property, such that applyDirection(x, 1) is simply the identity whereas applyDirection(x, -1) returns 1 - x (i.e. moving from one down): function applyDirection(x: number, direction: 1 | -1) { return 0.5 * (1 - direction) + direction * x; } console.log(applyDirection(0.75, 1)) console.log(applyDirection(0.75, -1)) console.log(applyDirection(1.25, -1)) ## 0.75 ## 0.25 ## -0.25 Second, we can define the applyPropsForward function which takes a normalized value and applies all of the Scale properties to it: type Props = { zero: number; one: number; direction: -1 | 1; scale: number; mult: number; }; function applyPropsForward(x: number, props: Props) { const { zero, one, direction, scale, mult } = props; x = x * scale * mult; x = zero + x * (one - zero); return applyDirection(x, direction); } Now we ready to define the full pushforward function. As one final note, we should probably be able to handle the case where the domain and codomain work on arrays of values rather than scalars (this can be helpful, for example, in the case of a parallel coordinates plot). As such, we can add an if block to check where the normalized value is an array and handle appropriately. In total: function pushforward&lt;T extends Expanse, U extends Expanse&gt;( scale: Scale&lt;T, U&gt;, value: Expanse.Value&lt;T&gt;, ): Expanse.Value&lt;U&gt; { const { domain, codomain, props } = scale; let normalized = Expanse.normalize(domain, value); if (Array.isArray(normalized)) { normalized = normalized.map((x) =&gt; applyPropsForward(x, props)); } else { normalized = applyPropsForward(normalized, props); } return Expanse.unnormalize(codomain, normalized); } This is the full definition of the pushforward function in plotscape as of 2024-11-06. The implementation for pullback function is very similar, with the only differences being that the order of the domain and codomain arguments reversed, and it uses the applyPropsBackward function, which is not too difficult to derive. 5.2.8 Expanses So far, we have discussed scales, and described them as a sort of bridge between two properties of type Expanse - the domain and the codomain. However, we have left the precise nature of the Expanse type vague. Now it is finally time to discuss Expanse and its various subtypes concretely. As mentioned previously, the job of the Expanse&lt;T&gt; is to translate values of type T (its domain) to and from the interval \\([0, 1]^n\\). This makes Expanse similar to the maps discussed in Section 5.2.7.3. The reason why the normalized interval is identified as \\([0, 1]^n\\) instead of the one-dimensional interval \\([0, 1]\\) is because, sometimes, we may want to map multi-dimensional values. For example, in the parallel-coordinates plot, we want to map values of several different variables to the y-axis. Typically, the dimensionality of the normalized values will be the same as that of T, however, we could imagine a situation where it might not be so, for example, we could imagine mapping 3-dimensional vectors to their (normalized) length. Most of the functionality is implemented by the specific subtypes of Expanse, however, there is also some shared behavior. The simplified interface of Expanse is: interface Expanse&lt;T&gt; { value: T; normalized: number | number[] } Here, the value and normalized properties are opaque types (used on type-level only), which simply indicate the domain type T and the dimensionality of the normalized values (number | number[]). Each namespace corresponding to a subtype of Expanse&lt;T&gt; exports two important functions: A normalize function \\(n: T \\to [0, 1]^n\\), mapping values from \\(T\\) to \\([0, 1]^n\\) An unnormalize function \\(u: [0, 1]^n \\to T\\), mapping values from \\([0, 1]^n\\) to \\(T\\) There are two other important methods that each Expanse subtype must export: train and breaks. The train function allows the expanse to train on new data (for example, after a histogram binwidth has been changed). The breaks function simply returns an array of breaks of type T. Thus, each subtype of Expanse implements the following polymorphic methods: interface ExpanseMethods&lt;T&gt; { normalize(expanse: Expanse&lt;T&gt;, value: T): number | number[]; unnormalize(expanse: Expanse&lt;T&gt;, value: number | number[]): T; train(expanse: Expanse&lt;T&gt;, values: T[], options?: Record&lt;string, any&gt;): void; breaks(expanse: Expanse&lt;T&gt;, zero?: number, one?: number): T[] | number[]; } 5.2.8.1 Continuous expanses A continuous expanse is a generalization of the linear mapping discussed in Section 5.2.7.3. That is, it translates values to and from a continuous interval given (roughly) by \\([\\text{min}, \\text{max}]\\). Here is a simplified interface: interface ExpanseContinuous { min: number; max: number; offset: number; trans: (x: number) =&gt; number; inv: (x: number) =&gt; number; ratio: boolean; } The min and max properties are fairly self-explanatory - they denote the minimum and maximum of the data. The offset property allows us to move values by some constant, either before they have been normalized or after they have been unnormalized. This is useful, for example, when we want to ensure that the width of a spineplot bar is exactly 1 pixel less than the available space. The trans and inv properties allow us to perform non-linear transformations (they should, intuitively, be inverses of each other). By default, they are both set to the identity function ((x) =&gt; x). Finally, the ratio property is a simple boolean flag which indicates whether the expanse is part of a ratio scale. If this flag is set to true, then the min value of the expanse must always be zero and we cannot change it by, for example, training on new data. The normalize and unnormalize functions in the ExpanseContinuous namespace are generalizations of the linear map: // ExpanseContinuous.ts export namespace ExpanseContinuous { export function normalize(expanse: ExpanseContinuous, value: number) { const { min, max, offset, trans } = expanse; return (trans(value - offset) - trans(min)) / (trans(max) - trans(min)); } export function unnormalize(expanse: ExpanseContinuous, value: number) { const { min, max, offset, trans, inv } = expanse; return inv(trans(min) + value * (trans(max) - trans(min))) + offset; } } And these work as we would expect: import { ExpanseContinuous } from &quot;./ExpanseContinuous&quot; const identity = (x) =&gt; x; // I could have defined a proper constructor above but opted not to to save lines const exp = { min: 1, max: 16, offset: 0, trans: identity, inv: identity }; console.log(ExpanseContinuous.normalize(exp, 4)); exp.trans = Math.sqrt; // Technically, we should also set inverse to square console.log(ExpanseContinuous.normalize(exp, 4)); ## 0.2 ## 0.3333333333333333 Finally, the ExpanseContinuous namespace also export a train function, which goes through an array values and updates the min and max properties (max only if ratio is set to true), and a breaks function which returns a list of breaks, using an algorithm inspired by base R’s pretty function. 5.2.8.2 Point expanses A point expanse is the simplest type of discrete expanse. It simply places values at equidistant points along the \\([0, 1]\\) interval, based on an ordered array of labels. Here is a simplified interface: interface ExpansePoint { labels: string[]; order: number[]; } The labels array contains the all of the unique values that data take (strings). The order array is a simple array of indices which represent the order in which the labels get assigned to points in the \\([0, 1]\\) interval. The normalize and unnormalize functions in the ExpansePoint namespace simply use a label to find the corresponding point in \\([0, 1]\\) or the use a point to find the closest label, while respecting the order: // ExpansePoint.ts export namespace ExpansePoint { export function normalize(expanse: ExpansePoint, value: string) { const { labels, order } = expanse; const index = order[labels.indexOf(value)]; if (index === -1) return index; return index / (labels.length - 1); } export function unnormalize(expanse: ExpansePoint, index: number) { const { labels, order } = expanse; index = Math.round(index * (labels.length - 1)); return labels[order[index]]; } } Again, these functions work as we would expect: import { ExpansePoint } from &quot;./ExpansePoint&quot; const cities = [&quot;Berlin&quot;, &quot;Prague&quot;, &quot;Vienna&quot;] const exp = { labels: cities, order: [0, 1, 2], }; console.log(cities.map(city =&gt; ExpansePoint.normalize(exp, city))); exp.order[0] = 1; // Swap the order of the first two values exp.order[1] = 0; console.log(cities.map(city =&gt; ExpansePoint.normalize(exp, city))); ## [ 0, 0.5, 1 ] ## [ 0.5, 0, 1 ] Like ExpanseContinuous, the ExpansePoint namespace also contains a train function, which loops through an array of labels and finds all of the unique values, as well as a breaks function, which simply returns ordered labels. Further, the namespace also contains a reorder function which mutates the order property based on an array of indices. 5.2.8.3 Band expanses While ExpansePoint places values at equidistant points along \\([0, 1]\\), ExpanseBand places values at the midpoints of corresponding bins or bands. These bands can have variable widths, which is useful, for example, when specifying the x-axis position in a barplot. The simplified interface of ExpanseBand is the following: export interface ExpanseBand { labels: string[]; order: number[]; weights: number[]; cumulativeWeights: number[]; } Like ExpansePoint, ExpanseBand has the labels and order properties, which work exactly the same way as before. However, additionally, it also has the weights and cumulativeWeights properties, which are numeric arrays that define the width of each band. weights record the width of each band, and cumulativeWeights record the cumulative sums of the weights, which are used in the normalize and unnormalize functions. Thus, each time we update weights, we need to also update cumulativeWeights as well. The normalize and unnormalize functions in the ExpanseBand namespace map labels to and from the midpoint of their corresponding bands: export namespace ExpanseBand { export function normalize(expanse: ExpanseBand, value: string) { const { labels } = expanse; const index = labels.indexOf(value); return getMidpoint(expanse, index); } export function unnormalize(expanse: ExpanseBand, value: number) { const { labels, order, cumulativeWeights } = expanse; const weight = value * last(cumulativeWeights); let index = 0; while (index &lt; cumulativeWeights.length - 1) { if (cumulativeWeights[index] &gt;= weight) break; index++; } return labels[order[index]]; } function getMidpoint(expanse: ExpanseBand, index: number) { const { order, cumulativeWeights } = expanse.props; index = order[index]; const lower = cumulativeWeights[index - 1] ?? 0; const upper = cumulativeWeights[index]; const max = last(cumulativeWeights); return (lower + upper) / 2 / max; } } Notice that, because of the cumulative nature of the bands, the logic in the functions’ bodies is a bit more complicated. First, to normalize a label, we need to first find the index of the label and then return the corresponding midpoint of the band, taking weights and order into account. Second, to unnormalize, we actually have to loop through the array of cumulativeWeights - there is no way to determine which bin a normalized value belongs to in \\(O(1)\\) time (as far as I am aware). This is not much of a problem since the ExpanseBand.unnormalize is not used anywhere in the system (all scales implemented thus far use only ExpanseBand.normalize), however, it is important to be mindful of this. References Bostock, Michael, Vadim Ogievetsky, and Jeffrey Heer. 2011. “D\\(^3\\) Data-Driven Documents.” IEEE Transactions on Visualization and Computer Graphics 17 (12): 2301–9. Fong, Brendan, and David I Spivak. 2019. An Invitation to Applied Category Theory: Seven Sketches in Compositionality. Cambridge University Press. Krzywinski, Martin. 2013. “Axes, Ticks and Grids.” Nature Methods 10 (February): 183. https://doi.org/10.1038/nmeth.2337. Lawvere, F William, and Stephen H Schanuel. 2009. Conceptual Mathematics: A First Introduction to Categories. Cambridge University Press. Michell, Joel. 1986. “Measurement Scales and Statistics: A Clash of Paradigms.” Psychological Bulletin 100 (3): 398. Murrell, Paul. 2005. R Graphics. Chapman; Hall/CRC. Observable. 2024. “D3-Scale \\(\\vert\\) D3 by Observable.” https://d3js.org/d3-scale. Satyanarayan, Arvind, Ryan Russell, Jane Hoffswell, and Jeffrey Heer. 2015. “Reactive Vega: A Streaming Dataflow Architecture for Declarative Interactive Visualization.” IEEE Transactions on Visualization and Computer Graphics 22 (1): 659–68. Stevens, Stanley Smith. 1946. “On the Theory of Scales of Measurement.” Science 103 (2684): 677–80. Tufte, Edward R. 2001. The Visual Display of Quantitative Information. Cheshire, Connecticut: Graphics Press LLC. V8 Core Team. 2017. “Elements Kinds in V8 \\(\\cdot\\) V8.” https://v8.dev/blog/elements-kinds. Wickham, Hadley. 2013. “Bin-Summarise-Smooth: A Framework for Visualising Large Data.” Had. Co. Nz, Tech. Rep. ———. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org. Wickham, Hadley, Thomas Lin Pedersen, and Dana Seidel. 2023. Scales: Scale Functions for Visualization. https://CRAN.R-project.org/package=scales. Wilkinson, Leland. 2012. The Grammar of Graphics. Springer. Ziemkiewicz, Caroline, and Robert Kosara. 2009. “Embedding Information Visualization Within Visual Representation.” In Advances in Information and Intelligent Systems, 307–26. Springer. Unless the length of the data changes. I have not implemented data streaming for plotscape/r yet, however, it would be easy to extend bijection factor for streaming by simply pushing/popping the array of indices.↩︎ Not currently implemented in plotscape/r↩︎ "],["applied-examples.html", "6 Applied examples", " 6 Applied examples Examples of applied data analysis using plotscaper will go here. "],["discussion.html", "7 Discussion", " 7 Discussion Discussion will go here. "],["glossary.html", "8 Glossary", " 8 Glossary 8.0.0.1 Array of Structs (AoS) vs. Struct of Arrays (SoA) Two-dimensional, tabular data is ubiquitous in data analytic workflows. However, since computer memory is fundamentally one-dimensional. Thus, when representing two-dimensional tables, we need to pick one of the dimensions as “primary” (and the other as “secondary”). This leads to two fundamentally different data representations. First, we can represent our data as an array of rows, also known as Array of Structs (AoS). In this representation, the rows of the data set are represented as heterogeneous key-value stores (structs/maps/dictionaries) and the entire data set is simply an array of these rows. For example, suppose we are storing a data set of users, such that, for each user, we record their name, age, and gender. Then the AoS way of representing this data (in TypeScript) would be: interface User { name: string; age: number; gender: string; } type Users = User[]; Second, we can represent our data as a dictionary of columns, known also as the Struct of Arrays (SoA). In this representation, columns are stored as homogeneous arrays in a single key-value store, and rows are represented implicitly. For example, the way to represent the same data in the SoA format would be: interface Users { name: string[]; age: number[]; gender: string[]; } The way how we choose to represent the data has an impact on various performance characteristics, primarily compute time and memory. The mechanisms underlying these differences are quite general and apply to both in-memory and on-disk data; hence why the topic is also studied in database design (see e.g. Abadi et al. 2013). The SoA layout has (typically) smaller memory footprint and better performance in tight loops that operate on individual columns, thanks to cache locality (Abadi et al. 2013; Acton 2014; Kelley 2023). The AoS layout has arguably better developer ergonomics and can perform better when retrieving individual records (hence why it is more common in traditional Online Transaction Processing databases, Abadi et al. 2013). Generally, for data analytic workflows where we need to summarize values across many rows of data, the column-based SoA representation has the better performance characteristics, and hence why it is typically preferred in data analytic libraries, for example in base R’s data.frame class or in the pandas DataFrame class (R Core Team 2024; Pandas Core Team 2024). However, this is not always the case: for example, in the popular JavaScript data visualization/transformation library D3, data sets are represented as arrays of (JSON) rows (Mike Bostock 2022). A possible objection to worrying about data layout in high-level interpreted languages like JavaScript and R is that these languages may represent data completely differently under the hood anyway. For example, JavaScript engines such as V8 utilize hidden classes to lay out data in memory more efficiently (Bruni 2017; V8 Core Team 2024), such that even AoS data structures are backed by underlying arrays. However, despite this, there is still good evidence that packed arrays of plain values (such as integers and float), such as used in SoA, present better performance characteristics (Bruni 2017; Stange 2024). 8.0.0.2 JSON Short for “JavaScript Object Notation”, JSON is a flexible data format based on the JavaScript object type (Ecma International 2024; see also e.g. Bourhis et al. 2017; Pezoa et al. 2016). On the top level, a JSON is a key-value store (also known as dictionary, object, struct, hash-table, or list in other languages) with string keys and values of any of the following types: string, number, boolean, null (an undefined/missing value), an array (which can contain any other valid JSON values), or another JSON object. For example, the following is a valid JSON: { &quot;name&quot;: &quot;Adam&quot;, &quot;age&quot;: 30, &quot;friends&quot;: [{ &quot;name&quot;: &quot;Sam&quot;, &quot;age&quot;: 30 }, { &quot;name&quot;: &quot;Franta&quot;, &quot;age&quot;: 26 }], &quot;can drive&quot;: true, &quot;problems&quot;: null } The JSON specification is more restrictive compared to the full JavaScript object type (as implemented in the browser and various JavaScript runtimes). JavaScript runtime objects are very flexible - they can contain non-string keys (numbers or symbols) and non-primitive values such as functions/methods. In contrast, JSON is a fairly “simple” format designed for declaring and transporting data. For this reason, JSON is often used as the medium for sending data to and from Web APIs (Bourhis et al. 2017; Pezoa et al. 2016) as well as for configuration documents. The main advantages of JSON are that it is a simple, flexible, and human-readable format. Also, due to its recursive nature (JSON arrays and objects can contain other JSON arrays and objects), it can be used to express a wide variety of hierarchical data structures which would not be efficient to express in “flat” data formats such as CSV. However, this flexibility also comes with some disadvantages. The recursive nature of the format makes parsing JSON files inherently more time- and compute-intensive, and, since the values in a JSON can be of any type (as long as it is a valid JSON type), it is often necessary to validate JSON inputs (Pezoa et al. 2016). 8.0.0.3 SVG Short for “Scalable Vector Graphics”, SVG is a flexible markup language for defining vector graphics (MDN 2024e). Based on XML, SVG graphics are specified as a hierarchy of elements enclosed by tags. These tags may be given attributes, further modifying their behavior. For example, the following is a valid SVG: &lt;svg width=&quot;400&quot; height=&quot;400&quot;&gt; &lt;circle cx=&quot;200&quot; cy=&quot;200&quot; r=&quot;50&quot; fill=&quot;skyblue&quot;&gt;&lt;/circle&gt; &lt;rect x=&quot;150&quot; y=&quot;150&quot; width=&quot;50&quot; height=&quot;50&quot; fill=&quot;firebrick&quot;&gt;&lt;/rect&gt; &lt;/svg&gt; And this is its output, as interpreted by a Web browser: Compared to typical raster formats such as PNG or JPEG, in which the image is defined as an array of bytes (pixels), SVG’s primary advantage is its lossless quality: images can be arbitrarily scaled or transformed without affecting the image’s quality. SVG images can also be easily manipulated and animated by modifying the elements’ attributes (for example, to move the red rectangle in the image above to the right, we could simply increment its “x” attribute). However, the main disadvantage of SVG is that the file size scales with the number of objects in the image. As such, SVG images with many small objects (such as points on a scatterplot) can become prohibitively large and slow to render. References Abadi, Daniel, Peter Boncz, Stavros Harizopoulos, Stratos Idreos, Samuel Madden, et al. 2013. “The Design and Implementation of Modern Column-Oriented Database Systems.” Foundations and Trends in Databases 5 (3): 197–280. Acton, Mike. 2014. “Data-Oriented Design and c++.” Luento. CppCon. https://www.youtube.com/watch?v=rX0ItVEVjHc. Bostock, Mike. 2022. “D3.js - Data-Driven Documents.” https://d3js.org. Bourhis, Pierre, Juan L Reutter, Fernando Suárez, and Domagoj Vrgoč. 2017. “JSON: Data Model, Query Languages and Schema Specification.” In Proceedings of the 36th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems, 123–35. Bruni, Camilo. 2017. “Fast Properties in V8.” https://v8.dev/blog/fast-properties. Ecma International. 2024. “JSON.” https://www.json.org/json-en.html. Kelley, Andew. 2023. “A Practical Guide to Applying Data Oriented Design (DoD).” Handmade Seattle. https://www.youtube.com/watch?v=IroPQ150F6c. ———. 2024e. “SVG: Scalable Vector Graphics \\(\\vert\\) MDN.” MDN Web Docs. https://developer.mozilla.org/en-US/docs/Web/SVG. Pandas Core Team. 2024. “DataFrame — Pandas 2.2.3 Documentation.” https://pandas.pydata.org/docs/reference/frame.html. Pezoa, Felipe, Juan L Reutter, Fernando Suarez, Martı́n Ugarte, and Domagoj Vrgoč. 2016. “Foundations of JSON Schema.” In Proceedings of the 25th International Conference on World Wide Web, 263–73. R Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/. Stange, Markus. 2024. “Fast JavaScript with Data-Oriented Design.” FOSDEM. https://archive.fosdem.org/2024/schedule/event/fosdem-2024-2773-fast-javascript-with-data-oriented-design. ———. 2024. “Maps (Hidden Classes) in V8.” https://v8.dev/docs/hidden-classes. "],["appendix.html", "9 Appendix", " 9 Appendix 9.0.0.1 Encapsulation in DOP For example, here’s how we can emulate private property access in JavaScript using Proxy. We create a namespace with a single constructor function that takes an object and a namespace and returns a proxy of the object which prevents access to the object fields outside of the namespace: // Private.ts export namespace Private { export function of&lt;T extends Object&gt;(object: T, namespace: Object) { return new Proxy(object, { get: (t, k, e) =&gt; (e === namespace ? Reflect.get(t, k) : undefined), set: (t, k, v, e) =&gt; (e === namespace ? (Reflect.set(t, k, v), true) : true), }); } } We can then use this namespace in the constructor functions of data we want to make private: import { Private } from &quot;./Private.ts&quot; // Data type - container for stateful data interface User { firstName: string; lastName: string; } // Code module - consists of stateless functions namespace User { // Constructor function export function of(firstName: string, lastName: string): User { return Private.of({firstName, lastName}, User); } // Internal getter function function get(user: User, key: keyof User) { return Reflect.get(user, key, User); } // We could do the same thing for a private setter export function getFullName(user: User) { return get(user, `firstName`) + ` ` + get(user, `lastName`); } } const user = User.of(`Adam`, `Bartonicek`); user.firstName = `Bob` console.log(user) console.log(user.lastName); console.log(User.getFullName(user)); ## { ## firstName: &quot;Adam&quot;, ## lastName: &quot;Bartonicek&quot;, ## } ## undefined ## Adam Bartonicek Clearly, it is possible to encapsulate data while maintaining separation between data and code. Specifically, the data underpinning User is still a plain data object and can be inspected using console.log. However, we cannot access or modify its properties outside of the User code module. "],["references.html", "10 References", " 10 References Abadi, Daniel, Peter Boncz, Stavros Harizopoulos, Stratos Idreos, Samuel Madden, et al. 2013. “The Design and Implementation of Modern Column-Oriented Database Systems.” Foundations and Trends in Databases 5 (3): 197–280. Abbate, J. 1999. “Getting small: a short history of the personal computer.” Proc. IEEE 87 (9): 1695–98. https://doi.org/10.1109/5.784256. Abrams, Dave. 2024. “Total Weak Order Vs Total Order.” Mathematics Stack Exchange. https://math.stackexchange.com/questions/3793222/total-weak-order-vs-total-order. Abukhodair, Felwa A, Bernhard E Riecke, Halil I Erhan, and Chris D Shaw. 2013. “Does Interactive Animation Control Improve Exploratory Data Analysis of Animated Trend Visualization?” In Visualization and Data Analysis 2013, 8654:211–23. SPIE. Acton, Mike. 2014. “Data-Oriented Design and c++.” Luento. CppCon. https://www.youtube.com/watch?v=rX0ItVEVjHc. ———. 2019. “Building a Data-Oriented Future.” WeAreDevelopers. https://www.youtube.com/watch?v=u8B3j8rqYMw. Alsallakh, Bilal, Luana Micallef, Wolfgang Aigner, Helwig Hauser, Silvia Miksch, and Peter Rodgers. 2014. “Visualizing Sets and Set-Typed Data: State-of-the-Art and Future Challenges.” Eurographics Conference on Visualization (EuroVis). Backus, John. 1978. “The History of Fortran i, II, and III.” ACM Sigplan Notices 13 (8): 165–80. Baez, John. 2023. “Applied Category Theory Course.” https://math.ucr.edu/home/baez/act_course. Barrett, Lisa Feldman. 2013. “Psychological Construction: The Darwinian Approach to the Science of Emotion.” Emotion Review 5 (4): 379–89. Batch, Andrea, and Niklas Elmqvist. 2017. “The Interactive Visualization Gap in Initial Exploratory Data Analysis.” IEEE Transactions on Visualization and Computer Graphics 24 (1): 278–87. Bayliss, Jessica D. 2022. “The Data-Oriented Design Process for Game Development.” Computer 55 (5): 31–38. Becker, Richard A, and William S Cleveland. 1987. “Brushing Scatterplots.” Technometrics 29 (2): 127–42. Beckmann, Peter E. 1995. “On the Problem of Visualizing Point Distributions in High Dimensional Spaces.” Computers &amp; Graphics 19 (4): 617–29. Bertin, Jacques. 1967. Sémiologie Graphique: Les diagrammes, les réseaux, les cartes. Gauthier-Villars. Black, Andrew P. 2013. “Object-Oriented Programming: Some History, and Challenges for the Next Fifty Years.” Information and Computation 231: 3–20. Blokt. 2020. “Flare \\(\\vert\\) Data Visualization for the Web.” Blokt - Privacy, Tech, Bitcoin, Blockchain &amp; Cryptocurrency. https://blokt.com/tool/prefuse-flare. Booch, Grady, Robert A Maksimchuk, Michael W Engle, Bobbi J Young, Jim Connallen, and Kelli A Houston. 2008. “Object-Oriented Analysis and Design with Applications.” ACM SIGSOFT Software Engineering Notes 33 (5): 29–29. Bostock, Michael, Vadim Ogievetsky, and Jeffrey Heer. 2011. “D\\(^3\\) Data-Driven Documents.” IEEE Transactions on Visualization and Computer Graphics 17 (12): 2301–9. Bostock, Mike. 2022. “D3.js - Data-Driven Documents.” https://d3js.org. Bourhis, Pierre, Juan L Reutter, Fernando Suárez, and Domagoj Vrgoč. 2017. “JSON: Data Model, Query Languages and Schema Specification.” In Proceedings of the 36th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems, 123–35. Brodbeck, Dominique, Riccardo Mazza, and Denis Lalanne. 2009. “Interactive Visualization - A Survey.” In Human Machine Interaction, 27–46. Berlin, Germany: Springer. https://doi.org/10.1007/978-3-642-00437-7_2. Bruni, Camilo. 2017. “Fast Properties in V8.” https://v8.dev/blog/fast-properties. Buja, Andreas, Dianne Cook, and Deborah F Swayne. 1996. “Interactive High-Dimensional Data Visualization.” Journal of Computational and Graphical Statistics 5 (1): 78–99. Byron, Lee, and Martin Wattenberg. 2008. “Stacked Graphs–Geometry &amp; Aesthetics.” IEEE Transactions on Visualization and Computer Graphics 14 (6): 1245–52. Cairo, Alberto. 2014. “Graphics Lies, Misleading Visuals: Reflections on the Challenges and Pitfalls of Evidence-Driven Visual Communication.” In New Challenges for Data Design, 103–16. Springer. ———. 2019. How Charts Lie: Getting Smarter about Visual Information. WW Norton &amp; Company. Chambers, John M. 2014. “Object-Oriented Programming, Functional Programming and r.” Statistical Science 29 (2): 167–80. https://doi.org/10.1214/13-STS452. Cleveland, William S. 1985. The Elements of Graphing Data. Wadsworth Publ. Co. Cleveland, William S, and Robert McGill. 1984. “Graphical Perception: Theory, Experimentation, and Application to the Development of Graphical Methods.” Journal of the American Statistical Association 79 (387): 531–54. Codd, Edgar F. 1970. “A Relational Model of Data for Large Shared Data Banks.” Communications of the ACM 13 (6): 377–87. Dijkstra, Edsger W. 1968. “Letters to the Editor: Go to Statement Considered Harmful.” Communications of the ACM 11 (3): 147–48. Dimara, Evanthia, and Charles Perin. 2019. “What Is Interaction for Data Visualization?” IEEE Transactions on Visualization and Computer Graphics 26 (1): 119–29. Dix, Alan, and Geoffrey Ellis. 1998. “Starting simple: adding value to static visualisation through simple interaction.” In AVI ’98: Proceedings of the working conference on Advanced visual interfaces, 124–34. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/948496.948514. Donoho, Andrew W, David L Donoho, and Miriam Gasko. 1988. “MacSpin: Dynamic Graphics on a Desktop Computer.” IEEE Computer Graphics and Applications 8 (4): 51–58. Ecma International. 2024. “JSON.” https://www.json.org/json-en.html. Electoral Commission New Zealand. 2020. “Official Referendum Results Released.” https://elections.nz/media-and-news/2020/official-referendum-results-released. ———. 2023. “E9 Statistics - Overall Results.” https://www.electionresults.govt.nz/electionresults_2023/index.html. Elmqvist, Niklas, Andrew Vande Moere, Hans-Christian Jetter, Daniel Cernea, Harald Reiterer, and TJ Jankun-Kelly. 2011. “Fluid Interaction for Information Visualization.” Information Visualization 10 (4): 327–40. Evan You and the Vue Core Team. 2024. “Vue.js.” https://vuejs.org. Fabian, Richard. 2018. “Data-Oriented Design.” Framework 21: 1–7. Fisherkeller, Mary Anne, Jerome H Friedman, and John W Tukey. 1974. “An Interactive Multidimensional Data Display and Analysis System.” SLAC National Accelerator Lab., Menlo Park, CA (United States). Foley, James D. 1990. “Scientific Data Visualization Software: Trends and Directions.” The International Journal of Supercomputing Applications 4 (2): 154–57. Fong, Brendan, and David I Spivak. 2019. An Invitation to Applied Category Theory: Seven Sketches in Compositionality. Cambridge University Press. Fowlkes, EB. 1969. “User’s Manual for a System Fo Active Probability Plotting on Graphic-2.” Tech-Nical Memorandum, AT&amp;T Bell Labs, Murray Hill, NJ. Frame, Scott, and John W Coffey. 2014. “A Comparison of Functional and Imperative Programming Techniques for Mathematical Software Development.” Journal of Systemics, Cybernetics and Informatics 12 (2): 1–10. Franconeri, Steven L, Lace M Padilla, Priti Shah, Jeffrey M Zacks, and Jessica Hullman. 2021. “The Science of Visual Data Communication: What Works.” Psychological Science in the Public Interest 22 (3): 110–61. Friendly, Michael. 2006. “A Brief History of Data Visualization.” In Handbook of Computational Statistics: Data Visualization, edited by C. Chen, W. Härdle, and A Unwin, III???–. Heidelberg: Springer-Verlag. Friendly, Michael, and Howard Wainer. 2021. A History of Data Visualization and Graphic Communication. Harvard University Press. Gelman, Andrew, and Antony Unwin. 2013. “Infovis and Statistical Graphics: Different Goals, Different Looks.” Journal of Computational and Graphical Statistics 22 (1): 2–28. Gross, Carson. 2024. “The Grug Brained Developer.” https://grugbrain.dev. Hadar, Irit. 2013. “When Intuition and Logic Clash: The Case of the Object-Oriented Paradigm.” Science of Computer Programming 78 (9): 1407–26. Härkönen, Toni. 2019. “Advantages and Implementation of Entity-Component-Systems.” Heer, Jeffrey, and Michael Bostock. 2010. “Crowdsourcing Graphical Perception: Using Mechanical Turk to Assess Visualization Design.” In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, 203–12. Heer, Jeffrey, Stuart K. Card, and James A. Landay. 2005. “prefuse: a toolkit for interactive information visualization.” In CHI ’05: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, 421–30. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/1054972.1055031. Hickey, Rich. 2011. “Simple Made Easy.” Strange Loop. https://www.youtube.com/watch?v=LKtk3HCgTa8. ———. 2013. “Design, Composition, and Performance.” ETE Conference. https://www.youtube.com/watch?v=QCwqnjxqfmY. ———. 2018. “Maybe Not.” Clojure Conj. https://www.youtube.com/watch?v=YR5WdGrpoug. Highsoft. 2022. “Render Millions of Chart Points with the Boost Module Highcharts.” Highcharts. https://www.highcharts.com/blog/tutorials/highcharts-high-performance-boost-module. ———. 2024. “Highcharts - Interactive Charting Library for Developers.” Highcharts Blog \\(\\vert\\) Highcharts. https://www.highcharts.com. Holtz, Yan. 2022. “Interactive charts \\(\\vert\\) the R Graph Gallery.” https://r-graph-gallery.com/interactive-charts.html. Howard, David, and Alan M MacEachren. 1995. “Constructing and Evaluating an Interactive Interface for Visualizing Reliability.” In Congresso Da Associação Cartográfica Internacional–ICA, 17:321–29. Jankun-Kelly, TJ, Kwan-Liu Ma, and Michael Gertz. 2007. “A Model and Framework for Visualization Exploration.” IEEE Transactions on Visualization and Computer Graphics 13 (2): 357–69. Jordan, Howell, Goetz Botterweck, John Noll, Andrew Butterfield, and Rem Collier. 2015. “A Feature Model of Actor, Agent, Functional, Object, and Procedural Programming Languages.” Science of Computer Programming 98: 120–39. Kay, Alan C. 1996. “The Early History of Smalltalk.” In History of Programming Languages—II, 511–98. Kehrer, Johannes, Roland N Boubela, Peter Filzmoser, and Harald Piringer. 2012. “A Generic Model for the Integration of Interactive Visualization and Statistical Computing Using r.” In 2012 IEEE Conference on Visual Analytics Science and Technology (VAST), 233–34. IEEE. Keim, Daniel A. 2002. “Information Visualization and Visual Data Mining.” IEEE Transactions on Visualization and Computer Graphics 8 (1): 1–8. Kelleher, Curran, and Haim Levkowitz. 2015. “Reactive Data Visualizations.” In Visualization and Data Analysis 2015, 9397:263–69. SPIE. Kelley, Andew. 2023. “A Practical Guide to Applying Data Oriented Design (DoD).” Handmade Seattle. https://www.youtube.com/watch?v=IroPQ150F6c. Kindlmann, Gordon, and Carlos Scheidegger. 2014. “An Algebraic Process for Visualization Design.” IEEE Transactions on Visualization and Computer Graphics 20 (12): 2181–90. Kosara, Robert. 2016. “Stacked Bars Are the Worst.” https://eagereyes.org/blog/2016/stacked-bars-are-the-worst. Kruskal, J. B. 1965. “Multidimensional Scaling.” https://community.amstat.org/jointscsg-section/media/videos. Krzywinski, Martin. 2013. “Axes, Ticks and Grids.” Nature Methods 10 (February): 183. https://doi.org/10.1038/nmeth.2337. Kunst, Joshua. 2022. Highcharter: A Wrapper for the ’Highcharts’ Library. Lawvere, F William, and Stephen H Schanuel. 2009. Conceptual Mathematics: A First Introduction to Categories. Cambridge University Press. Leman, Scotland C, Leanna House, Dipayan Maiti, Alex Endert, and Chris North. 2013. “Visual to Parametric Interaction (V2pi).” PloS One 8 (3): e50474. Mayr, Ernst. 1999. Systematics and the Origin of Species, from the Viewpoint of a Zoologist. Harvard University Press. MDN. 2024a. “EventTarget - Web APIs \\(\\vert\\) MDN.” MDN Web Docs. https://developer.mozilla.org/en-US/docs/Web/API/EventTarget. ———. 2024b. “Classes - JavaScript \\(\\vert\\) MDN.” MDN Web Docs. https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Classes. ———. 2024c. “Symbol - JavaScript \\(\\vert\\) MDN.” MDN Web Docs. https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Symbol. ———. 2024d. “Functions - JavaScript \\(\\vert\\) MDN.” MDN Web Docs. https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Functions. ———. 2024e. “SVG: Scalable Vector Graphics \\(\\vert\\) MDN.” MDN Web Docs. https://developer.mozilla.org/en-US/docs/Web/SVG. ———. 2024f. “JavaScript Language Overview - JavaScript \\(\\vert\\) MDN.” MDN Web Docs. https://developer.mozilla.org/en-US/docs/Web/JavaScript/Language_overview. Meta. 2024. “React.” https://react.dev. Meyer, Bertrand. 1997. Object-Oriented Software Construction. Vol. 2. Prentice hall Englewood Cliffs. Michell, Joel. 1986. “Measurement Scales and Statistics: A Clash of Paradigms.” Psychological Bulletin 100 (3): 398. Milewski, Bartosz. 2018. Category Theory for Programmers. Blurb. Moseley, Ben, and Peter Marks. 2006. “Out of the Tar Pit.” Software Practice Advancement (SPA) 2006. Murrell, Paul. 2005. R Graphics. Chapman; Hall/CRC. Nicolai Parlog. 2024. “Data Oriented Programming in Java 21.” Devoxx. https://www.youtube.com/watch?v=8FRU_aGY4mY. Nikolov, Stoyan. 2018. “OOP Is Dead, Long Live Data-Oriented Design.” CppCon. https://www.youtube.com/watch?v=yy8jQgmhbAU&amp;t=2810s. nLab. 2024a. “Posetal Reflection in nLab.” https://ncatlab.org/nlab/show/posetal+reflection. ———. 2024b. “Strict Weak Order in nLab.” https://ncatlab.org/nlab/show/strict+weak+order. ———. 2024c. “Weak Order in nLab.” https://ncatlab.org/nlab/show/weak+order. Observable. 2024. “D3-Scale \\(\\vert\\) D3 by Observable.” https://d3js.org/d3-scale. Pandas Core Team. 2024. “DataFrame — Pandas 2.2.3 Documentation.” https://pandas.pydata.org/docs/reference/frame.html. Parihar, Raj. 2015. “Branch Prediction Techniques and Optimizations.” University of Rochester, NY, USA. Petricek, Tomas. 2021. “Composable Data Visualizations.” Journal of Functional Programming 31: e13. Pezoa, Felipe, Juan L Reutter, Fernando Suarez, Martı́n Ugarte, and Domagoj Vrgoč. 2016. “Foundations of JSON Schema.” In Proceedings of the 25th International Conference on World Wide Web, 263–73. Pike, William A, John Stasko, Remco Chang, and Theresa A O’connell. 2009. “The Science of Interaction.” Information Visualization 8 (4): 263–74. Pinter, Charles C. 2010. A Book of Abstract Algebra. Courier Corporation. Plotly Inc. 2022. “Part 4. Interactive Graphing and Crossfiltering \\(\\vert\\) Dash for Python Documentation \\(\\vert\\) Plotly.” https://dash.plotly.com/interactive-graphing. ———. 2024. “Webgl.” https://plotly.com/python/webgl-vs-svg. Quadri, Ghulam Jilani, and Paul Rosen. 2021. “A Survey of Perception-Based Visualization Studies by Task.” IEEE Transactions on Visualization and Computer Graphics. R Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/. Raghavan, P., H. Shachnai, and M. Yaniv. 1998. “Dynamic Schemes for Speculative Execution of Code.” In Proceedings. Sixth International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems (Cat. No.98TB100247), 24. IEEE. https://doi.org/10.1109/MASCOT.1998.693711. Rheingans, Penny. 2002. “Are We There yet? Exploring with Dynamic Visualization.” IEEE Computer Graphics and Applications 22 (1): 6–10. Rich Harris and the Svelte Core Team. 2024. “Svelte.” https://svelte.dev. Rust Foundation. 2024. “Pub - Rust.” https://doc.rust-lang.org/std/keyword.pub.html. Satyanarayan, Arvind, Ryan Russell, Jane Hoffswell, and Jeffrey Heer. 2015. “Reactive Vega: A Streaming Dataflow Architecture for Declarative Interactive Visualization.” IEEE Transactions on Visualization and Computer Graphics 22 (1): 659–68. Sharvit, Yehonathan. 2022. Data-Oriented Programming: Reduce Software Complexity. Simon; Schuster. Smeltzer, Karl, and Martin Erwig. 2018. “A Domain-Specific Language for Exploratory Data Visualization.” In Proceedings of the 17th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences, 1–13. Smeltzer, Karl, Martin Erwig, and Ronald Metoyer. 2014. “A Transformational Approach to Data Visualization.” In Proceedings of the 2014 International Conference on Generative Programming: Concepts and Experiences, 53–62. Stange, Markus. 2024. “Fast JavaScript with Data-Oriented Design.” FOSDEM. https://archive.fosdem.org/2024/schedule/event/fosdem-2024-2773-fast-javascript-with-data-oriented-design. Stepanov, Alexander A. 2013. “Efficient Programming with Components.” A9. Youtube. https://www.youtube.com/playlist?list=PLHxtyCq_WDLXryyw91lahwdtpZsmo4BGD. Stepanov, Alexander A, and Paul McJones. 2009. Elements of Programming. Addison-Wesley Professional. Stevens, Stanley Smith. 1946. “On the Theory of Scales of Measurement.” Science 103 (2684): 677–80. Swayne, Deborah F., Dianne Cook, and Andreas Buja. 1998. “XGobi: Interactive Dynamic Data Visualization in the X Window System.” J. Comput. Graph. Stat. 7 (1): 113–30. https://doi.org/10.1080/10618600.1998.10474764. Swayne, Deborah F., Duncan Temple Lang, Andreas Buja, and Dianne Cook. 2003. “GGobi: evolving from XGobi into an extensible framework for interactive data visualization.” Comput. Statist. Data Anal. 43 (4): 423–44. https://doi.org/10.1016/S0167-9473(02)00286-4. Tal, Eran. 2015. “Measurement in Science.” Theus, Martin. 2002. “Interactive Data Visualization using Mondrian.” J. Stat. Soft. 7 (November): 1–9. https://doi.org/10.18637/jss.v007.i11. Thudt, Alice, Jagoda Walny, Charles Perin, Fateme Rajabiyazdi, Lindsay MacDonald, Diane Vardeleon, Saul Greenberg, and Sheelagh Carpendale. 2016. “Assessing the Readability of Stacked Graphs.” In Proceedings of Graphics Interface Conference (GI). Tufte, Edward R. 2001. The Visual Display of Quantitative Information. Cheshire, Connecticut: Graphics Press LLC. Tukey, John W. 1962. “The Future of Data Analysis.” The Annals of Mathematical Statistics 33 (1): 1–67. Tukey, John W et al. 1977. Exploratory Data Analysis. Vol. 2. Reading, MA. Unwin, Antony. 1999. “Requirements for interactive graphics software for exploratory data analysis.” Comput. Statist. 14 (1): 7–22. https://doi.org/10.1007/PL00022706. Urbanek, Simon. 2011. “iPlots eXtreme: Next-Generation Interactive Graphics Design and Implementation of Modern Interactive Graphics.” Computational Statistics 26 (3): 381–93. Urbanek, Simon, and Martin Theus. 2003. “iPlots: High Interaction Graphics for r.” In Proceedings of the 3rd International Workshop on Distributed Statistical Computing. Citeseer. V8 Core Team. 2017. “Elements Kinds in V8 \\(\\cdot\\) V8.” https://v8.dev/blog/elements-kinds. ———. 2024. “Maps (Hidden Classes) in V8.” https://v8.dev/docs/hidden-classes. Van Eerd, Tony. 2024. “Value Oriented Programming Part v - Return of the Values.” C++ Now. Youtube. https://www.youtube.com/watch?v=sc1guyo5Rso. Van Roy, Peter et al. 2009. “Programming Paradigms for Dummies: What Every Programmer Should Know.” New Computational Paradigms for Computer Music 104: 616–21. Vega Project. 2022. “Example Gallery: Interactive.” https://vega.github.io/vega-lite/examples/#interactive. ———. 2024a. “Brushing Scatter Plots Example.” Vega. https://vega.github.io/vega/examples/brushing-scatter-plots. ———. 2024b. “Vega and D3.” Vega. https://vega.github.io/vega/about/vega-and-d3. Velleman, Paul F, and Leland Wilkinson. 1993. “Nominal, Ordinal, Interval, and Ratio Typologies Are Misleading.” The American Statistician 47 (1): 65–72. Wickham, Hadley. 2013. “Bin-Summarise-Smooth: A Framework for Visualising Large Data.” Had. Co. Nz, Tech. Rep. ———. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org. ———. 2019. Advanced r. Chapman; Hall/CRC. Wickham, Hadley, Thomas Lin Pedersen, and Dana Seidel. 2023. Scales: Scale Functions for Visualization. https://CRAN.R-project.org/package=scales. Wikipedia. 2022. “Duck test - Wikipedia.” https://en.wikipedia.org/w/index.php?title=Duck_test&amp;oldid=1110781513. Wilke, Claus O. 2019. Fundamentals of Data Visualization: A Primer on Making Informative and Compelling Figures. O’Reilly Media. Wilkinson, Leland. 2012. The Grammar of Graphics. Springer. Will, Brian. 2016. “Object-Oriented Programming Is Bad.” Youtube. https://www.youtube.com/watch?v=QM1iUe6IofM. Wills, Graham. 2008. “Linked Data Views.” In Handbook of Data Visualization, 217–41. ch. II. 9. Springer Berlin/Heidelberg, Germany. Wirfs-Brock, Allen, and Brendan Eich. 2020. “JavaScript: the first 20 years.” Proc. ACM Program. Lang. 4 (HOPL): 1–189. https://doi.org/10.1145/3386327. Yi, Ji Soo, Youn ah Kang, John Stasko, and Julie A Jacko. 2007. “Toward a Deeper Understanding of the Role of Interaction in Information Visualization.” IEEE Transactions on Visualization and Computer Graphics 13 (6): 1224–31. Yorgey, Brent A. 2012. “Monoids: Theme and Variations (Functional Pearl).” ACM SIGPLAN Notices 47 (12): 105–16. Young, Forrest W, Pedro M Valero-Mora, and Michael Friendly. 2011. Visual Statistics: Seeing Data with Dynamic Interactive Graphics. John Wiley &amp; Sons. Ziemkiewicz, Caroline, and Robert Kosara. 2009. “Embedding Information Visualization Within Visual Representation.” In Advances in Information and Intelligent Systems, 307–26. Springer. Zig Software Foundation. 2024. “Documentation - the Zig Programming Language.” https://ziglang.org/documentation/master. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]

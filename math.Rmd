---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Mathematical theory

This chapter provides an overview of essential concepts from category theory, abstract algebra, and general mathematics. Starting with some foundational topics, such as functions, relations, and orders, it slowly builds up to more advanced concepts such as categories, monoids, and functors. Readers familiar these concepts may feel free to skip ahead. However, even the fundamental concepts will be used throughout the thesis, so a refresher might be beneficial. 

The material follows primarily from @fong2019, @lawvere2009, @baez2023, @pinter2010, and @milewski2018. For an accessible introduction to the topic, interested readers are encouraged to consult these references, particularly @fong2019 and @lawvere2009.

#### Note on past applications of abstract algebra and category theory

Abstract algebra and category theory have been applied to data visualization in the past, in a limited way. Handful authors have used concepts from either field to describe the more abstract aspects of data visualization. For example, @beckmann1995 had used categories and functors to lay down a broad theoretical framework for visualization, @hutchins1999 attempted the same with algebras, @kindlmann2014 used functors to define valid perceptual representations of the data, and @hibbard1994 used lattice theory to describe data visualization in the presence of incomplete or approximate data (such as finite-precision floating-point numbers). 

Several authors have also discussed category theory and data visualization in a more applied context, by way of functional programming. Specifically, there have been a number of libraries and domain-specific languages (DSLs) for data visualization which have relied on functional programming concepts and have thus been linked to category theory. For some examples, see @yorgey2012, @petricek2021, @smeltzer2014, or @smeltzer2018. 

In contrast to these prior works, this thesis leverages category theory to address certain practical problems that arise during interactive data visualization, without introducing a specific functional programming model or DSL. Specifically, the goal is to use concepts from category theory to reason about combinations of graphics, statistical summaries, and interactive features. Ultimately, I argue that these algebraic concepts can help us make well-founded statements about which interactive data visualizations will behave in sensible ways and which ones will not.

And now for the theory.

### Relations

A relation is one of the simplest mathematical structures. Given two sets $X$ and $Y$, a relation $R$ between $X$ and $Y$ is a subset of the Cartesian product of the two sets, $R \subseteq X \times Y$. In other words, a relation can be thought of as the subset of pairs $(x, y) \in X \times Y$ for which the condition "$x$ and $y$ relate" holds. Note that $X$ and $Y$ can be the same set, such that $R \subseteq X \times X$. 

There are many different types of relations. One of the most fundamental relations is equality; in this case, "$x$ and $y$ relate" means that, for our purposes, $x$ and $y$ are the same, i.e. $x = y$. Other examples of relations include the usual order relations $<$, $\leq$, $>$, or $\geq$, and the divides operator $\mid$ ($x \mid y$ means "$x$ divides $y$ without remainder").

Since a relation is a subset of the product set $X \times Y$, we can visualize it as a matrix, with values of $X$ as rows, values of $Y$ as columns, and the related pairs $(x, y)$ marked out in some specific way. For example, here's how we can display the order relation $\leq$ on the set $X = \{ 1, 2, 3 \}$:

```{r}
#| echo: false
#| dpi: 300
#| fig-cap: "A relation is a subset of the Cartesian product of two sets.
#| The diagram shows the usual order relation $\\leq$.
#| We can see that 1 is less than or equal to every other element, 
#| 2 is less than or equal to 2 and 3, and 3 is less than or equal to 3 only.
#| Note the symmetry between rows and columns - this is due to the fact that
#| the same set ($X$) is display on both dimensions." 

library(ggplot2)

df_relation <- expand.grid(x = 1:3, y = 1:3)
df_relation$rel <- df_relation$x <= df_relation$y

ggplot(df_relation, aes(x, y, col = factor(rel))) +
  geom_point(size = 25) +
  scale_x_continuous(breaks = 1:3, limits = c(0.5, 3.5)) +
  scale_y_continuous(breaks = 1:3, limits = c(0.5, 3.5)) +
  scale_color_manual(values = c("white", "indianred")) +
  labs(x = "X", y = "X") +
  guides(col = "none") +
  clean_theme +
  theme(panel.background = element_rect(fill = "antiquewhite"),
        panel.border = element_blank(),
        axis.title.y = element_text(angle = 0, vjust = 0.5),
        axis.title = element_text(colour = "grey60"))
```

A relation $R$ can be signified with an infix symbol (such as $\star$), such that, if $x$ and $y$ relate, $(x, y) \in R$, then we write $x \star_R y$ or $x \star y$ ($R$ implicit), for example, $x = y$, $x \leq y$, and so on. Alternatively, for less common types of relations, $R$ can also be used as the infix symbol, such that $x R y$ means "$x$ and $y$ relate under $R$". If two elements do not relate, $(x, y) \not \in R$, we typically do not write this out explicitly - the lack of relation is indicated by its absence.

Relations can have properties. For example, some types of relations are *reflexive*, such that every element relates to itself: $x \star x$ for all $x \in X$. This is the case for equivalence relations. In fact, we can define equivalence relations using just three properties:

::: {.definition name="Equivalence relation"}
A relation $\sim$ on $X$ is called an equivalence relation if it is:

1. *Reflexive*: $x \sim x$ for all $x \in X$
2. *Symmetric*: $x \sim y$ if and only if $y \sim x$ for all $x, y \in X$
3. *Transitive*: if $x \sim y$ and $y \sim z$, then $x \sim z$
:::

Equivalence relations encode the notion that two things are the same, *for whatever our purpose is*. We can further use them to assign objects in $X$ to *equivalence classes*, which divide $X$ into groups of equivalent objects:

::: {.definition name="Equivalence class"}
Given a set $X$ and an element $a \in X$, an equivalence class of $a$ is defined as follows:

$$[a] = \{ x \in X : x \sim a \}$$
:::

While relations might seem like very simple constructions, they are incredibly versatile. The next few sections will discuss three important examples of relations: functions, partitions, and preorders. 

### Functions

A function is a special kind of relation which encodes a mapping between two sets. More specifically, let $S$ be the set of sources (also called the *domain*) and $T$ be the set of possible targets (also called the *codomain*). Then, we can think of a function as a relation $F \subseteq S \times T$ of valid source-target pairs $(s, t)$, such that for every $s \in S$ in there exists a unique $t \in T$ with $(s, t) \in F$ (see Figure \@ref(fig:function-subset)). In other words, every source relates to exactly one target:

```{r function-subset}
#| echo: false
#| dpi: 300
#| fig-cap: "A function is a type of relation.
#|  Specifically, it is a subset of the Cartesian product of its domain ($S$) and codomain ($T$),
#|  such that each element in the domain marks out exactly one element in the codomain (shown in red). 
#| The depicted function has the following characteristics:
#|  $F: \\{ 1, 2, 3 \\} \\to \\{ 1, 2, 3 \\}$, such that $F(1) = 1$, $F(2) = 1$, and $F(3) = 2$.
#|  One possible example of a function which conforms to this diagram
#|   might be $f(x) = \\lfloor x / 2 \\rceil$ 
#|  (divide $x$ by two and round to the nearest whole number). 
#|  Note that, for any function, each source maps to exactly one target
#|  (exactly one dot in each column), 
#|  however, some targets may not be reached from any source and others
#|  may be reachable from many sources (zero or multiple dots in any row)."

library(ggplot2)

df_relation <- expand.grid(x = 1:3, y = 1:3)
df_relation$rel <- with(df_relation, round(x / 2 + 0.1) == y)

ggplot(df_relation, aes(x, y, col = factor(rel))) +
  geom_point(size = 25) +
  scale_x_continuous(breaks = 1:3, limits = c(0.5, 3.5)) +
  scale_y_continuous(breaks = 1:3, limits = c(0.5, 3.5)) +
  scale_color_manual(values = c("white", "indianred")) +
  labs(x = "S", y = "T") +
  guides(col = "none") +
  clean_theme +
  theme(panel.background = element_rect(fill = "antiquewhite"),
        panel.border = element_blank(),
        axis.title.y = element_text(angle = 0, vjust = 0.5),
        axis.title = element_text(colour = "grey60"))
```

We can classify functions based on the shape of the relation between the domain and the codomain (see Figure \@ref(fig:function-types)). If every target in the function's codomain has a path leading to it from some source, such that no target is unreachable, then we call the function *surjective* or *onto*. More formally:

::: {.definition name="Surjectivity"}
A function $f$ is surjective if, for all $t \in T$, there exists a $s \in S$ such that $f(s) = t$. 
:::


Alternatively, if each source in the function's domain leads to a unique target, such that no two sources map to the same target, then we call such a function *injective* or *one-to-one*. That is: 

::: {.definition name="Injectivity"}
A function is injective if, for all $s_1, s_2 \in S$, if $f(s_1) = t$ and $f(s_2) = t$, then $s_1 = s_2$. 
:::

Finally, if a function is both surjective and injective, meaning that every target can be reached from, and only from, a unique source, then we call such a function *bijective* or a *bijection*.

::: {.definition name="Bijectivity"}
A function is a bijection and only if it is both surjective and injective, which is also the case if and only if it is invertible. 
:::

```{r, function-types}
#| echo: false
#| dpi: 300
#| fig-cap: "Types of functions. 
#| Left: in a *surjective* function, each target can be reached from some source. 
#| Middle: in an *injective* function, there is a unique source for each target.
#| Right: in a *bijection*, each target can be reached from, and only from, a unique source."
 
library(ggplot2)
library(patchwork)

normalize <- function(x) (x - min(x)) / (max(x) - min(x))

plot_function <- function(mapping, codomain_size = NULL) {
  
  codomain_size <- ifelse(is.null(codomain_size), max(mapping), codomain_size)
  
  domain <- 1:length(mapping)
  codomain <- 1:codomain_size
  
  # Center on the same value
  diff <- mean(domain) - mean(codomain)
  domain <- domain - diff / 2
  codomain <- codomain + diff / 2
  
  domain <- normalize(domain)
  codomain <- normalize(codomain)
  
  point_data <- data.frame(x = c(domain, codomain), 
                           y = rep(c(1, 0), sapply(list(domain, codomain), length)))
  arrow_data <- data.frame(x = domain, xend = codomain[mapping], 
                           y = 1, yend = 0)
  
  h <- 0.25
  
  ggplot(point_data, aes(x, y)) +
    geom_rect(aes(xmin = -0.2, xmax = 1.2, ymin = 0 - h / 2, ymax = 0 + h / 2),
              fill = "antiquewhite") +
    geom_rect(aes(xmin = -0.2, xmax = 1.2, ymin = 1 - h / 2, ymax = 1 + h / 2),
              fill = "antiquewhite") +
    geom_text(x = -0.1, y = 1.08, label = "S", size = 4, col = "grey60") +
    geom_text(x = -0.1, y = -0.08, label = "T", size = 4, col = "grey60") +
    geom_point(size = 5, col = "indianred") +
    geom_segment(data = arrow_data, aes(x = x, xend = xend, y = y, yend = yend),
                 arrow = arrow(angle = 30, length = unit(0.025, "npc")),
                 col = "steelblue") +
    scale_x_continuous(limits = c(-0.2, 1.2)) +
    scale_y_continuous(limits = c(-0.2, 1.2)) +
    clean_theme +
    theme(panel.border = element_blank(),
          axis.text = element_blank(),
          axis.title = element_blank(),
          plot.title = element_text(vjust = -5, hjust = 0.5))
}

p1 <- plot_function(c(1, 2, 1, 3, 4, 4)) + labs(title = "Surjective")
p2 <- plot_function(c(3, 4, 2, 5), 6) + labs(title = "Injective")
p3 <- plot_function(c(4, 2, 6, 3, 1, 5)) + labs(title = "Bijective")
 
p1 + p2 + p3

```

#### More on bijections

Bijections are special since they encode the idea of reversible transformations. Any bijective function $f$ has an associated inverse $f^{-1}$ such that $f^{-1}(f(x)) = x$ and $f(f^{-1}(y)) = y$ for all $x$ and $y$ in the function's domain and codomain, respectively. In other words, we can keep translating the value from the domain to codomain and back without losing any information. We will generalize this idea later when we discuss *isomorphisms*.

As an example, suppose I have a group of friends $x \in X$ that each went to one city $y \in Y$ in Europe during the holiday. I can construct a function $f: X \to Y$ that sends each friend to his or her holiday destination. If every city $y \in Y$ was visited by at least one friend, then the function is surjective. If each friend went to a different destination, then the function is injective. If both are true - that is, if every city on our list was visited by exactly one friend - then the function is bijective. 

In the context of this example, a bijection means that we can just as well use the names of cities $y \in Y$ when we speak of friends $x \in X$. If Sam went to Rome, and he is the only person who went to Rome, I can say "the person who went to Rome" and it will be clear who I am talking about. Thus, bijections apply interchangeability and reversibility. Conversely, a lack of bijection implies that a transformation may lead to information loss. If two people went to Rome and I say "the person who went to Rome", I am inevitably discarding the information about the identity of that person.

#### Composition

An important property of functions is that they can be composed. Specifically, if the domain of one function matches the codomain of another, the functions can be composed by piping the output of the first function as the input of the second. We then end up with a new, composite function: 

::: {.definition name="Function composition"}
Given two functions $f: X \to Y$ and $g: Y \to Z$, we can form a new function $h: X \to Z$ by composing the two functions together such that:

$$h(x) = g(f(x))$$

There are several different ways to denote function composition. One is to write out the composition explicitly using the variable $x$ as in the example above. However, mathematical texts often omit the explicit reference to the variable ($x$) and write the composition in one of several ways:

1. $h = g \circ f$ (read: "apply $g$ after $f$")
2. $h = gf$ (same as above)
3. $h = f ⨾ g$ (read "apply $f$ then $g$")
:::

Throughout this thesis, I will use the bracket notation ($h(x) = g(f(x))$) when explicitly referring to the variable, and the postfix/fat semicolon notation ($h = f ⨾ g$) otherwise. 

Surjectivity, injectivity, and bijectivity propagate through composition: composition of two surjective functions is surjective, composition of two injective functions is injective, and composition of two bijective functions is bijective. However, the converse does not necessarily hold: a bijective function does not have to be composed of two bijections:

```{r bijection}
#| echo: false
#| dpi: 300
#| fig-height: 3
#| fig-cap: "A bijection does not necessarily have to be composed of bijections. The function $f$ is not surjective, and the function $g$ is not injective, nevertheless, their composition $f ⨾ g$ yields a bijective function."

library(ggplot2)
normalize <- function(x) (x - min(x)) / (max(x) - min(x))

df <- data.frame(
  x = c(rep(1:3, c(3, 4, 3))),
  y = c(1:3 + 0.5, 1:4, 1:3 + 0.5)
)

arrow_df <- data.frame(
  x = rep(1:2, c(3, 4)) + 0.0125,
  xend = rep(c(2, 3), c(3, 4)) - 0.0125,
  y = c(1:3 + 0.5, 1:4),
  yend = c(c(1, 2, 4), c(1, 2, 2, 3) + 0.5)
)

ggplot(df, aes(x, y)) +
  geom_rect(xmin = 0.8, xmax = 1.2, ymin = 1, ymax = 4, fill = "antiquewhite") +
  geom_rect(xmin = 1.8, xmax = 2.2, ymin = 0.5, ymax = 4.5, fill = "antiquewhite") +
  geom_rect(xmin = 2.8, xmax = 3.2, ymin = 1, ymax = 4, fill = "antiquewhite") +
  geom_point(size = 5, col = "indianred") +
  geom_segment(data = arrow_df, aes(x = x, xend = xend, y = y, yend = yend),
               arrow = arrow(angle = 30, length = unit(0.025, "npc")),
               col = "steelblue") +
  geom_text(x = 1.5, y = 2.5, label = "f") +
  geom_text(x = 2.5, y = 2.5, label = "g") +
  scale_x_continuous(limits = c(0.5, 3.5)) +
  scale_y_continuous(limits = c(0.5, 4.5)) +
  clean_theme +
  theme(panel.border = element_blank(),
        axis.text = element_blank(),
        axis.title = element_blank())

```

For other interesting examples of inverse function composition problems, see @lawvere2009.

#### The image and the pre-image

There are other things we can do with functions. For example, given a subset of sources, we can ask about the *image* - the set of targets we can reach from those sources:

::: {.definition name="Image"}
For some subset $S_i \subseteq S$, its image under $f$ is defined as $f_!(S_i) = \{ f(s) \in T \lvert s \in S_i \}$. 
:::

Likewise, given a subset of targets, we can ask about the *pre-image* - the set of sources that could have produced those targets. That is:

::: {.definition name="Pre-image"}
For some subset $T_i \subseteq T$, its pre-image under $f$ is defined as $f^*(T_i) = \{ s \in S \lvert f(s) \in T_i \}$. 
:::

An important fact to note is that, although the pre-image $f^*$ is also sometimes called the "inverse image", it is *not* the inverse of the image $f_!$, for most functions (ones which are not bijections). That is, by applying the pre-image after image or vice versa, we cannot expect to always come up with the same set as we started with. Specifically, if we have a non-injective function and apply the pre-image after the image, we may come up with *more* sources that we started with, $S_i \subseteq f^*(f_!(S_i))$ (equality if injective), and similarly, if we have a non-surjective function and apply the image after the pre-image, we might end up with *fewer* targets than we started with, $f_!(f^*(T_i)) \subseteq T_i$ (again, equality if surjective). 

As an example, suppose again I have the function $f$ which maps each friend to a holiday destination. The image of that function, $f_!$, maps a set of friends to the set of all cities that at least one of them went to, and similarly, the pre-image, $f^*$, maps a set of cities to the set of friends that went to them.  
Now, suppose that Sam and Dominic went to Rome, and I ask:

> *"who went to [the city that Sam went to]?"*

I will get both Sam and Dominic back, since:

$$f^*(f_!(\{ Sam \})) = f^*(\{ Rome \}) = \{ Sam, Dominic \}$$

That is, I will get back Sam and Dominic *even though I had initially only asked about Sam*. Similarly, if no friends had visited Paris and I ask:

> *"what are the cities that [people who went to Paris or Rome] went to?"*

then I will get Rome only, since 

$$f_!(f^*(\{Paris, Rome \})) = f_!(\{ Sam, Dominic \}) = \{ Rome \}$$

This odd relationship between the the image and the pre-image is due to the fact that the image is actually something called *left adjoint* [@baez2023; @fong2019]. Adjoints can be thought of as the "best approximate answer to a problem that has no solution" [no inverse, @baez2023], and they come in pairs - a left and a right adjoint - with the left adjoint being more permissive or "liberal" and the right adjoint being more strict or "conservative" [@baez2023].

Proper treatment of adjoints is beyond the scope of this thesis, however.

### Partitions

Another interesting simple mathematical constructions are partitions. Like functions, partitions are a type of relation, and can in fact be constructed using functions. That is, if we have a "labeling" function $f$, we can construct a partition as follows:

::: {.definition name="Function definition of a partition"}
Given some set $X$, a set of part labels $P$, and a surjective function $f: X \to P$, we can partition $A$ by assigning every element $x \in X$ a part label $p \in P$, by simply applying the function: $f(x) = p$.
:::

We can also define partitions using equivalence classes. By taking any part label $p \in P$, we can recover the corresponding subset of $X$ by pulling out its pre-image: $f^*(\{p\}) = X_p \subseteq X$. We can then define a partition without reference to $f$:

::: {.definition name="Equivalence class definition of a partition"}
A partition of $A$ consists of a set of part labels $P$, such that, for all $p \in P$, there is a non-empty subset $A_p \subseteq A$ which forms an equivalence class on $A$ and:

$$X = \bigcup_{p \in P} X_p \qquad \text{and} \qquad \text{if } p \neq q, \text{ then } X_p \cap X_q = \varnothing$$
I.e. the parts $X_p$ jointly cover the entirety of $X$ and parts cannot share any elements.
:::

We can rank partitions by their coarseness. That is, for any set $X$, the coarsest partition is one with only one part label $P = \{ 1 \}$, such that each element of $X$ gets assigned $1$ as label. Conversely, the finest partition is one where each element gets assigned its own unique part label, such that $\lvert X \lvert = \lvert P \lvert$. 

Given two partitions, we can form a finer (or at least as fine) partition by taking their intersection, i.e. by taking the set of all unique pairs of labels that co-occur for any $x \in X$ as the new part labels. For example, suppose $X = \{ 1, 2, 3 \}$ and partition 1 assigns part labels:

$$p_1(x) = \begin{cases} 
a & \text{if } x = 1 \text{ or } x = 2 \\
b & \text{if } x = 3
\end{cases}$$

and partition 2 assigns part labels the following way:

$$
p_2(a) = \begin{cases}
s & \text{if } x = 1 \\
t & \text{if } x = 2 \text{ or } x = 3
\end{cases}
$$

Then the intersection partition will have the following part labels $P_3 = \{ (a, s), (a, t), (b, t) \}$ such that:

$$
p_3(a) = \begin{cases}
(a, s) & \text{if } x = 1 \\
(b, s) & \text{if } x = 2 \\ 
(b, t) & \text{if } x = 3
\end{cases}
$$

### Preorders

Another important class of relations are ones that have to do with order. Among these, one of the simplest constructions is a preorder:

::: {.definition name="Preorder"}
A preorder is a set $X$ equipped with a binary relation $\leq$ that conforms to the following two properties:

1. *Reflexivity*: $x \leq x$ for all $x \in X$
2. *Transitivity*: if $x \leq y$ and $y \leq z$, then $x \leq z$, for all $x, y, z \in X$
:::

Simply speaking, this means that, if we pick any two elements in the set $X$, they either relate and one element is "less than or equal to" the other, in some sense, or they do not relate at all.

One simple example of a preorder is the family tree, see Figure \@ref(fig:family-tree). Here, the underlying set is the family: $X = \{  daughter, son, mother, father, grandmother, ... \}$ and the binary relation is ancestry or familial relation. Thus, for example, $daughter \leq father$, since the daughter is related to (is offspring of) the father, and $father \leq father$, since a person is related to themselves (for the sake of this example). However, there is no relation ($\leq$) between $father$ and $mother$ since they are not related. Finally, since $daughter \leq father$ and $father \leq grandmother$, then, by reflexivity, $daughter \leq grandmother$.

```{r family-tree, engine='tikz'}
#| echo: false
#| fig-height: 3
#| fig-cap: "An example of a simple preorder: family tree ordered by familial relation."

\begin{tikzpicture}

  \filldraw[white] (-5, -1) rectangle (4, 5);

  \node (s) at (-1, 0) [draw] {Son};
  \node (d) at (1, 0) [draw] {Daughter};
  \node (f) at (-1, 2) [draw] {Father};
  \node (m) at (1, 2) [draw] {Mother};
  \node (g) at (-1, 4) [draw] {Grandmother};

  \draw[->] (s.north) to (f.south);
  \draw[->] (s.70) -- (m.250);
  \draw[->] (d.110) -- (f.290);
  \draw[->] (d.north) -- (m.south);
  \draw[->] (f.north) -- (g.south);

\end{tikzpicture}
```{}
```

Another common example of a preorder is the set of natural numbers $\mathbb{N}$, ordered by the usual order relation, or by the division relation: $x \leq y$ iff $x \mid y$ ($x$ divides $y$ without remainder).

#### Specializing preorders

We can specialize preorders by imposing additional properties, such as:

3. If $x \leq y$ and $y \leq x$, then $x = y$ (anti-symmetry)
4. Either $x \leq y$ or $y \leq x$ (comparability)

If a preorder conforms to property 3, we speak of a partially ordered set or *poset*. If it conforms to both 3 and 4, then it is called a *total order*. 

#### Preserving structure: Monotone maps

Preorders are interesting because they give us a first taste of something will be discussed a lot throughout this thesis: structure-preserving maps. Specifically, if we have two preorders $(X, \leq_X)$ and $(Y, \leq_Y)$, and a set of functions $F$ which map values between $X$ and $Y$, we can classify the functions in $F$ by whether they preserve the order in $X$ or not. That is, we call a function $f \in F$ order-preserving or a "monotone map", if:

::: {.definition name="Monotone map"}
A monotone map $f: X \to Y$ is a function between two preorders $(X, \leq_X)$ and $(Y, \leq_Y)$, such that, for all $x_1, x_2 \in X$:

$$\text{if} \;\; x_1 \leq_X x_2 \;\; \text{then} \;\; f(x_1) \leq_Y f(x_2) $$
:::

For example, suppose we have a set of functions mapping between the set of natural numbers ordered by the usual order relation, $(\mathbb{N}, \leq)$, and the set of reals, again ordered by the usual order relation $(\mathbb{R}, \leq)$. Then the function $f(x) = \log(x)$ is a monotone map, since:

$$\text{if} \;\; x_1 \leq_{\mathbb{N}} x_2 \;\; \text{then} \;\; \log(x_1) \leq_{\mathbb{R}} \log(x_2)$$

However, there are many other types of functions which do not preserve order, e.g. $g(x) = \sin(x)$ or $h(x) = -x$. As another example, if we take as our domain the powerset of some set $X$, ordered by inclusion relation, $(\mathcal{P}(X), \leq)$, then one order preserving map is the cardinality of the sets.

Monotone maps compose: if $f: X \to Y$ is a monotone map, and $g: Y \to Z$ is a monotone map, then their composite $h: X \to Z$ is also a monotone map.

### Monoids

In the previous subsection, we discussed one example of taking a set and imposing some kind of structure on it: namely, we took a set and imposed an order relation on it and called the result a preorder. However, we can impose many other kinds of structure on collections of objects (sets). One such type of a structure is a monoid.  

A monoid is an algebraic structure that represents a "whole equal to the sum of its parts", if we relax our idea about what it means to "sum". More formally:

::: {.definition name="Monoid"}
A monoid is a tuple $(M, e, \otimes)$ consisting of:

- A set of objects $M$
- A neutral element $e$ called the *monoidal unit*
- A binary operation (function) $\otimes: M \times M \to M$ called the *monoidal product*

Such that the binary operation $\otimes$ has the following properties:

1. Unitality: $m \otimes e = e \otimes m = m$ for all $m \in M$
2. Associativity: $m_1 \otimes (m_2 \otimes m_3) = (m_1 \otimes m_2) \otimes m_3 = m_1 \otimes m_2 \otimes m_3$ for all $m_1, m_2, m_3 \in M$
:::

In simple terms, when we have a monoid $(M, \otimes, e)$, we have some elements $m \in M$ and a way to combine them, such that, when we combine the same group of elements, we always get back the same result, no matter in what order we do it in (associativity: brackets do not matter). We also have some neutral element $e$ that, when combined with any other element, does nothing and simply yields back the original element. 

::: {.theorem name="Uniqueness of the neutral element"}
The neutral element in a monoid is always unique. 
:::

Proof: suppose $e_1$ and $e_2$ are elements in $M$ that have the unital property. Then $e_1 \otimes e_2 = e_1$ but also $e_1 \otimes e_2 = e_2$ (treating either as "the" neutral element). So, $e_1 = e_2$.

#### Simple examples of monoids

One common example of a monoid is summation on natural numbers (including zero), $(\mathbb{N}, 0, +)$:

\begin{aligned}

1 + 0 = 0 + 1 = 1 & \qquad \text{(unitality)} \\
1 + (2 + 3) = (1 + 2) + 3 = 1 + 2 + 3 & \qquad \text{(associativity)}

\end{aligned}

Another example of a monoid are products of real numbers $(\mathbb{R}, 1, \times)$:

\begin{aligned} 

1 \cdot 2 = 2 \cdot 1 = 2 & \qquad \text{(unitality)} \\
1 \cdot (2 \cdot 3) = (1 \cdot 2) \cdot 3 = 1 \cdot 2 \cdot 3 & \qquad \text{(associativity)}

\end{aligned}

However, not every mathematical operation forms a monoid. One counterexample is exponentiation. Exponentiation does not meet the definition of a monoid, since it is not associative:

$$x^{(y^z)} \neq (x^y)^z$$

and there is no two-sided neutral element:

$$x^1 = x \qquad \text{but} \qquad 1^x \neq x$$

Likewise, the operation of taking an average of two numbers is not associative:

$$\frac{\frac{x + y}{2} + z}{2} \neq \frac{x + \frac{y + z}{2}}{2} \\$$

And there is no neutral element since there is no number that we could average $x$ with to get back the same value (that does not depend on $x$):

$$\not\exists c \; \text{s.t.} \; \frac{x + c}{2} = x$$

Therefore, the average operator is not a monoid either.

#### Beyond numbers

However, the definition of a monoid is broader than just simple operations on numbers, and can extend to far more exotic structures. For example, multiplication of $n \times n$ square matrices $(\mathbf{M}_{n \in \mathbb{Z}}, \mathbf{I}, \cdot)$, is a monoid. Also, the operation of appending a value to a vector and taking the Euclidean norm can too be recast as a monoid [@stepanov2009]:

$$||(||(x, y)||_2, z)||_2 = \sqrt{\bigg(\sqrt{(x^2 + y^2)}\bigg)^2 + z^2} = \sqrt{(x^2 + y^2) + z^2} = ||(x, y, z)||_2$$

Even completely non-number like entities can form monoids. For example, the operation of concatenating strings is a monoid, since it is associative and comes equipped with a neutral element (the empty string):

\begin{aligned}

\text{"hello"} + \text{""} = \text{""} + \text{"hello"} = \text{"hello"} & \qquad \text{(unitality)} \\ 
(\text{"quick"} + \text{"brown"}) + \text{"fox"} = \text{"quick"} + (\text{"brown"} + \text{"fox"}) & \qquad \text{(associativity)}

\end{aligned}

#### Specializing monoids

As with [preorders](#Preorders), we can make more specialized version of monoids by imposing additional properties. One such property is commutativity:

3. Commutativity: $m_1 \otimes m_2 = m_2 \otimes m_1$ for all $m_1, m_2 \in M$

Both associativity and commutativity can both be viewed as saying "order does not matter" in some sense, however, they are fundamentally different. While associativity is about the "temporal" order of operations, commutativity is about the "spatial" order of terms. Let's illustrate this on an example. 

Suppose we have three wires of different colours $\{ \text{red}, \text{green}, \text{blue} \}$. We can connect these wires, and let's call this operation our monoidal product. Let's also imagine that the $\text{red}$ wire is connected to a power source and the $\text{blue}$ wire is connected to a light bulb, and the $\text{green}$ wire amplifies the current from the power source such that it is enough to power the light bulb. To turn on the lightbulb, we need to connect the wires in the following order: $\text{red} \to \text{green} \to \text{blue}$. The temporal order in which we do this does not matter: we can connect $\text{green} \to \text{blue}$ first and $\text{red} \to \text{green}$ second or vice versa, either way we get the same result (the lightbulb turns on). However, the spatial order in which we connect the wires *does* matter: if we connect $\text{red} \to \text{blue}$, then the current will not be enough to power the light bulb. Hence, the operation is associative (temporal order does not matter) but not commutative (spatial order matters).  

Further interesting kinds of structure can arise when the set $M$ is itself a part of a preorder $(M, \leq)$. Then, we may want the monoidal product to be monotonic, such that it does not break the ordering imposed by $\leq$:  

4. Monotonicity: $m_1 \leq m_1 \otimes m_2$ and $m_2 \leq m_1 \otimes m_2$ for all $m_1, m_2 \in M$

This means that when we combine two things, we get back something that's at least as big as the bigger of the two things. Summation of natural numbers $(\mathbb{N}, \leq, 0, +)$ again works, but for example summation of integers $(\mathbb{Z}, 0, +)$ or multiplication of reals $(\mathbb{R}, \leq, 1, \times)$ does not. 

Mathematicians tend to give these structures with different sets of properties different names. For example, the tuple $(M, \leq, e, \otimes)$, where the monoidal product $\otimes$ has the properties of unitality, associativity, commutativity, and monotonicity is called a symmetric monoidal preorder [@fong2019]. For our purposes here, I will not dive too deep into this taxonomy, however, interested reader should consult @fong2019 or [nLab](https://ncatlab.org/nlab/show/HomePage).   

#### Structure preserving maps: Monoid homomorphisms

As with preorders, when we have functions which map from one monoid to another, we can ask whether they preserve properties we care about. Before, this was order; now, we want the operations to preserve the fundamental properties of the monoidal product, unitality and associativity:

:::{.definition name="Monoid homomorphism"}
Let $(M, e_M, \otimes_M)$ and $(N, e_N, \otimes_N)$ be monoids. A function $f: M \to N$ is called a monoid homomorphism if it:

1. Preserves product: $f(m_1 \otimes_M m_2) = f(m_1) \otimes_N f(m_2)$
2. Preserves unitality: $f(e_M) = e_N$
:::

(technically, 2. can be deduced from 1., if we let $m_1$ or $m_2$ equal $e_M$)

To give an example of a monoid homomorphism, suppose that our first monoid is string concatenation, $(String, "", ++)$ and the second is natural numbers with addition, $(\mathbb{N}, 0, +)$. Then, one simple monoid homomorphism $f$ is simply counting the number of characters in a string:

$$10 = f(\text{"helloworld"}) = f(\text{"hello"} \text{++} \text{"world"}) = f(\text{"hello"}) + f(\text{"world"}) = 5 + 5$$
$$e_N = 0 = f("") = f(e_M)$$

### Groups

Another well-known example of algebraic structures are groups. Groups are studied in group theory, and encode the idea of reversible transformations. Despite the different term, groups are really just a monoid with one additional property.

:::{.definition name="Group"}
A group is a monoid with an inverse operator. Specifically, as with a monoid, we have the tuple $(M, e, \otimes)$, which includes the set $M$, the neutral element $e$, and the product $\otimes$, and the product fullfills the following properties:

1. Unitality: $m \otimes e = e \otimes m = m$, for all $m \in M$
2. Associativity: $m_1 \otimes (m_2 \otimes m_3) = (m_1 \otimes m_2) \otimes m_3 = m_1 \otimes m_2 \otimes m_3$, for all $m_1, m_2, m_3 \in M$

Additionally, the monoidal product $\otimes$ has an inverse $\otimes^{-1}$, such that the following property holds^[Note, that in group theory, it is common to omit explicit references to the operator and write the group products without it, such that, e.g. $m \otimes n$ is written as $mn$. In that case, instead of the inverse *operator*, we speak of inverse *elements*, such that, for each $m \in M$ there is an $m^{-1}$ such that $m^{-1}m = mm^{-1} = e$. However, this really is a distinction without difference since we can easily define the inverse element using the inverse operator and the neutral element: $m^{-1} = (e \otimes^{-1}m)$. Thus, for the sake of keeping with the notation in the previous sections, I will use the inverse operator explicitly.]:

3. Invertibility: $m \otimes^{-1} m = e$, for all $m \in M$
:::



### Categories

While discussing mathematical structures like preorders, monoids, and groups, some common have been cropping up: namely, structure and structure-preserving maps. Now it is time to shift gears and define these common themes more generally. To do this, we can make use of one powerful concept: categories [@fong2019; @lawvere2009].

:::{.definition name="Category"}

To define a category $\mathcal{C}$, we specify:

- A collection of objects $\text{Ob}(\mathcal{C})$
- For every two objects $c, d \in \text{Ob}(\mathcal{C})$, we define a set of morphisms (arrows) $\mathcal{C}(c, d)$ 
- For any object $c \in \text{Ob}(\mathcal{C})$, we define a special morphism $\text{id}_c \in \mathcal{C}(c, c)$, called the identity morphism
- For every three objects $c_1, c_2, c_3 \in \text{Ob}(\mathcal{C})$ and two morphisms $f \in \mathcal{C}(c_1, c_2)$ and $g \in \mathcal{C}(c_2, c_3)$, we define a composite morphism $f ⨾ g \in \mathcal{C}(c_1, c_3)$  

Such that the composition operation is:

1. Unital: $\text{id}_{c_1} ⨾ f = f$ and $f ⨾ \text{id}_{c_2}=f$
2. Associative: $(f ⨾ g)⨾ h = f ⨾ (g ⨾ h) = f ⨾ g ⨾ h$

(for all $f \in \mathcal{C}(c_1, c2)$, $g \in \mathcal{C}(c_2, c3)$, and $f \in \mathcal{C}(c_3, c4)$, and $c_1, c_2, c_3, c_4 \in \text{Ob}(\mathcal{C})$)

:::

The definition above may look daunting, because of the introduction of new notation and the use of abstract terms like "collection", "objects", and "morphisms". However, it is really nothing new. To simplify things, we can think of the collection of objects as a set, and the morphisms/arrows as functions, and we retain most of the original meaning; the abstract definition above simply gives us more flexibility.

In other words, all that the definition of a category says is that we have some objects $c \in \text{Ob}(\mathcal{C})$ which relate to each other via morphisms, and these morphisms obey some common sense properties. First, the morpshisms compose: if we can get from $c_1$ to $c_2$ and $c_2$ to $c_3$, we can get from $c_1$ to $c_3$. Second the composition is associative, meaning that the order in which we compose does not matter. Finally, we always have a way of "staying in the same place", which is the identity morphism.  


### Functors

---
title: "The Algebra of Graphics, Statistics, and Interaction"
subtitle: "Towards Fluent Data Visualization Pipelines"
author: "Adam Bartonicek"
site: bookdown::bookdown_site
documentclass: book
always_allow_html: true
output:
  bookdown::gitbook:
    css: styles.css
  bookdown::pdf_document2:
    latex_engine: xelatex
  bookdown::pdf_book:
    latex_engine: xelatex
bibliography: [references.bib]
biblio-style: "apalike"
link-citations: true
editor_options: 
  chunk_output_type: console
header-includes:
  - \usepackage{tikz-cd}
---

# Abstract 

Placeholder



<!--chapter:end:index.Rmd-->

# Introduction {#introduction}

> It’s written here: ‘In the Beginning was the Word!’ <br>
> Here I stick already! Who can help me? It’s absurd, <br>
> [...] <br>
> The Spirit helps me! I have it now, intact. <br>
> And firmly write: ‘In the Beginning was the Act!
>
> Faust, Part I, Johann Wolfgang von Goethe [-@goethe2015]

Humans are intensely visual creatures. About 20-30% of our brain is involved in visual processing [@van2003; @sheth2016], utilizing a highly sophisticated and powerful visual processing pipeline [see e.g. @goebel2004; @knudsen2020; for a brief review, see @ware2019]. It is well-established the brain can process certain salient visual stimuli in sub-20-millisecond times, outside of conscious attention [@ledoux2000; @ledoux2003], and that people can make accurate, parallel, and extremely rapid visual judgements, in phenomena known as subitizing and pre-attentive processing [@mandler1982; @treisman1985]. These features make the visual cortex the most powerful information channel that humans possess, both in terms of bandwidth and throughput.

Statisticians have known about this power of visual presentation for a long time. Starting with early charts and maps, data visualization co-evolved alongside mathematical statistics, offering an alternative and complementary perspective [for a review, see @friendly2006 or Section \@ref(brief-history)]. While mathematical statistics tended to focus on confirmatory hypothesis testing, data visualization provided avenues for unsupervised exploration, "forcing us to notice that which we would never expect to see" [@tukey1977]. Eventually, this valuable role of forcing us to see the unexpected established data visualization as a respected tool within the applied statistician's toolkit.

Seeing an object from a distance is one thing, but being able to also touch, manipulate, and probe it is another. Within the human brain, action and perception are not independent, but are instead intricately linked, mutually reinforcing processes [see e.g. @dijkerman2007; @lederman2009]. Beginning in the 1970's, statisticians acquired a new set of tools for exploiting this connection. The advent of computer graphics and interactive data visualization transformed the idea of "interrogating a chart" from a mere turn of phrase into tangible reality. All of a sudden, it became possible to work with the visual representation of data in a tactile way, getting new perspectives and insights at the stroke of a key or click of a button. 

This compelling union of the visual and the tactile has made interactive data visualization a popular method of presenting data. Nowadays, there are many packages and libraries for building interactive data visualizations across all the major data analytic languages. Interactive figures make frequent appearance in online news articles and commercial dashboards. However, despite this apparent popularity, significant gaps remain in the use and understanding of interactive visualizations. Individual analysts rarely utilize interactive data visualization in their workflow [see e.g. @batch2017], the availability of certain more sophisticated features is fairly limited (see Section \@ref(background)), and researchers still point to a lack of a general interactive data visualization pipeline [@wickham2009; @vanderplas2020]. 

This thesis explores these interactive data visualization paradoxes and the inherent challenges surrounding interactive data visualization pipelines more specifically. I argue that, contrary to some prevailing views, interactivity is not simply an add-on to static graphics. Instead, interactive visualizations must be designed with interactivity as a primary consideration. Furthermore, I contend that certain interactive features fundamentally influence the types of visualizations that can be effectively presented. My claim is that popular types of interactive visualizations exhibit a particular kind congruence between graphics, statistics, and interaction, and that the absence of this congruence results in suboptimal visualizations. I formalize this congruence using the framework of category theory. Finally, I validate these theoretical concepts by developing an open-source interactive data visualization library and demonstrate its application to real-world data.

#### Thesis Overview

The thesis is organized as follows. Section \@ref(background) reviews the history of interactive data visualization and discusses general trends and issues in the field. Section \@ref(problems), focuses on specific problems encountered when designing an interactive data visualization pipeline. Section \@ref(goals), outlines the the goals and aims that guided the development of the interactive data visualization library. Section \@ref(system) details the system's components and design considerations. Section \@ref(applied-example), presents an applied example of exploring a real-world data set using the developed library. Finally, Section \@ref(discussion), discusses lessons learned and potential future research directions.

<!--chapter:end:introduction.Rmd-->


# Background {#background}

Placeholder


## Brief history of interactive data visualization {#brief-history}
### Static data visualization: From ancient times to the space age
### Early interactive data visualization: By statisticians for statisticians {#early-interactive}
#### Open-source Statistical Computing
#### Common features and limitations of early interactive systems
### Interactive data visualization and the internet: Web-based interactivity {#web-based}
#### D3
#### Plotly and Highcharts
#### Vega and Vega-Lite
#### Common features and limitations of web-based interactive systems
## What even is interactive data visualization? {#what-is-interactive-visualization}
### Interactive vs. interacting with {#interactive-interacting}
### Interactive *enough*? {#interactive-enough}
### Complexity of interactive features {#complexity-of-features}
### Working definition
### Common interactive features {#common-features}
#### Changing size and opacity
#### Zooming and panning
#### Querying
#### Sorting and reordering
#### Parametric interaction
#### Animation and projection
#### Representation switching {#representation-switching}
#### Linked selection {#linked-selection}
## General data visualization theory
### Visualization goals {#visualization-goals}
### Visual perception {#visual-perception}
### Scales and measurement {#scales-measurement}
### Graphics formats {#graphics-formats}
#### Raster graphics
#### Vector graphics

<!--chapter:end:litreview.Rmd-->


# Challenges {#problems}

Placeholder


## The structure of this chapter: Data visualization pipeline
## Partitioning {#partitioning}
### Showing the full data {#show-all-data}
### Comparison and disjointness {#comparison-disjointness}
#### Naturality of disjointness
#### Disjointness and bijections
#### Disjointness in visualizations: Real-world example
#### Disjointness and interaction {#disjointness-interaction}
### Plots as partitions
#### Bijection on cases vs. bijection on subsets
#### Products of partitions {#products-of-partitions}
#### Limits of flat product partitions
### Partitions, hierarchy, and preorders {#hierarchy}
#### Plots as preorders {#plots-as-preorders}
#### The graph behind the graph
## Aggregation
### The relationship between graphics and statistics
#### Independence: The grammar-based model
#### Motivating example: Limits of independence {#stacking-not-graphical}
#### Some statistics are stackable but others are not {#stackable-or-not}
#### Advantages of stacking: Part-whole relations {#stacking-part-whole}
### Stackable summaries: A brief journey into Category Theory {#aggregation-category-theory}
#### Past applications of category theory to data visualization {#visualization-category-theory}
#### Generalizing preorders: Categories {#preorders-categories}
#### Structure preserving maps: Functors
#### Aggregation: A functor from the preorder of data subsets to the preorder of summary statistics {#aggregation-functor}
#### Functorial summaries and set union
#### Whole equal to the sum of its parts: Monoids
#### Programming with monoids {#programming-with-monoids}
#### Groups and inverses {#groups-inverses}
#### Other properties: monotonicity and commutativity
##### Monotonicity
##### Commutativity
#### Transforming summaries: Stacking, normalizing, and shifting {#transforming-summaries}
##### Stacking
##### Normalizing
##### Shifting
## Scaling and encoding {#scaling}
#### Scales as functions {#scales-as-functions}
#### Limits of modeling scales with simple functions {#simple-scale-limits}
#### Solution: Scales as function composition {#scales-composition}
##### Reusability and discrete scales
##### The intermediate interval {#scaling-intermediate}
##### Implementing scale features via the intermediate interval
###### Margins
###### Panning
###### Zooming
##### Inverses {#scaling-inverses}
##### Scale transformations
#### Comparison to past implementations of scales {#scales-comparison}
## Rendering
### Frames {#frames}
### Graphical elements

<!--chapter:end:problems.Rmd-->


# Goals {#goals}

Placeholder


## User profile
## Programming interface
## User interface
## Interactive features

<!--chapter:end:goals.Rmd-->


# High-level design

Placeholder


### User profile
## Programming paradigm
### Imperative programming
### Functional programming
### Object oriented programming
#### Abstraction
#### Encapsulation
#### Polymorphism
#### Inheritance
#### Domain-driven design
#### Criticism of OOP
### Data oriented programming
#### Data first
#### The data
#### The code
##### Encapsulation
##### Inheritance
## Reactivity
## Data representation
#### Row-based vs. column-based
#### Performance
## Rendering engine

<!--chapter:end:design.Rmd-->


# System description {#system}

Placeholder


## Core requirements
## High-level API (`plotscaper`) {#high-level-api}
### API design {#api-design}
### Basic example {#basic-example}
#### Figure vs. plot and selectors {#figure-plot}
#### Variable names
#### Variables and encodings {#variables-encodings}
### The scene and the schema {#scene-and-schema}
### Client-server communication {#communication}
### HTML embedding
## Low-level implementation (`plotscape`) {#low-level-implementation}
### Reactivity
#### Observer pattern {#observer}
#### Streams
#### Virtual DOM
#### Signals {#signals}
#### Reactivity in `plotscape` and final thoughts {#reactivity-solution}
### System components
#### Indexable {#Indexable}
#### Getter {#Getter}
#### Dataframe
#### Factors
##### Bijection and constant factors
##### Discrete factors
##### Binned factors
##### Product factors
#### Marker
##### Transient vs. persistent selection {#transient-persistent}
##### Group assignment indices {#group-indices}
##### Updating group assignment indices
#### Aggregation: Reducers, reduceds, and summaries {#aggregation-summaries}
##### Reducers
##### Reduced
##### Summaries {#summaries}
#### Scales {#scales}
##### Scale properties
#### Expanses
#### Continuous expanses
#### Point expanses
#### Band expanses
##### Compound and split expanses
#### Plot {#plot}
##### Data {#plot-data}
##### Scales {#plot-scales}
##### Rendering {#plot-rendering}
#### Events and interactive features
##### Activation and deactivation
##### Growing/shrinking objects
##### Mouse-move interaction: selection, panning, and zooming {#plot-mousemove}
####### Selection
####### Panning
####### Querying
#### Zooming
#### Scene {#scene}
##### Plots
##### Marker
##### WebSockets client
##### Events and Keybindings

<!--chapter:end:system.Rmd-->


# Applied example {#applied-example}

Placeholder


### About the data set
### Interactive exploration
#### The relationship between cases and days
#### Number of cases over time
#### Age and child and adolescent mental health
#### Prevalence of diagnoses
#### Prevalence of diagnoses over time
#### Characteristics of patient cohorts over time
## Summary

<!--chapter:end:example.Rmd-->

# Discussion

This thesis explored the role of interaction in data visualization pipelines. More specifically, I investigated how interaction affects the four stages of data visualization pipelines - partitioning, aggregation, scaling, and rendering - and explored the inherent problems and challenges. The core argument was that the popular model implied by the Grammar of Graphics [@wilkinson2012], which treats statistics and geometric objects as independent entities, is insufficient for describing the complex relationships between the components of interactive figures [see also @wu2024]. As an alternative, I proposed a simple category-theoretic model, conceptualizing the data visualization pipeline as a functor. 

The essence of the proposed model is as follows. Ultimately, when we visualize, we want to represent our data with a set of geometric objects. To do this, we need to break our data into a collection of subsets. This collection of subsets is not arbitrary, but, most of the time, has a very specific kind of structure: a hierarchy of data partitions ordered by set union. To maintain consistency during interactions like linked selection, the subsequent steps of the data visualization pipeline should preserve this structure. In plain words, the geometric objects in our plots and the underlying summary statistics should *behave like set union*. In category theoretic terms, this means that the mapping from data subsets to summary statistics should be a functor, and so should be the mapping from summary statistics to geometric objects. Further, using the properties of set union, we can identify specific algebraic structures that our statistics and objects should conform to: groups and monoids. Specifically, to behave consistently, the operations underlying our plots should be associative and unital, and potentially also invertible, monotonic, and commutative. When these algebraic constraints are satisfied, the geometric objects in our plots will compose well, meaning that their parts will add up to a meaningful whole, and this makes linked selection consistent.     

To test the proposed model, I developed `plotscaper`, an interactive data visualization R package. In fact, this implementation served as a crucial feedback loop for the development of the model, as many of the theoretical concepts emerged from practical challenges that I encountered during the design of the system. By translating theory into code, I was able to empirically test and refine assumptions about the structure and behavior of interactive data visualizations. 

However, `plotscaper` was also developed to provide a practical tool for data exploration, not just theory testing. As outlined in Section \@ref(background), within the R ecosystem, there is currently no shortage of interactive data visualization packages and frameworks; however, many offer only fairly basic interactive features out of the box. Implementing complex features such as linked selection, representation switching, or parametric interaction (see section \@ref(common-features)) often requires substantial programming expertise and time-investment, creating a barrier to entry for casual users [see e.g. @batch2017]. Thus, a secondary goal of this project was to try to address this perceived gap, and this goal seems to have been met with moderate success. Despite its experimental status and the fact that it is competing with a number of far larger and better-established interactive data visualization frameworks, `plotscaper` has been downloaded over `r cranlogs::cran_downloads("plotscaper", from = "2024-10-01", to = Sys.Date())$count |> sum()` times^[The number only includes downloads from the RStudio CRAN mirror.], in the `r as.numeric(Sys.Date() - as.Date("2024-10-19"))` days since its initial release.

Despite all of these relative successes, both the theoretical model and its practical implementation in `plotscaper` have, of course, their limitations. These will be the subject of the next few sections.
 
## Limitations of the theoretical model

First, it is important discuss the limitations of the theoretical model presented in this thesis. This model, described in Section \@ref(problems), conceptualizes the data visualization pipeline as a structure-preserving mapping, also known as a functor. Specifically, a key initial assumption is that, when visualizing data, we start with a hierarchy of data subsets. These subsets are disjoint (share no overlapping data) and are ordered by set inclusion/union. To produce meaningful graphics, the subsequent steps of the data visualization pipeline should preserve this inherent structure. In plain words, the transformations of data into summary statistics and of statistics into aesthetics should also *behave like set union*. In category theoretic terms, this means that the summarizing function and the rendering process have to be functors. Based on the properties of set union, this naturally identifies algebraic structures as groups and monoids. 

This proposed model of the data visualization pipeline naturally raises several questions and potential criticisms, some of which have been already discussed in Section \@ref(problems). These will be further reviewed and expanded upon in the following subsections. My objective is to clearly articulate the model's advantages, while also acknowledging its limitations and suggesting opportunities for future work.

### Why the model is necessary

First, let's briefly reiterate why the model is even necessary. As discussed in Section \@ref(partitioning), when visualizing data, we render geometric objects representing parts of our data set. We establish these parts by conditioning on discrete variable, such as a categorical variable in a barplot or a binned continuous variable in a histogram. Often, we also want to further subdivide these parts by conditioning on another variable or variables. In particular, in interactive data visualization, this nested conditioning is a pre-requisite of linked selection, which naturally partitions the data according to the selection status.

After being subdivided into parts, the data is summarized via summary statistics and rendered as geometric objects. Importantly, the rendered objects should reflect the hierarchical part-whole structure created by the partitioning. This is the case with techniques like stacking, which represent the subdivided data subsets as highlighted parts of whole objects. While alternative techniques such as dodging or layering do exist, they provide a subpar solution for interactive graphics (see Section \@ref(stacking-part-whole)). 

However, a key insight is that the graphical representation is *not* independent of the way we summarize our data. Specifically, as discussed in Section \@ref(stackable-or-not), regarding stacking, past data visualization researchers have noted the fact that while some summary statistics such as sums or counts can be effectively stacked, others like averages or quantiles cannot [see e.g. @wilke2019; @wills2011; @wu2022]. For instance, while the sum of sums or maximum of maximums represent valid overall statistics, the average of averages is different from grand mean. To formally describe this property of "stackability", we can leverage some fundamental ideas from category theory/abstract algebra.

By grounding our reasoning in this algebraic model, we can quickly identify which combinations of statistics and geometric objects will behave well under linked selection. For instance, since we know that the average operator is not monoidal, we know that it will not be stackable and will pose certain challenges if we try to implement it alongside linked selection. Further, the model also allows us to identify what *kinds* of linked selection will work. For instance, while the maximum and the convex hull operators are monoids, they both lack inverse (they are not groups, in the algebraic sense), and thus we know that they will be able to display the selection of one highlighted group, however, they will not work for comparisons of multiple selection groups (see Section \@ref(groups-inverses)). 

### Disjointness

A point which is important to discuss is that the partition hierarchy model does not describe *all* types of plots in use today. For instance, it precludes visualizations where geometric objects within the same graphical layer represent overlapping data subsets, as seen, for example, in certain visualizations of set-typed data [see e.g. @alsallakh2014] or two-dimensional kernel density plots (see Section \@ref(comparison-disjointness)). However, these visualizations represent a fraction of available plot types. In the vast majority of "standard" plots - including barplots, histograms, scatterplots, density plots, heatmaps, violin plots, boxplots, lineplots, and parallel coordinate plots - each geometric object represents a distinct subset of cases^[As a short exercise, I tried going through `ggplot2`'s list of [`geoms`](https://ggplot2.tidyverse.org/reference/) and identifying all non-disjoint data representations. The only examples I have been able to find have been find were `geom`s based on two-dimensional kernel density estimates, i.e. `geom_density_2d` and `geom_density_2d_filled` (each line or polygon represents densities computed across all datapoints).].

Further, in Section \@ref(comparison-disjointness), I argued that this tendency towards disjoint data representations is not a mere convention or accident, but instead stems from a fundamental naturality of disjointness. Disjointness underlies many fundamental concepts in statistics, programming, and mathematics, and, in general, seems to align better with our cognitive processes (see Section \@ref(comparison-disjointness)). Put simply, reasoning about objects which are distinct is easier than about objects which are entangled [see also @hickey2011].

Thus, while not the only option, I argue that disjointness provides a good default model for data visualization, just as it does in the other fields mentioned above. Furthermore, disjointness seems to be particularly well-suited to interactive data visualization, and, conversely, with non-disjoint data representations, many interactive features may prove difficult or even impossible to implement. For instance, how would one implement non-trivial linked selection with a two-dimensional kernel density plot, without referring to the underlying (disjoint) data points? To summarize, in general, objects which represent distinct data entities are far easier to reason about and manipulate interactively.

### Associativity and unitality

The core idea of the model is preservation of the properties of set union. Among these, the two most important ones are associativity, allowing for arbitrary ordering of operations, and unitality, guaranteeing the existence of a neutral element. Together, these two primary properties identify monoids as the fundamental algebraic structure.

This connection between set union and monoids is not novel. For instance, monoids have long been known to have desirable properties for parallel computing [see e.g. @parent2018; @fegaras2017; @lin2013]. More broadly, monoids are also popular functional programming [see e.g. @milewski2018], and form the basis of certain algorithms in generic programming [@stepanov2009; @stepanov2013]. Finally, monoids and other concepts from category theory have also been used in certain areas of relational database theory [see e.g. @fong2018; @gibbons2018]

However, as far as I am aware, I am the first to make the connection between monoids and many popular visualization types. As discussed in Section \@ref(visualization-category-theory), the model described in the present thesis differs significantly from past applications of category theory to data visualization, being on one hand more applied than some applications (it relates to concrete statistics, geometric objects, and interactive features) and more theoretical than others (not a functional programming library). 

Interestingly, the challenges we face we face when visualizing data are in some ways remarkably similar to the parallel computing: often, we want to break the data into parts, compute some summaries, and the combine these summaries back together in a consistent way.

### Model constraints and utility

A potential point of contention is that the constraints which the model provides are too rigid. Specifically, requiring all plots to be groups or monoids excludes a significant fraction of popular visualizations. Further, many non-monoidal plot types, such as boxplots, have been successfully implemented alongside linked selection in the past [see e.g. @theus2002; @urbanek2011]. Thus, a valid question one might have is whether the limitations imposed by the model are justified by its practical utility.

However, I believe this is not an issue, since the model is not meant to be prescriptive. Instead, it simply provides a framework for identifying visualizations which are "naturally" compatible with features like linked selection, and we are free to violate this naturality whenever we see fit. However, we should be aware of the fact that reconciling non-monoidal visualizations with linked selection may require some ad hoc design decisions. For instance, since a boxplot box is not a monoid, there is no such thing as highlighting a "part of a boxplot box". We may solve this issue by e.g. plotting the highlighted cases as a second box next to the original one, however, then we lose the nice interactive properties of part-whole relations provided by monoidal plots (see Section \@ref(stacking-part-whole)). Thus, the model helps us to identify inherent trade-offs in the design and implementation of interactive graphics.    

## Limitations of the software

During this project, I had time to think through and test out different implementations of the various features of the interactive data visualization system. While I succeeded in some areas, there were also areas which could still use some improvement. 

### Declarative schemas

As discussed in Section \@ref(system), one limitation of the delivered system is the absence of a simple declarative schema-based approach for specifying plots. While declarative schemas, popularized by the Grammar of Graphics [@wilkinson2012], have become a highly popular in modern data visualization libraries, my system deviates from this paradigm. Although certain core components within `plotscape` are modeled declaratively (as shown in Section \@ref(summaries)), the overall plot definitions remain largely procedural, and the user-facing API (`plotscaper`) relies on nominal plot types^[Currently there are six different types of plots implemented.]. 

However, as I argued in Section \@ref(variables-encodings), this lack of a full declarative plot specification is not unique to my system. Instead, many currently available declarative data visualization libraries provide incomplete solutions, frequently concealing or obscuring key implementation details [see also @wu2024]. During the project, I did try to come up with a simple and convenient way of specifying declarative schemas for interactive graphics, however, ultimately I failed, and chose to simply make this implementation gap explicit. 

Specifying a complete declarative schema interactive data visualizations is inherently challenging, for several reasons. First, there is the lack of direct correspondence between data variables and visual encodings. Specifically, as discussed in Section \@ref(variables-encodings), often, what we plot *not* variables found in the original data, but instead variables which have been computed or derived in some way. This is exists even in static graphics; however, it is further amplified in interactive graphics, where dynamic remapping of variables to aesthetics is often desirable (such as when transforming a barplot to a spineplot). Thus, directly mapping data variables to aesthetics seems to be an inadequate approach. However, to map derived variables to aesthetics, we have to explicitly specify *what* we compute and *how*. For example, to fully specify a histogram, we have to describe the fact that we want to bin cases based on some continuous variable, compute the counts within bins, stack these counts, also within bins, and finally map the bin borders to the x-axis variable and the counts to the y-axis variable. While I have been able to get reasonably far with providing a mechanism for specifying declarative schema for this process (see Section \@ref(variables-encodings)), ultimately, I was not able to integrate it with the rest of the system without some amount of procedural code.

Second, a further challenge to declarative schemas in interactive data visualization stems from the hierarchical nature of graphics (see Section \@ref(hierarchy)). Specifically, while in static graphics, we can often act as if the data underlying our plots is "flat", interactivity necessitates hierarchical data structures. Interactive features like linked selection and parametric manipulation trigger updates to summary statistics, requiring hierarchical organization for efficient updates, especially with features like stacking and normalization (see Sections \@ref(transforming-summaries) and \@ref(aggregation-summaries)). Furthermore, hierarchical data can also be leveraged to provide more efficient scale and axis updates (see Section \@ref(plot-scales)), and display more information during querying (e.g., by displaying summary statistics for both objects and segments). However, specifying declarative schemas with hierarchical data is inherently more complex than with flat data, due to both increased volume (multiple data sets) and hierarchical dependencies.

Third and related issue which complicates declarative schemas is reactivity. Interactive visualizations need to, by definition, respond to user input. However, this input can impact various stages of the data visualization pipeline differently. For instance, while shrinking or growing the size of points in a scatterplot is a purely graphical computation, shrinking or growing the width of histogram bins requires recomputation of the underlying data. Consequently, the declarative schema should be able to handle reactive parameters which may have hierarchical dependencies. Signals (see Section \@ref(signals)) may offer a hypothetical general solution; however, as discussed in Section \@ref(reactivity-solution), my experience with developing `plotscape` revealed that reactivity typically tends to enter the visualization process at discrete points, corresponding to the four stages of the data visualization pipeline. While I did not manage to develop a general declarative mechanism for setting these parameters at these four stages, the following list outlines the identified points of reactivity:

- Pre-partitioning: Data streaming, partitioning parameter updates (e.g. histogram bin width, selection status)
- Pre-aggregation: Aggregation parameter updates (e.g. regression line regularization)
- Pre-scaling: Scale parameter updates (e.g. sorting, size/alpha adjustments)
- Pre-rendering: Surface-level changes (e.g. layout modifications, DOM resize events)

### Performance

A key aspect to discuss is the performance of the delivered software. As discussed in Section \@ref(background), responsiveness is crucial for interactive data visualization. Slow systems frustrate users, negating any potential benefits of sophisticated features. Given today's expectation of highly responsive GUIs and the growing size of data sets, performance is an important concern.

Despite not being the sole focus, I am happy to report that `plotscaper` achieves solid performance even on moderately sized data sets. Specifically, I was able to achieve highly responsive point-wise linked selection on data sets with tens of thousands of data points (such as the `diamonds` data set; see the [Performance vignette](https://bartonicek.github.io/plotscaper/articles/performance.html) on `plotscaper`'s package website). This performance is, of course, largely attributable to the highly optimized nature of JavaScript engines such as V8 rather than any targeted optimization. Nevertheless, I did try to be generally mindful of performance while developing the package, by adhering to data oriented practices such as relying on the Structure of Arrays (SoA) data layout.  

Profiling of the package revealed that the primary performance bottleneck was rendering. This manifests quite clearly even on the macro level: figures composed entirely of aggregate plots tend remain responsive even with fairly large data sets, whereas figures with multiple bijective plots (such as scatterplots or parallel coordinate plots) can start to become sluggish even with moderately-sized data sets (thousands or tens-of-thousands of data points). Therefore, alternative rendering strategies may offer significant performance gains. Currently, `plotscaper` relies on the default HTML5 `canvas` rendering context, which provides a simple interface for rendering 2D graphics. GPU-based rendering, particularly WebGL [@webgl2025], could lead to significant performance improvements.

Beyond rendering, there are other parts of the system that may provide opportunities for easy performance wins. For instance, currently, as mentioned in Section \@ref(plot-mousemove), querying and selection both rely on a naive collision-detection loop, such that all objects are looped through until matches are found. An approach using spatial data structures, for example quadtrees [@samet1988], could significantly accelerate these searches. Another area which may be open to performance improvements is reactivity. While reactivity is implemented using the - fairly performant - observer pattern, and event callbacks throttled on hot paths (such as mousemove events), further optimization of the reactive system may be possible. Finally, though less performance critical, client-server communication could also be made more performant. Currently, it is implemented with WebSockets [@cheng2024; @mdn2024g] and JSON-based payloads, which incurs serialization/deserialization overhead on each message. An alternative protocol like TCP could mitigate this.

<!--chapter:end:discussion.Rmd-->


# Glossary

Placeholder


#### API {#API}
#### Array of Structs (AoS) vs. Struct of Arrays (SoA) {#SoA}
#### JSON {#JSON}
#### IDE
#### SVG

<!--chapter:end:glossary.Rmd-->


# Appendix

Placeholder


#### Encapsulation in DOP {#dop-encapsulation}
## Gauss method and the russian peasant algorithm for monoids {#gauss-russian-monoids}

<!--chapter:end:appendix.Rmd-->


# Mathematical theory

Placeholder


### Relations {#relations}
### Functions {#functions}
#### More on bijections {#bijections}
#### Composition
#### The image and the pre-image
### Partitions {#partitions}
### Preorders {#preorders}
#### Specializing preorders
#### Structure preserving maps: Monotone maps
### Monoids
#### Simple examples of monoids
#### Beyond numbers
#### Specializing monoids
#### Structure preserving maps: Monoid homomorphisms {#monoid-homomorphism}
### Groups
#### Simple examples of groups
#### Structure preserving maps: Group homomorphisms {#group-homomorphism}
### Categories {#categories}
#### Isomorphisms within categories {#isomorphism}
#### Algebraic structures as categories {#algebraic-as-categories}
### Functors {#functors}

<!--chapter:end:math.Rmd-->

# References


<!--chapter:end:references.Rmd-->


---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Discussion

This thesis explored the role of interaction in data visualization pipelines. More specifically, I investigated how interaction affects the four stages of data visualization pipelines - partitioning, aggregation, scaling, and rendering - and explored the inherent problems and challenges. The main thrust of the argument was that the popular model implied by the Grammar of Graphics [@wilkinson2012], which treats statistics and geometric objects as independent entities, is inadequate for describing the complex relationships between the components of interactive figures [see also @wu2024]. As an alternative, I proposed a simple category-theoretic model, conceptualizing the data visualization pipeline as a functor. 

The essence of the proposed model is as follows. Ultimately, when we visualize, we want to represent our data with a set of geometric objects. To do this, we need to partition our data into a collection of subsets. However, often, particularly with features like linked selection, we want to further partition these subsets, to be able to display highlighted parts of objects. This naturally creates a hierarchical structure: a tree (preorder) of data subsets ordered by set union. 

Importantly, to maintain consistency during interactions like linked selection, the subsequent steps of the data visualization pipeline should preserve this structure. Put simply, the geometric objects in our plots and the underlying summary statistics should *behave like set union*. To describe this formally, in category theoretic terms, we can conceptualize the mapping from data subsets to summary statistics and from summary statistics to geometric objects as a functor (leading to a single, data-to-objects functor, by composition). Further, by reasoning about the properties of functors and set union, we can identify specific algebraic structures that our statistics (and geometric objects) should conform to: groups and monoids. Specifically, to behave consistently, the operations underlying our plots should be associative and unital, and potentially also invertible, monotonic, and commutative. When these algebraic constraints are satisfied, the geometric objects in our plots will compose well, meaning that their parts will add up to a meaningful whole, and this ensures that features like linked selection remain consistent.     

To test the proposed model, I developed `plotscaper`, an R package for interactive data exploration (along with `plotscape`, its TypeScript-based backend). Together, the theoretical model and the software formed a crucial feedback loop, allowing me to refine important concepts. Specifically, many of the model's key concepts emerged from practical challenges I encoutered during the package's development, and, by translating theory into code, I was able to empirically test and refine assumptions about the structure and behavior of interactive data visualizations. 

However, `plotscaper` was also developed to provide a practical tool for data exploration, not just theory testing. As outlined in Section \@ref(background), within the R ecosystem, there is currently no shortage of interactive data visualization packages and frameworks; however, many offer only fairly basic interactive features out of the box. Implementing complex features such as linked selection, representation switching, or parametric interaction (see section \@ref(common-features)) often requires substantial programming expertise and time-investment, creating a barrier to entry for casual users [see e.g. @batch2017]. Thus, a secondary goal of this project was to try to address this perceived gap, and this goal seems to have been met with moderate success. Despite its experimental status and the fact that it is competing with a number of far larger and better-established interactive data visualization frameworks, `plotscaper` has been downloaded over^[The number only includes downloads from the RStudio CRAN mirror.] `r cranlogs::cran_downloads("plotscaper", from = "2024-10-01", to = Sys.Date())$count |> sum()` times in the `r as.numeric(Sys.Date() - as.Date("2024-10-18"))` days since its initial release, see Figure \@ref(fig:plotscaper-downloads):

```{r plotscaper-downloads}
#| echo: false
#| fig-cap: "The cumulative number of downloads of the `plotscaper` package, starting at the date of its initial release (18th of October 2024)."
library(ggplot2)

df <- cranlogs::cran_downloads("plotscaper", from = "2024-10-18", to = Sys.Date() - 2)
df$cum_count <- cumsum(df$count)

theme_set(theme_bw(base_size = 12) +
          theme(axis.ticks = element_blank(),
                panel.grid = element_blank()))

ggplot(df, aes(date, cum_count)) +
  geom_line(col = "#1F78B4") +
  scale_x_date(date_labels = "%b %Y") +
  labs(x = "Date", y = "Cumulative downloads")
```

Despite all of these relative successes, both the theoretical model and its practical implementation in `plotscaper` have, of course, their limitations and opportunities for future improvement. These will be the subject of the next few sections.
 
## Theoretical model

First, it is important to discuss the limitations of the theoretical model presented in this thesis. This model, described in Section \@ref(problems), conceptualizes the data visualization pipeline as a structure-preserving mapping, also known as a functor. Specifically, a key initial assumption is that, when visualizing data, we start with a hierarchy of data subsets. These subsets are disjoint (share no overlapping data) and are ordered by set inclusion/union. To produce meaningful graphics, the subsequent steps of the data visualization pipeline should preserve this inherent structure and *behave like set union*. In category theoretic terms, this means that the aggregation and rendering steps of the data visualization pipeline have to be functors. Based on the properties of set union, this naturally identifies algebraic structures as groups and monoids. 

This proposed model of the data visualization pipeline naturally raises several questions and potential criticisms. Some of these have been already pre-empted in Section \@ref(problems), however, they will be further discussed and expanded upon in the following subsections. My objective is to clearly articulate the model's advantages, while also acknowledging its limitations and suggesting opportunities for future work.

### Why the model is necessary

First, let us briefly restate the main argument presented throughout Section \@ref(problems). As discussed in Section \@ref(partitioning), ultimately, when visualizing data, we want to represent our data with geometric objects. These objects need to map onto some underlying data subsets. We create these subsets by partitioning based on some variable, such as a categorical variable in a barplot or a binned continuous variable in a histogram. Often, we also want to further partition the subsets by conditioning on another variable or variables. In particular, in interactive data visualization, features such as linked selection naturally impose this hierarchical partitioning on our data, by partitioning on selection status [and in the presence of aggregated views like barplots, see e.g. @wills2008].

After partitioning our data into subsets, we compute summary statistics on these subsets, map these summaries to aesthetic encodings, and finally render these encodings as geometric objects. Importantly, when the partitioning is hierarchical, the geometric objects we use to represent our data need to reflect this. To do this, data visualizations practitioners employ techniques like stacking, dodging, or layering. While dodging and layering are popular in static data visualization, in interactive visualization, stacking offers some unique advantages (see Section \@ref(stacking-part-whole)). Specifically, since objects formed with stacking better map onto the part-whole relationships in the underlying data, they have more consistent behavior under interaction, and also simplify certain computations.       

However, an important issue related to stacking is that the way we visually represent data is *not* independent of the way we summarize it. As discussed in Section \@ref(stackable-or-not), past data visualization researchers have noted the fact that while some summary statistics such as sums or counts can be effectively stacked, others like averages or quantiles cannot [see e.g. @wilke2019; @wills2011; @wu2022]. For instance, while the sum of sums or maximum of maximums represent valid overall statistics, the average of averages is different from the grand mean. Further, the issue clearly extends to other types of plots than just barplots: some statistics allow us to represent objects composed of parts, while others do not. 

Early in the project, I came across some ideas from functional programming, which in turn lead me to the key insight that these part-whole relationships underlying geometric objects and summary statistics can be described algebraically, by using some fundamental concepts from category theory. This lead me to develop a simple algebraic model of graphics, relying on functors, monoids, and groups. By grounding our reasoning in this algebraic model, we can quickly identify which combinations of statistics and geometric objects will compose into part-whole relationships, and thus behave well under features like linked selection. For instance, since the average operator is not a monoid, it will not compose into part-whole relations, and thus we know that it will present certain challenges if we try to combine it with linked selection. Further, the model also allows us to identify what *kinds* of linked selection will work. For instance, because the maximum and convex hull operators are monoids, they will work well when comparing nested subsets (e.g. single selection group vs. all cases). However, because they lack an inverse, they will be inappropriate for comparing disjoint subsets (multiple distinct selection groups, see Section \@ref(groups-inverses)). 

### Disjointness

As discussed above, a core assumption of the model is that data can be organized into a hierarchy of partitions made up of *disjoint* subsets. While this assumption holds true for most common plot types, some visualizations deviate from this model, specifically, those in which geometric objects within the same graphical layer represent overlapping data subsets. Examples include certain visualizations of set-typed data [see e.g. @alsallakh2014] and two-dimensional kernel density plots (aka contour plots, see Section \@ref(comparison-disjointness)). However, these non-disjoint representations tend to be fairly rare. In the vast majority of *standard* plots - including barplots, histograms, scatterplots, density plots, heatmaps, violin plots, boxplots, lineplots, radar plots, treemaps, and parallel coordinate plots - each geometric object represents a disjoint subset of the cases^[For example, I tried going through `ggplot2`'s list of [`geoms`](https://ggplot2.tidyverse.org/reference/) and identifying all non-disjoint data representations. The only examples I have been able to find have been find were `geom`s based on two-dimensional kernel density estimates, i.e. `geom_density_2d` and `geom_density_2d_filled` (each line or polygon represents densities computed across all datapoints).].

In Section \@ref(comparison-disjointness), I argued that this ubiquity of disjoint data representations is not an accident, but instead stems from a fundamental naturality. Disjointness underpins many core concepts in statistics, programming, and mathematics, and, in general, seems to align well with our cognitive processes (see Section \@ref(comparison-disjointness)). Put simply, it is far easier to reason about distinct objects, rather than ones which are entangled [see also @hickey2011]. This, I propose, make disjointness a particularly attractive feature for interactive data visualization. Conversely, with non-disjoint data representations, certain interactive features may become difficult or even impossible to implement. For example, how would one implement meaningful linked selection with a two-dimensional kernel density plots, without referencing the underlying (disjoint) data points? Thus, while not the only option, I contend that disjointness provides a good default model for interactive graphics. 

### Associativity and unitality {#associativity-unitality}

The core idea of the model is preservation of the properties of set union. Among these, the two most important ones are *associativity* and *unitality*. Associativity ensures that we can perform the summary operations in any order, while unitality guarantees that empty data subsets are handled properly. Further, as discussed in Section \@ref(monoids), together, these two properties identify a central class of algebraic structures: monoids.

This connection between set union and monoids is not novel. For instance, monoids have long been known to have desirable properties for parallel computing [see e.g. @parent2018; @fegaras2017; @lin2013]^[As a side note, the challenges we face when visualizing data often are, in certain ways, remarkably similar to parallel computing: often, we want to break the data into parts, compute some summaries, and the combine these back together to yield meaningful aggregates]. Furthermore, more broadly, monoids are also popular functional programming [see e.g. @milewski2018], and form the basis of certain algorithms in generic programming [@stepanov2009; @stepanov2013]. Finally, monoids and other concepts from category theory have also been used in certain areas of relational database theory [see e.g. @fong2019; @gibbons2018]

However, to my knowledge, I am the first to make the connection between monoids and popular visualization types. As discussed in Section \@ref(visualization-category-theory), while there has been some limited number of applications of category theory to data visualization, the way I have used category-theoretic concepts differs significantly from all of these. On one hand, it is significantly more applied than some other applications [see e.g. @beckmann1995; @hutchins1999; @vickers2012], due to the fact that it relates to concrete statistics, geometric objects, and interactive features, rather than broad ideas about the nature of the visualization process. On the other hand, it is also more theoretical than other applications, since it does not require any specific implementation; the work presented in this thesis is not a functional programming library. During my survey of the data visualization literature, I did not find any references explicitly linking common plot types such as barplots or histograms to groups and monoids. The only hints of similar ideas I have been able to find has been in the documentation of Crossfilter, a JavaScript library for exploring large multi-dimensional data sets [@crossfilter2023; @crossfilter2025]. However, this documentation only discusses properties like associativity and commutativity, without connecting them to algebraic structures.

Associativity and unitality have some interesting implications for data visualization systems. Firstly, due to the previously mentioned connections to parallel computing, associativity suggests that a data visualization system built around monoids can be easily parallelized. This enables improved performance and makes it suitable for distributed computing. Secondly, unitality also has interesting consequences. Specifically, it ensures that empty data subsets have unambiguous representations, and, conversely, lack of unitality may cause ambiguities. For example, the average operator is not unital, since the average of an empty set is not defined. This can create issues when visualizing data, such as drawing a bar plot where some categories are empty. While we might choose to omit bars for empty categories, this leads to an ambiguous representation, such that absence of a bar can indicate one of two things: either there were *no* cases in the category and their average is undefined, or there *were* cases and their average was equal to the y-axis intercept. In contrast, since the sum is unital, the representation is always clear: a missing bar unambiguously indicates a sum of zero, regardless of whether there were cases in the category or not. This also provides a potential alternative perspective on a perennial debate in data visualization: should the base of the barplot always start at zero or not [see e.g. @cleveland1985; @wilkinson2012]? Since the monoidal unit represents the default state of "no data", it can serve as the default choice for the base, such that, for example, a barplot of products may have one as its base.  

### Groups and inverses {#discussion-groups-inverses}

As discussed in Section \@ref(groups-inverses), another useful property of set union which it may be useful to preserve is the presence of an inverse. Specifically, if we have two sets $A$ and $B$ and take their union $A \cup B$, we can always recover either set by taking the difference with the other: $A = (A \cup B) \setminus B$ and $B = (A \cup B) \setminus A$. While monoids do not necessarily possess an inverse, those that do are called groups. Groups are a well-known algebraic structure, and have certain advantages over monoids which will be reviewed in this section.

First and foremost, as discussed in Section \@ref(groups-inverses), groups allows us to preserve not only set union, but also set difference. This has an important implication for interactive visualization: it allows us to compare selections across multiple distinct groups. Specifically, while with monoids, we can compare data subsets of the form $A$, $A \cup B$, $A \cup B \cup C$, with groups, we can always compare $A$, $B$, and $C$ directly, as disjoint subsets. This identifies monoids as suitable for single-group (transient vs. none) selection, whereas (algebraic) groups are necessary for multiple persistent selection groups.  

More broadly, this distinction underscores an intriguing philosophical problem regarding the interpretation of data partitions: partitioning variables can sometimes be interpreted in two distinct ways. Specifically, in some cases, certain levels of a partitioning variable can also be interpreted as nested subsets of the others. Such is the case, for example, with the above-mentioned single-group linked selection: we can interpret the selected cases as a *special subset of all cases*. Likewise, another example, discussed in Section \@ref(groups-inverses), is interpreting smokers as a special subset of all study participants. However, in other cases, this nested interpretation is not appropriate. For instance, with a political party affiliation or gender variable, it is typically inappropriate to consider one level as nested within others; instead, a *direct* comparison between the different levels is usually desired. However, as I demonstrated in Section \@ref(groups-inverses), to preserve composition of objects, this direct comparison requires the presence of inverses, which imposes an additional constraint on the summary statistics we can visualize. Finally, it is important to note that this distinction is generally *not* encoded in the data itself. Common data structures such as string arrays or R's `factor`s typically do not specify whether factor levels represent nested or disjoint subsets, necessitating metadata knowledge. 

Second, under specific conditions, groups can also provide useful computational shortcuts.  For instance, in the context of linked selection, the inverse operator enables efficient updates. Specifically, consider a scenario with a million selected scatterplot points; if one point is deselected, the inverse operator allows us to subtract contribution of that single data point. In contrast, since monoids lack an inverse, all summary statistics need to be recomputed from scratch each time selection changes. Therefore, in the presence of large volume data sets, groups can provide a distinct advantage.

Whether to choose groups or monoids as the foundational algebraic model for objects within an interactive data visualization system presents an interesting design consideration. Fundamentally, both structures encode the concept of objects composed of parts, but they differ in the nature of this composition: monoidal composition implies nesting, whereas group composition allows for disjointness. Further, as discussed above, groups offer advantages such as supporting multi-group linked selection and enabling computational shortcuts. However, the definition of a group is also more restrictive than that of a monoid; for instance, the maximum and convex hull operators, while monoids, lack inverses and therefore do not satisfy the criteria for groups \@ref(groups-inverses). Overall, since monoids are the more general structure, and can be usefully combined with single-group linked selection as well as many other interactive features, I would in general recommend these as the basis of objects in interactive data visualization systems. However, data visualization practitioners should be cautioned against using multi-group selection in the absence of inverses, similar to how past authors have warned against stacking statistics other than sums (see Section \@ref(stacking-not-graphical)).

### Additional properties: Monotonicity, commutativity, and others

Beyond associativity, unitality, and inverses, set union possesses several other algebraic properties that may be useful to preserve in specific contexts. While I consider these somewhat less essential, as their absence does not inherently invalidate the part-whole model (discussed throughout Section \@ref(problems)), they can still be advantageous in specific circumstances. Consequently, I believe their discussion here is warranted. Of these additional properties of set union, monotonicity and commutativity are, in my view, the two most important ones (see Section \@ref(monotonicity-commutativity)).

#### Monotonicity

In the context of set union, *monotonicity* implies that the cardinality of the union of two subsets is always greater than or equal to the cardinality of either subset: $\lvert A \cup B \rvert \geq \lvert A \rvert \land \cup B \rvert \geq \lvert B$. Therefore, it seems reasonable that our summary statistics should also perhaps exhibit this property, yielding greater values when summarizing larger data subsets. Indeed, this property is useful because many geometric objects exhibit unidirectional growth: bars increase in height, circles expand in radius, and so forth. However, there are also some geometric representations which can depict additive changes in negative directions. For instance, as discussed in Section \@ref(monotonicity-commutativity), a polyline composed of vector segments can simultaneously "grow" in positive and negative directions along the x- and y-axes. Thus, for depicting objects composed of parts, monotonicity is not strictly necessary, although it does align well with properties of many common geometric objects, and can also offer other benefits, for instance, simplifying the calculation of axis limits (see Section \@ref(monotonicity-commutativity)).

#### Commutativity

An operation is *commutative* if the order of the elements does not matter. For set union, this is always the case, by definition: sets are unordered collections, and, as such, $A \cup B = B \cup A$. However, in applied data analysis, data is frequently stored in two-dimensional tables with an inherent row order, which can influence data summaries and their visual representation. For instance, lineplots are often non-commutative: given that a line connects multiple points, different row arrangements will yield visually distinct lines^[Unless sorted by the x-axis variable; compare `geom_line` and `geom_path` in `ggplot2`, [@wickham2016].]. Furthermore, certain plots may be commutative in some systems and not others. For example, barplot and histogram highlighting is often implemented commutatively: the highlighted cases are merged into a single segment at the bar's base. However, some systems instead highlight individual cases, creating diffuse, striped patterns (see, for example, Figure \@ref(fig:xlisp-stat)). 

Regarding summary statistics, while many common operators such as sums, averages, or maxima are commutative, there also are some examples of non-commutative monoids, such as string concatenation and matrix multiplication, which could, hypothetically, be used for summaries. Overall, commutativity generally appears to be desirable, as it renders row order unimportant, thereby reducing cognitive load and offering potential advantages in the context of distributed computing or resampling techniques (e.g. bootstrapping).

#### Idempotence, unitality, and distributivity

Finally, there are few other properties of set union which, in my view, are less important but which it may be useful to briefly touch on here. The first is idempotence. Given any set $A$, the union of that set with itself is itself, $A \cup A = A$. While some statistical operators, such as minimum, maximum, and convex hull, exhibit this property, I have not identified a specific use case for it in graphics, interactive or otherwise. 

Second, just as the monoidal neutral element corresponds to the empty set ($\varnothing$, such that $A \cup \varnothing = A$), there may also be a so-called terminal element or object [see e.g. @fong2019] that behaves like the universal set $\Omega$ under union, such that $A \cup \Omega = \Omega$. For instance, in the case of sums, products, or maximums, our data may contain special $\infty$ or `Inf` value, which, when combined with any other value, is always yielded back: $x + \infty = x \cdot \infty = \max(x, \infty) = \infty$ for all $x$. Similarly, the special missing value `NA` in R also behaves somewhat like the terminal element for many operations^[Although going by precise category-theoretic definitions, introducing missing values or `NA`'s does create some problems: by definition, the terminal element is always unique, so `NA` and `Inf` would either have to be isomorphic (both be *the* terminal object), or only one of them could be (based on R's implementation, this would have to be `NA` since e.g. `NA * Inf == NA`).]. The terminal element could hypothetically be useful for more efficient aggregation, due to the fact that, once we encounter it, we can terminate the aggregation loop. However, I have not been able to think of any examples, beyond trivial ones^[E.g. logical conjunction and disjuction with $\text{FALSE}$ and $\text{TRUE}$], where the terminal element would be easily and generally representable by graphics (for instance, how do we represent a bar of height $\infty$?). As such, the practical utility of this hypothetical optimization is debatable^[Further, checking for the terminal element would require adding a branching (`if`) statement inside the body of the aggregation loop. In performance-critical code sections, branching can introduce performance bottlenecks, especially when the branching condition is hard to predict [see e.g. @fabian2018]. Although terminal values may occur infrequently (a data set may not have any), making branching fairly predictable, it may still be more efficient, on balance, to simply aggregate over all data, even if it includes terminal values.]. 

Lastly, set union distributes over set intersection. However, the relation of set intersection to the part-whole object model is not clear, particularly when the parts are disjoint subsets. An analogue to set intersection would require that the summary statistic has a complementary binary operation, distinct from an inverse (which is identified with set difference, see \@ref(discussion-groups-inverses)), which would need to distribute over the primary aggregation operation. Furthermore, unlike inverses and neutral elements, this complementary distributive operation does not need to be unique (for instance, maximum distributes over both products and minimums). As such, I do not really see a benefit in identifying such operations.

### Model constraints

A potential point of contention is that the constraints the model imposes are too rigid. Specifically, requiring all plots to be groups or monoids would exclude a significant fraction of popular visualization types and styles. Conversely, many non-monoidal plot types, such as boxplots, have been successfully implemented alongside linked selection in the past [see e.g. @theus2002; @urbanek2011]. Thus, a valid question one might have is whether the limitations imposed by the model are justified by its practical utility.

I contend that this is the wrong perspective, since the model is not meant to be prescriptive. Instead, it simply provides a framework for identifying visualizations which are "naturally" compatible with features like linked selection. We are free to violate this naturality whenever we see fit, however, we should also be aware of the fact that doing this may require some ad hoc design choices. For instance, since a boxplot box is not a monoid, there is no such thing as highlighting a "part of a boxplot box". We can solve this issue by e.g. plotting the highlighted cases as a second box next to the original one [see e.g. @theus2002; @urbanek2011], however, we may lose some of the nice properties of part-whole relations in monoidal plots (see Section \@ref(stacking-part-whole)). Thus, the real strength of the model lies in its ability to identify inherent trade-offs in the design and implementation of interactive graphics.  

### Potential future directions

The model described in Section \@ref(problems) is, in its current state, far from exhaustive. Ultimately, it amounts to conceptualizing the data visualization pipeline as a functor, from the category of data subsets to the category of geometric objects (with category of summary statistics being the intermediate domain). While I believe this simple model allow us to capture and reason about a fairly large number of common graphics, there are of course many opportunities for improvement and extension.

One already discussed feature of the model which could be further refined is the set of algebraic properties of set union that the model (functor) should preserve. Specifically, I identified associativity and unitality as the two key properties which it is necessary to preserve if we wish to visualize meaningful part-whole relationships. I also discussed inverses, monotonicity, and commutativity as other potentially useful properties. A deeper analysis might reveal some of these additional properties to be essential, or even identify other properties that could play an important role. Furthermore, while I identified the fundamental operation as set union, a case could perhaps be made that perhaps, given that real-world data is often ordered and contains duplicates, the fundamental operation should be identified as the union of bags or concatenation of tuples (ordered collections). Exploring the implications of this alternative perspective could yield valuable insights.

More fundamentally, beyond the simple category-theoretic concepts used throughout this thesis, there may be other, more advanced concepts which may be useful for describing the behaviour of graphics. For instance, since we describe plots as functors, a preliminary idea I have considered is to model interactive representation switching as natural transformation [transformation of one functor into another which respects composition of morphisms, see @fong2019]. Moreover, just as we can conceptualize a visualization as a functor from data to geometric objects, we could perhaps conceptualize the process of reasoning about a plot as the opposite functor, with the two functors forming an adjunction [@fong2019]. These are both merely tentative ideas, and there are likely many other concepts in applied category theory that could be hypothetically useful for advancing theory of (interactive) graphics.

Further, an important gap in the model concerns plots in which geometric objects do not represent distinct data entities. One interesting counter-example are two-dimensional kernel density plots, due to the fact that each polygon in them represents an isopleth over a joint probability density (and thus does not uniquely identify a distinct set of data points). However, my review of example galleries from popular data visualization libraries such as `ggplot2` [@wickham2016] and `matplotlib` [@matplotlib2025], revealed only few such instances. While other examples undoubtedly exist within the data visualization and scientific literature, it seems that, in the vast majority of popular plot types, individual geometric objects within the same layer represent distinct subsets of the data. Nevertheless, extending the algebraic model to include non-disjoint data representations could constitute an interesting avenue for future research.  

## Delivered software

A second significant part of this project involved developing an interactive data visualization package (`plotscaper`). I had the time to evaluate, test out, and implement a range of interactive features, which were designed to facilitate effective data exploration. While the project was largely successful in many areas, there are also others which could still warrant further improvement. In this section, I will discuss these applied concerns regarding the nature of the delivered software. 

### Scope and features

Currently, `plotscaper` offers a robust set of features for creating and manipulating interactive figures, as detailed in Section \@ref(system) or in Section \@ref(applied-example). While these features can be used to create a substantial range of useful interactive figures, some limitations and implementation gaps still exist, particularly when compared to other, more established data visualization packages. 

#### Plot types

In the current version, `plotscaper` offers six plot types: scatterplots, barplots, histograms, fluctuation diagrams, histograms, two-dimensional histograms, and parallel coordinate plots. Additionally, normalized representations, such as spineplots and spinograms, are available for all aggregate plots (and also parallel coordinate plots). While these six plot types can already be used to create a wide range of useful interactive figures, there are numerous other plots which could be highly useful in applied data analysis. Density plots, radar plots, mosaic plots, and maps, for example, should be compatible with the `plotscaper` model and could be potential additions in future versions of the package. Furthermore, a relatively simple addition which could be useful would be horizontal barplots. 

Finally, in the current version of the system, all plots are specified nominally^[Meaning that, for instance, to add a scatterplot, we call the `add_scatterplot` function.]. While a full declarative plot specification in R would be appealing, implementing such a system has presented challenges, and the issue will be discussed in more depth in Section \@ref(declarative-schemas). This somewhat limits the package's extensibility, however, users familiar with TypeScript can still create arbitrary new plot types using `plotscape` code.

#### Consistency

A major advantage of `plotscaper` is that all plots fully support linked selection. Specifically, in any plot, the users can click or click-and-drag to select geometric objects, and the resulting cases are then highlighted across all the other plots. Furthermore, for aggregate types of plots, users can specify their own aggregation functions (`Reducers`), will result in well-defined behavior under linked selection, provided that they fulfill the monoidal contract.

This consistency is highly useful because it reduces cognitive load. The users do not have to learn which plots support selection and which only display it - instead, all plots behave in the same, predictable manner. Further, the guarantees offered by part-whole relationships (see Section \@ref(stacking-part-whole)) prevent undesirable visual behaviors, such as rapidly changing axis limits. Overall, this arguably leads to a smoother, more "natural" user experience.

#### Selection model

As described in Section \@ref(system), `plotscaper` currently uses a simple transient-persistent product model for linked selection. Users can transiently select cases or assign them to permanent groups via click or click-and-drag interactions. The combination of transient and persistent selection results in $2 \times 4 = 8$ possible selection states, per case. This model facilitates simple conditional queries, such as "how many cases assigned to group X are also transiently selected?" For instance, users can assign a bar in a barplot to a permanent group and transiently select a point cluster in a scatterplot to easily identify cases belonging to either.

This simple selection model is fairly effective, however, it does have some limitations: for instance, it does not facilitate combining selections via logical operators. Implementing comprehensive selection operators [see e.g. @urbanek2003; @urbanek2011; @theus2002; @wilhelm2008] would enable more sophisticated analytical workflows. Finally, selection geometry is currently restricted to point-clicks or rectangular regions defined by clicking-and-dragging. While these are fairly simple and intuitive, alternative selection strategies, such as a movable selection brush, a circular region expanding around a point, or arbitrary lasso/polygon, could potentially be useful in specific contexts [see e.g. @wills2008].

#### Feature depth

There are also several other interactive features which could benefit from further polish. First, scale-related features present opportunities for improvement. For instance, while it is currently possible to sort barplot bars by height, this functionality could be expanded to other plot types and alternative sorting schemes, such as sorting by the heights of highlighted segments, could be beneficial. Similarly, enabling manual (interactive) axis reordering in parallel coordinate plots could be valuable. For continuous scales, the ability to interactively switch to alternative transformations, like a logarithmic scale, would also be a useful addition.

Beyond scale-related improvements, other interactive features could also benefit from refinement. For instance, while the system currently supports querying custom statistics aggregated via `Reducer`s, expanding the scope to non-monoidal statistics warrants consideration, due to the fact that consistency requirements for statistics displayed in the query table are less rigid than those mapped to visual aesthetics. Further, a more generic model for parametric interaction (which will be touched on in Section \@ref(declarative-schemas)) would also be useful. There is also wide a range of other, more ambitious features such as animation (time aesthetic) or semantic zooming, which could offer significant utility in specific use cases. However, these features also involve a substantial implementation overhead.

Finally, the client-server model also offers several opportunities for improvement. For instance, being able to render arbitrary graphical elements (e.g., computed regression lines) directly within existing plotscaper plots or figures would be highly beneficial. Similarly, features like dynamically switching `Reducer`s or sending the underlying summary statistics to the server (akin to querying) warrant consideration. Furthermore, more fine-grained selection controls could also prove useful. Features such as these should not be too difficult to implement, given that the underlying infrastructure is in place.

#### Customization

Plot and figure customization in `plotscaper` is currently fairly limited. While users can adjust attributes such as the figure layout and plot scales, surface-level graphical attributes like colors, margins, and axis styling are not yet exposed. While the support for adding these customization options does exist, their implementation was not prioritized. They may be added in future package versions, when the API becomes more stable.

#### Performance {#perf}

As will be discussed in more depth in Section \@ref(performance), while `plotscaper` provides decent performance even on moderately sized data sets (thousands or tens of thousands of data points), a particularly valuable feature for performance-sensitive applications would be the ability to register custom rendering backends. For instance, GPU-based rendering could enable visualization of much larger datasets [to get some ideas of the scope, see e.g. @highschartsboost2022; @lekschas2024; @unwin2006].  

### Declarative schemas and extensibility {#declarative-schemas}

As discussed in Section \@ref(system), one limitation of the delivered system is the absence of a simple declarative schema-based approach for specifying plots. While declarative schemas, popularized by the Grammar of Graphics [GoG, @wilkinson2012], have become a highly popular in modern data visualization libraries [see e.g. @wickham2016; @satyanarayan2016; @plotly2023], my system deviates from this paradigm. Although certain core components within `plotscape` are modeled declaratively (as shown in Section \@ref(summaries)), the overall plot definitions remain largely procedural, and the user-facing API (`plotscaper`) relies on nominal plot types^[Currently there are six different types of plots implemented.]. While users familiar with TypeScript can still create new plot types using the underlying `plotscape` code, this approach does limit the extensibility of the software for R users.  

However, as I argued in Section \@ref(variables-encodings), the issue underlying this lack of a full declarative plot specification is not unique to my system. Instead, I contend that many currently available declarative data visualization libraries offer partial or incomplete solutions, and frequently conceal key implementation details [see also @wu2024]. For example, operations like grouping, binning, stacking, and normalization are frequently handled implicitly, with limited user control. In essence, there are gaps in the GoG model which, as far as I am aware, none of the currently available declarative data visualization libraries address. To quote @wu2024:

> Despite these broad-ranging benefits, we observe that this [GoG-based] semantic delineation obscures the process of visual mapping in a way that makes it difficult to reason about visualizations and their relationships to user tasks.
> 
> We observe that the core reason for this dissonance between visualization specifications and functions is that the Visual Mapping step represents two distinct substeps. The first involves additional transformations over the data tables in order to compute, e.g., desired statistics and spatially place marks that are specific to the visualization design. We term these Design-specific Transformations. The second is the visual encoding that maps each row in the transformed table to a mark and data attributes to mark attributes using simple scaling functions.

During the course of my project, I have independently reached several conclusions similar to those of @wu2024^[I became aware of Eugene Wu's work only after he contacted me concerning the publication of a paper I co-authored during this project's development: @bartonicek2024]. Most importantly, I have also come to the conclusion that separating the aggregation (Design-specific Transformation) and scaling (visual encoding) stages is crucial. By mapping data variables directly onto aesthetics, most declarative data visualization libraries entangle these two concepts together, hindering interactive features like representation switching.

However, specifying declarative schemas for separate aggregation and visual encoding stages presents challenges. While working on `plotscaper`, I attempted to devise a method for specifying these schemas, but despite some partial successes (see Section \@ref(aggregation-summaries)), a truly comprehensive solution ultimately eluded me. I opted to make this implementation gap explicit in `plotscaper` by specifying plots nominally. Nevertheless, I believe that a useful outcome of this effort is that I have been able to identify three key factors which contribute to this difficulty of developing declarative schemas for interactive data visualization, and these are detailed below. 

#### Data and encodings

The primary factor complicating declarative schemas in interactive graphics is the weak correspondence between data variables and visual encodings. Often, what we plot are *not* variables found in the original data, but instead variables which have been computed or derived in some way (see Section \@ref(variables-encodings)). This problem exists even in static graphics; however, it is amplified in interactive graphics, where dynamically remapping variables to aesthetics (like transforming a barplot into a spineplot) is often desirable.

Therefore, directly mapping original data variables to aesthetics is clearly an inadequate approach. However, if we instead want to map derived variables to aesthetics, we have to explicitly specify *what* we compute and *how*. For instance, to fully specify a histogram, we need to describe binning cases based on a continuous variable, computing counts within those bins, stacking these counts, and finally mapping bin borders to the x-axis and counts to the y-axis. While I believe I have made a significant progress in providing a mechanism for specifying declarative schemas for this process (see Section \@ref(variables-encodings)), ultimately, I ultimately could not integrate it fully with the rest of the system without relying on some procedural code.

#### Graphs and hierarchy

The hierarchical nature of graphics, as discussed in Section \@ref(hierarchy), also poses a significant challenge for declarative schemas in interactive data visualization. While static data visualization systems often treat underlying data as "flat," interactivity demands hierarchical data structures. Interactive features like linked selection and parametric manipulation trigger updates to summary statistics, necessitating hierarchical organization for efficiency, particularly when features like stacking and normalization are involved (see Sections \@ref(transforming-summaries) and \@ref(aggregation-summaries)). Moreover, hierarchical data can also be leveraged for more efficient scale and axis updates (see Section \@ref(plot-scales)), and for displaying multiple levels of information during querying (e.g. displaying summary statistics for both objects and segments). However, specifying declarative schemas with hierarchical data is inherently more challenging than with flat data due to both increased complexity (multiple datasets) and hierarchical dependencies.

Currently, `plotscape` represents these hierarchical dependencies by special aggregated variables with references to parent-level variables (as well as the factors they were computed over, and the reducers used for their computation). These references can then be leveraged during stacking and normalization (see Section \@ref(aggregation-summaries)). An alternative approach, not explored in this project, would be to "flatten" the hierarchical structure via SQL-like table joins (i.e. storing a secondary key to parent data and left-joining). However, as also discussed in Section \@ref(aggregation-summaries), this method has limitations, such as the fragility of storing hierarchical relationships within column names. Further, each aggregated variable has to *remember* additional metadata, such as the factor cardinality and aggregation strategy (see Sections \@ref(aggregation) and \@ref(aggregation-summaries)) anyway, and so the flat, relational model does not appear to be entirely adequate.

#### Reactivity

The final factor which makes declarative schemas challenging is reactivity. Interactive visualizations, by their very nature, must respond to user input. However, a significant complexity arises due to the fact that this input can affect various stages of the data visualization pipeline, and all resulting changes need to be propagated accordingly. 

Interactive features can differ signficantly in how much downstream effect they have on the rest of the pipeline. For instance, while adjusting the size of points in a scatterplot is a purely graphical computation, changing the width of histogram bins requires recomputation of the underlying data. Therefore, any declarative schema aiming to incorporate reactivity must be able to handle reactive parameters with hierarchical dependencies.

While signals (see Section \@ref(signals)) may offer a general solution, as discussed in Section \@ref(reactivity-solution), I found their developer ergonomics during `plotscaper`'s development lacking. I eventually concluded that most desired reactivity could be effectively integrated at several discrete points within the four stages of the data visualization pipeline, forming a simple, linear dependency chain. Although I did not have time to develop a general declarative mechanism for specifying reactive parameters in this way, the following list identifies these potential entry points for reactivity:

- Pre-partitioning: Data streaming, partitioning parameter updates (e.g. histogram bin width, selection)
- Pre-aggregation: Aggregation parameter updates (e.g. regression line regularization)
- Pre-scaling: Scale parameter updates (e.g. sorting, size/alpha adjustments)
- Pre-rendering: Surface-level changes (e.g. layout modifications, DOM resize events)

To summarize, modeling interactive data visualizations declaratively presents significant challenges. The seemingly straightforward model of "assign variable $x$ to aesthetic $y$" presents challenges for implementing efficient interactive graphics, due to three key factors: weak correspondence between data variables and aesthetics, hierarchical nature of graphics, and dependencies between reactive parameters. Importantly, I contend that these issues are not unique to `plotscaper` but extend to other data visualization systems as well. While a truly general solution may require substantial amount of additional effort, I hope that, by identifying and classifying these issues, this thesis offers insights that may be capitalized on by future research.

### Performance {#performance}

A final key aspect of the delivered software to discuss is performance. As discussed in Section \@ref(background), in interactive data visualization, responsiveness is paramount. Slow systems can quickly frustrate users, undermining the value of even the most sophisticated features. With today's expectations for highly responsive graphical user interfaces (GUIs) and the ever-increasing size of datasets, performance is a major concern.

Despite not being the sole focus, I am happy to report that `plotscaper` delivers solid performance even on moderately sized data sets. Specifically, I was able to achieve highly responsive point-wise linked selection on data sets with tens of thousands of data points (such as the `diamonds` data set; see the [Performance vignette](https://bartonicek.github.io/plotscaper/articles/performance.html) on `plotscaper`'s package website). 

It is important to mention that these performance characteristics are largely attributable to the highly optimized nature of modern JavaScript engines (such as V8), rather than any targetted optimization. Nevertheless, I did try to maintain a reasonably performance-minded approach while developing the package. I primarily did this by adhering to data oriented practices such as utilizing the Structure of Arrays (SoA) data layout. I have found this approach to be quite effective and would recommend it to other visualization developers.

Profiling revealed that the primary performance bottleneck was rendering. This is quite evident even on the user level: figures composed solely of aggregate plots remain responsive even with fairly large data sets, whereas figures with bijective plots (such as scatterplots or parallel coordinate plots) can become sluggish with even moderately sized datasets (thousands or tens of thousands of data points). Therefore, alternative rendering strategies could offer significant performance gains. Currently, `plotscaper` relies on the default HTML5 `canvas` rendering context, which provides a simple interface for rendering 2D graphics. GPU-based rendering, particularly WebGL [@webgl2025], could lead to significant performance improvements. Since `plotscaper`'s current rendering logic is tightly integrated with HTML5 `canvas`, decoupling this logic would be a top priority in a potential major rewrite.

Beyond rendering, other parts of the system may provide opportunities for performance wins. For instance, as mentioned in Section \@ref(plot-mousemove), currently, querying and selection both rely on a naive collision-detection method, such that all objects are iterated through until matches are found. An approach using spatial data structures, for example quadtrees [@samet1988], could significantly accelerate these searches. Another area open to performance improvements is reactivity. While reactivity is implemented using the fairly performant Observer pattern, and event callbacks throttled on hot paths (such as mousemove events), further optimization of the reactive system may be possible. Finally, though less performance-critical, client-server communication could also be made more performant. Currently, it is implemented with WebSockets [@cheng2024; @mdn2024g] and JSON-based payloads, which incurs serialization/deserialization overhead on each message. An alternative protocol like TCP could be used to mitigate this.

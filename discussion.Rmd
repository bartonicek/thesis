---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Discussion

This thesis explored the role of interaction in data visualization pipelines. More specifically, I investigated how interaction affects the four stages of data visualization pipelines - partitioning, aggregation, scaling, and rendering - and explored the inherent problems and challenges. The main thrust of the argument was that the popular model implied by the Grammar of Graphics [@wilkinson2012], which treats statistics and geometric objects as independent entities, is inadequate for describing the complex relationships between the components of interactive figures [see also @wu2024]. As an alternative, I proposed a simple category-theoretic model, conceptualizing the data visualization pipeline as a functor. 

The essence of the proposed model is as follows. Ultimately, when we visualize, we want to represent our data with a set of geometric objects. To do this, we need to partition our data into a collection of subsets. However, often, particularly with features like linked selection, we want to further partition these subsets, to be able to display highlighted parts of objects. This naturally creates a hierarchical structure: a tree (preorder) of data subsets ordered by set union. 

Importantly, to maintain consistency during interactions like linked selection, the subsequent steps of the data visualization pipeline should preserve this structure. Put simply, the geometric objects in our plots and the underlying summary statistics should *behave like set union*. To describe this formally, in category theoretic terms, we can conceptualize the mapping from data subsets to summary statistics and from summary statistics to geometric objects as a functor (leading to a single, data-to-objects functor, by composition). Further, by reasoning about the properties of functors and set union, we can identify specific algebraic structures that our statistics (and geometric objects) should conform to: groups and monoids. Specifically, to behave consistently, the operations underlying our plots should be associative and unital, and potentially also invertible, monotonic, and commutative. When these algebraic constraints are satisfied, the geometric objects in our plots will compose well, meaning that their parts will add up to a meaningful whole, and this ensures that features like linked selection remain consistent.     

To test the proposed model, I developed `plotscaper`, an R package for interactive data exploration (along with `plotscape`, its TypeScript-based backend). Together, the theoretical model and the software formed a crucial feedback loop, allowing me to refine important concepts. Specifically, many of the model's key concepts emerged from practical challenges I encoutered during the packages' development, and, by translating theory into code, I was able to empirically test and refine assumptions about the structure and behavior of interactive data visualizations. 

However, `plotscaper` was also developed to provide a practical tool for data exploration, not just theory testing. As outlined in Section \@ref(background), within the R ecosystem, there is currently no shortage of interactive data visualization packages and frameworks; however, many offer only fairly basic interactive features out of the box. Implementing complex features such as linked selection, representation switching, or parametric interaction (see section \@ref(common-features)) often requires substantial programming expertise and time-investment, creating a barrier to entry for casual users [see e.g. @batch2017]. Thus, a secondary goal of this project was to try to address this perceived gap, and this goal seems to have been met with moderate success. Despite its experimental status and the fact that it is competing with a number of far larger and better-established interactive data visualization frameworks, `plotscaper` has been downloaded over^[The number only includes downloads from the RStudio CRAN mirror.] `r cranlogs::cran_downloads("plotscaper", from = "2024-10-01", to = Sys.Date())$count |> sum()` times in the `r as.numeric(Sys.Date() - as.Date("2024-10-18"))` days since its initial release, see Figure \@ref(fig:plotscaper-downloads):

```{r plotscaper-downloads}
#| echo: false
#| fig-cap: "The cumulative number of downloads of the `plotscaper` package, starting at the date of its initial release (18th of October 2024)."
library(ggplot2)

df <- cranlogs::cran_downloads("plotscaper", from = "2024-10-18", to = Sys.Date() - 2)
df$cum_count <- cumsum(df$count)

theme_set(theme_bw(base_size = 12) +
          theme(axis.ticks = element_blank(),
                panel.grid = element_blank()))

ggplot(df, aes(date, cum_count)) +
  geom_line(col = "#1F78B4") +
  scale_x_date(date_labels = "%b %Y") +
  labs(x = "Date", y = "Cumulative downloads")
```

Despite all of these relative successes, both the theoretical model and its practical implementation in `plotscaper` have, of course, their limitations and opportunities for future improvement. These will be the subject of the next few sections.
 
## Features and limitations of the theoretical model

First, it is important discuss the limitations of the theoretical model presented in this thesis. This model, described in Section \@ref(problems), conceptualizes the data visualization pipeline as a structure-preserving mapping, also known as a functor. Specifically, a key initial assumption is that, when visualizing data, we start with a hierarchy of data subsets. These subsets are disjoint (share no overlapping data) and are ordered by set inclusion/union. To produce meaningful graphics, the subsequent steps of the data visualization pipeline should preserve this inherent structure and *behave like set union*. In category theoretic terms, this means that the aggregation and rendering steps of the data visualization pipeline have to be functors. Based on the properties of set union, this naturally identifies algebraic structures as groups and monoids. 

This proposed model of the data visualization pipeline naturally raises several questions and potential criticisms. Some of these have been already pre-emptyed in Section \@ref(problems), however, they will be further discussed and expanded upon in the following subsections. My objective is to clearly articulate the model's advantages, while also acknowledging its limitations and suggesting opportunities for future work.

### Why the model is necessary

First, let's briefly restate the main argument presented throughout Section \@ref(problems). As discussed in Section \@ref(partitioning), ultimately, when visualizing data, we want to represent our data with geometric objects. These objects need to map onto some underlying data subsets. We create these subsets by partitioning based on some variable, such as a categorical variable in a barplot or a binned continuous variable in a histogram. Often, we also want to further partition the subsets by conditioning on another variable or variables. In particular, in interactive data visualization, features such as linked selection naturally impose this hierarchical partitioning on our data, by partitioning on selection status [and in the presence of aggregated views like barplots, see e.g. @wills2008].

After partitioning our data into subsets, we compute summary statistics on these subsets, map these summaries to aesthetic encodings, and finally render these encodings as geometric objects. Importantly, when the partitioning is hierarchical, the geometric objects we use to represent our data need to reflect this. To do this, data visualizations practitioners employ techniques like stacking, dodging, or layering. While dodging and layering are popular in static data visualization, in interactive visualization, stacking offers some unique advantages (see Section \@ref(stacking-part-whole)). Specifically, since objects formed with stacking better map onto the part-whole relationships in the underlying data, they have more consistent behavior under interaction, and also simplify certain computations.       

However, an important issue related to stacking is that the way we visually represent data is *not* independent of the way we summarize it. As discussed in Section \@ref(stackable-or-not), past data visualization researchers have noted the fact that while some summary statistics such as sums or counts can be effectively stacked, others like averages or quantiles cannot [see e.g. @wilke2019; @wills2011; @wu2022]. For instance, while the sum of sums or maximum of maximums represent valid overall statistics, the average of averages is different from grand mean. Further, the issue clearly extends to other types of plots than just barplots: some statistics allow us to represent objects composed of parts, while others do not. 

Early in the project, I was lucky to come across some ideas from functional programming, which in turn lead me to the key insight that these part-whole relationships underlying geometric objects and summary statistics can be described algebraically, by using some fundamental concepts from category theory. This lead me to develop a simple algebraic model of graphics, relying on functors, monoids, and groups. By grounding our reasoning in this algebraic model, we can quickly identify which combinations of statistics and geometric objects will compose into part-whole relationships, and thus behave well under features like linked selection. For instance, since the average operator is not a monoid, it will not compose into part-whole relations, and thus we know that it will present certain challenges if we try to combine it with linked selection. Further, the model also allows us to identify what *kinds* of linked selection will work. For instance, because the maximum and convex hull operators are monoids, they will work well when comparing nested subsets (e.g. single selection group vs. all cases). However, because they lack an inverse, they will be inappropriate for comparing disjoint subsets (multiple distinct selection groups, see Section \@ref(groups-inverses)). 

### Disjointness

As discussed above, the models fundamentally rests on a hierarchy of data subsets organized into partitions, such that all subsets within a partition are disjoint. This is a core assumption of the model. While this assumption holds true for most common plot types, certain visualizations deviate from this model. Specifically, visualizations where geometric objects within the same graphical layer represent overlapping data subsets do not adhere to this assumption. Examples include specific visualizations of set-typed data [see e.g. @alsallakh2014] or two-dimensional kernel density plots (see Section \@ref(comparison-disjointness)). However, these non-disjoint visualizations represent only a small fraction of all available plot types. In the vast majority of "standard" plots - including barplots, histograms, scatterplots, density plots, heatmaps, violin plots, boxplots, lineplots, and parallel coordinate plots - each geometric object represents a distinct subset of cases^[As a short exercise, I tried going through `ggplot2`'s list of [`geoms`](https://ggplot2.tidyverse.org/reference/) and identifying all non-disjoint data representations. The only examples I have been able to find have been find were `geom`s based on two-dimensional kernel density estimates, i.e. `geom_density_2d` and `geom_density_2d_filled` (each line or polygon represents densities computed across all datapoints).].

In Section \@ref(comparison-disjointness), I argued that this tendency towards disjoint data representations is not a mere convention or accident, but instead stems from a fundamental naturality of disjointness. Disjointness underlies many fundamental concepts in statistics, programming, and mathematics, and, in general, seems to align better with our cognitive processes (see Section \@ref(comparison-disjointness)). Put simply, it is far easier to reason about and manipulate objects which are distinct rather than ones which are entangled [see also @hickey2011]. Therefore, I contend that disjointness is particularly well-suited to interactive data visualization. Conversely, with non-disjoint data representations, certain interactive features may become difficult or even impossible to implement. For example, how would one implement meaningful linked selection with a two-dimensional kernel density plots, without referencing the underlying (disjoint) data points? To summarize, while not the only option, I contend that disjointness provides a good default model for interactive graphics. 

### Associativity and unitality {#associativity-unitality}

The core idea of the model is preservation of the properties of set union. Among these, the two most important ones are associativity and unitality. Associativity ensures that we can perform the summary operations in any order, while unitality guarantees that empty data subsets are handled properly. Further, as discussed in Section \@ref(monoids), together, these two properties identify a central class of algebraic structures: monoids.

This connection between set union and monoids is not novel. For instance, monoids have long been known to have desirable properties for parallel computing [see e.g. @parent2018; @fegaras2017; @lin2013]. As a side note, the challenges we face when visualizing data are in some ways remarkably similar to those in parallel computing: often, we want to break the data into parts, compute some summaries, and the combine these back together to yield some meaningful aggregate. More broadly, monoids are also popular functional programming [see e.g. @milewski2018], and form the basis of certain algorithms in generic programming [@stepanov2009; @stepanov2013]. Finally, monoids and other concepts from category theory have also been used in certain areas of relational database theory [see e.g. @fong2019; @gibbons2018]

However, to my knowledge, I am the first to make the connection between monoids and many popular visualization types. As discussed in Section \@ref(visualization-category-theory), while there has been some limited number of applications of category theory to data visualization, the way I have used category-theoretic concepts differs significantly from all of these. On one hand, it is significantly more applied than some other past applications [see e.g. @beckmann1995; @hutchins1999; @vickers2012], due to the fact that it relates to concrete statistics, geometric objects, and interactive features, rather than broad ideas about the nature of the visualization process. On the other hand, it is also more theoretical than other applications, since it does not require any specific implementation; the work presented in this thesis is not a functional programming library. During my survey of the data visualization literature, I did not find any references explicitly linking common plot types such as barplots or histograms to groups and monoids. The only hints of similar ideas I have been able to find has been in the documentation of Crossfilter, a JavaScript library for exploring large multi-dimensional data sets [@crossfilter2023, @crossfilter2025]. However, this documentation only discusses properties like associativity and commutativity, without connecting them to algebraic structures.

Associativity and unitality have some interesting implications for data visualization systems. Firstly, due to the previously mentioned connections to parallel computing, associativity suggests that a data visualization system built around monoids can be easily parallelized. This enables improved performance and make it suitable for distributed computing. Secondly, unitality also has interesting consequences. Specifically, it ensures that empty data subsets have unambiguous representations, and, conversely, lack of unitality may cause ambiguities. For example, the average operator is not unital, since the average of an empty set is not defined. This can create issues when visualizing data, such as drawing a bar plot where some categories are empty. While we might choose to omit bars for empty categories, this leads to an ambiguous representation, such that absence of a bar can indicate one of two things: either there were *no* cases in the category and their average is undefined, or there *were* cases and their average was equal to the y-axis intercept. In contrast, since the sum is unital, the representation is always clear: a missing bar unambiguously indicates a sum of zero, regardless of whether there were cases in the category or not. This also provides a potential alternative perspective on a perennial debate in data visualization: should the base of the barplot always start at zero or not [see e.g. @cleveland1985; @wilkinson2012]? Since the monoidal unit represents the default state of "no data", it can serve as the default choice for the base, such that, for example, a barplot of products may have one as its base.  

### Groups and inverses {#discussion-groups-inverses}

As discussed in Section \@ref(groups-inverses), another useful property of set union which it may be useful to preserve is the presence of an inverse. Specifically, if we have two sets $A$ and $B$ and take their union $A \cup B$, we can always recover either set by taking the difference with the other: $A = (A \cup B) \setminus B$ and $B = (A \cup B) \setminus A$. While monoids do not necessarily possess an inverse, those that do are called groups. Groups are a well-known algebraic structure, and have certain advantages over monoids which will be reviewed in this section.

First and foremost, as discussed in Section \@ref(groups-inverses), groups allows us to preserve not only set union, but also set difference. This has an important implication for interactive visualization: it allows us to compare selections across multiple distinct groups. Specifically, while with monoids, we can compare data subsets of the form $A$, $A \cup B$, $A \cup B \cup C$, with groups, we can always compare $A$, $B$, and $C$ directly, as disjoint subsets. This identifies monoids as suitable for single-group (transient vs. none) selection, whereas (algebraic) groups are necessary for multiple persistent selection groups.  

More broadly, this distinction underscores an intriguing philosophical problem regarding the interpretation of data partitions: partitioning variables can sometimes be interpreted in two distinct ways. Specifically, in some cases, certain levels of a partitioning variable can also be interpreted as nested subsets of the others. Such is the case, for example, with the above-mentioned single-group linked selection: we can interpret the selected cases as a *special subset of all cases*. Likewise, another example, discussed in Section \@ref(groups-inverses), is interpreting smokers as a special subset of all study participants. However, in other cases, this nested interpretation is not appropriate. For instance, with a political party affiliation or gender variable, it is typically inappropriate to consider one level as nested within others; instead, a *direct* comparison between the different levels is usually desired. However, as I demonstrated in Section \@ref(groups-inverses), to preserve composition of objects, this direct comparison requires the presence of inverses, which imposes an additional constraint on the summary statistics we can visualize. Finally, it is important to note that this distinction is generally *not* encoded in the data itself. Common data structures such as string arrays or R's `factor`s typically do not specify whether factor levels represent nested or disjoint subsets, necessitating metadata knowledge. 

Second, under specific conditions, groups can also provide useful computational shortcuts.  For instance, in the context of linked selection, the inverse operator enables efficient updates. Specifically, consider a scenario with a million selected scatterplot points; if one point is deselected, the inverse operator allows us to subtract contribution of that single data point. In contrast, since monoids lack an inverse, all summary statistics need to be recomputed from scratch each time selection changes. Therefore, in the presence of large volume data sets, groups can provide a distinct advantage.

Whether to choose groups or monoids as the foundational algebraic model for objects within an interactive data visualization system presents an interesting design consideration. Fundamentally, both structures encode the concept of objects composed of parts, but they differ in the nature of this composition: monoidal composition implies nesting, whereas group composition allows for disjointness. Further, as discussed above, groups offer advantages such as supporting multi-group linked selection and enabling computational shortcuts. However, the definition of a group is also more restrictive than that of a monoid; for instance, the maximum and convex hull operators, while monoids, lack inverses and therefore do not satisfy the criteria for groups \@ref(groups-inverses). Overall, since monoids are the more general structure, and can be usefully combined with single-group linked selection as well as many other interactive features, I would in general recommend these as the basis of objects in interactive data visualization systems. However, data visualization practitioners should be cautioned against using multi-group selection in the absence of inverses, similar to how past authors have warned against stacking statistics other than sums (see Section \@ref(stacking-not-graphical)).

### Additional properties: Monotonicity, commutativity, and others

Beyond associativity, unitality, and inverses, set union possesses several other algebraic properties that may be useful to preserve in specific contexts. While I consider these somewhat less essential, as their absence does not inherently invalidate the part-whole model (discussed throughout Section \@ref(problems)), they can still offer certain advantageous in specific circumstances. Consequently, I believe their discussion here is warranted.

Of these additional properties of set union, monotonicity and associativity are, in my view, the two most significant (see Section \@ref(monotonicity-commutativity)). First, let's discuss monotonicity. In the context of set union, monotonicity implies that the cardinality of the union of two subsets is always greater than or equal to the cardinality of either subset: $\lvert A \cup B \rvert \geq \lvert A \rvert \land \cup B \rvert \geq \lvert B$. Therefore, it seems reasonable that our summary statistics should also perhaps exhibit this property, yielding greater values when summarizing larger data subsets. And indeed, monotonicity is often useful since many geometric objects also exhibit unidirectional growth: bars increase in height, circles expand in radius, and so forth. However, there are also some geometric representations which can depict additive changes in negative directions. For instance, as discussed in Section \@ref(monotonicity-commutativity), a polyline composed of vector segments can simultaneously "grow" in positive and negative directions along the x- and y-axes, through the addition of vectors with negative components. This means that, for depicting objects composed of parts, monotonicity is not strictly required, however, it does align with many common geometric object types such as bars or circular areas, and can also have certain other benefits, for instance for simplifying the calculation of axis limits (see Section \@ref(monotonicity-commutativity)). 

Second, as for commutativity, an operation is commutative if the order of the elements does not matter. For set union, this is always the case, by definition: sets are unordered collections, and, as such, $A \cup B = B \cup A$. However, in applied data analysis, data is frequently stored in two-dimensional tables with an inherent row order, and this can sometimes influence data summaries and their visual representation. For instance, lineplots are typically non-commutative: given that a line connects multiple points, different row arrangements will yield visually distinct lines^[Where appropriate, the issue can be mitigated by sorting the data points by the x-axis variable.]. Furthermore, certain plot types may be commutative in some systems and not others. For example, while most systems render barplot and histogram highlighting commutatively - representing highlighted cases as a single segment at the base - some systems highlight individual cases as lines, potentially creating diffuse, striped patterns (see, for example, Figure \@ref(fig:xlisp-stat)). Regarding summary statistics, while many common operators such as sums, averages, or maximums are commutative, there also are some examples of non-commutative monoids, such as string concatenation and matrix multiplication, which could, hypothetically, be used for summaries. Overall, commutativity generally appears to be desirable, as it renders row order unimportant, thereby reducing cognitive load and offering potential advantages in the context of distributed computing or resampling techniques (e.g. bootstrapping).

Finally, there are few other properties of set union which, in my view, are less important but which it may be useful to briefly touch on here. The first is idempotence. Given any set $A$, the union of that set with itself is itself, $A \cup A = A$. While some statistical operators, such as minimum, maximum, and convex hull, exhibit this property, I have not identified a specific use case for it in graphics, interactive or otherwise. Second, just as the monoidal neutral element corresponds to the empty set ($\varnothing$, such that $A \cup \varnothing = A$), there may also be a so-called terminal element or object [see e.g. @fong2019] that behaves like the universal set $\Omega$ under union, such that $A \cup \Omega = \Omega$. For instance, in the case of sums, products, or maximums of numbers, our data may contain special $\infty$ or `Inf` value, which, when combined with any other value, is always yielded back: $x + \infty = x \cdot \infty = \max(x, \infty) = \infty$ for all $x$. Hypothetically, this terminal element could be leveraged for more efficient aggregation, since we know that, once we encounter this terminal value, we can terminate the aggregation loop. However, I have not been able to think of any examples, beyond trivial ones^[E.g. logical conjunction and disjuction with $\text{FALSE}$ and $\text{TRUE}$], where the terminal element would be easily and generally representable by graphics (for instance, how do we represent a bar of height $\infty$?), and, as such, the utility of this hypothetical optimization is debatable^[Further, checking for the terminal element would require adding a branching (`if`) statement inside the body of the aggregation loop. In performance-critical code sections, branching can introduce performance bottlenecks, especially when the branching condition is unpredictable [see e.g. @fabian2018]. While terminal values may occur infrequently (a data set may not have any), making branching fairly predictable, it may still be more efficient, on balance, to simply aggregate over all data, even if that includes terminal values.]. Lastly, set union distributes over set intersection. However, the relation of set intersection to the part-whole object model is not clear, particularly when the parts are disjoint subsets. An analogue to set intersection would require that the summary statistic has a complementary binary operation, distinct from an inverse (which is identified with set difference, see \@ref(discussion-groups-inverses)), which would need to distribute over the primary aggregation operation. Furthermore, unlike inverses and neutral elements, this complementary distributive operation does not need to be unique (for instance, maximum distributes over both products and minimums). As such, I do not really see a benefit of identifying a single such operation.

### Model constraints

A potential point of contention is that the constraints the model imposes are too rigid. Specifically, requiring all plots to be groups or monoids would exclude a significant fraction of popular visualization types and styles. Conversely, many non-monoidal plot types, such as boxplots, have been successfully implemented alongside linked selection in the past [see e.g. @theus2002; @urbanek2011]. Thus, a valid question one might have is whether the limitations imposed by the model are justified by its practical utility.

I contend that this is the wrong perspective, since the model is not meant to be prescriptive. Instead, it simply provides a framework for identifying visualizations which are "naturally" compatible with features like linked selection. We are free to violate this naturality whenever we see fit, however, we should also be aware of the fact that doing this may require some ad hoc design choices. For instance, since a boxplot box is not a monoid, there is no such thing as highlighting a "part of a boxplot box". We can solve this issue by e.g. plotting the highlighted cases as a second box next to the original one [see e.g. @theus2002; @urbanek2011], however, we may lose some of the nice properties of part-whole relations in monoidal plots (see Section \@ref(stacking-part-whole)). Thus, the real strength of the model lies in its ability to identify inherent trade-offs in the design and implementation of interactive graphics.  

### Potential future directions

The model described in Section \@ref(problems) is, in its current state, far from exhaustive. Ultimately, it amounts to conceptualizing the data visualization pipeline as a functor, from the category of data subsets to the category of geometric objects (with category of summary statistics being the intermediate domain). While I believe this arguably simple model allow us to capture and reason about a fairly large number of common graphics, .

One already discussed feature of the model which may be further refined is the set of algebraic properties of set union that we wish the model (functor) to preserve. Specifically, I identified associativity and unitality as the two key properties which it is necessary to preserve if we wish to render meaningful part-whole relationships. I also discussed inverses, monotonicity, and commutativity as other potentially desirable properties. However, a deeper analysis of the properties of graphics might reveal some of these additional properties to be essential, or even identify other relevant properties that should be incorporated into the model. Furthermore, while I identified the fundamental operation as set union, a case could perhaps be made that perhaps, given that real-world data is often ordered and contains duplicates, the fundamental operation may be better identified as the union of bags or concatenation of tuples (ordered collections). Exploring the implications of this alternative perspective could yield valuable insights.

More fundamentally, beyond the simple category-theoretic concepts used throughout this thesis, there may be other, more advanced concepts which may be useful for describing the behaviour of graphics. For instance, since we describe plots as functors, a preliminary idea I have considered is to model interactive representation switching as natural transformation [transformation of one functor into another which respects composition of morphisms, see @fong2019]. Moreover, just as we can conceptualize a visualization as a functor from data to geometric objects, we could perhaps conceptualize the process of reasoning about a plot as the opposite functor, with the two functors forming an adjunction [@fong2019]. These are both merely tentative ideas, and there are likely many other concepts in applied category theory that could be hypothetically useful for advancing theory of (interactive) graphics.

Further, an important gap in the model concerns plots in which geometric objects do not represent distinct data entities. Two-dimensional kernel density plots offer an interesting example, as each polygon in them represents an isopleth over a joint probability density, and thus does not uniquely identify a distinct set of data points. However, my review of example galleries from popular data visualization libraries such as `ggplot2` [@wickham2016] and `matplotlib` [@matplotlib2025], revealed only few such instances. While other examples undoubtedly exist within the data visualization and scientific literature, it seems that, in the vast majority of popular plot types, individual geometric objects within the same layer represent distinct subsets of the data. Nevertheless, extending the algebraic model to include non-disjoint data representations could constitute an interesting avenue for future research.  

## Features and limitations of the delivered software

During this project, I had time to think through and test out different implementations of the various features of the interactive data visualization system. While I succeeded in some areas, there were also others which could still use further improvement. 

### Scope and features

Currently, `plotscaper` offers a number of features for creating and manipulating interactive figures, many of which have been discussed either in Section \@ref(system) or in Section \@ref(applied-example). While these features can be used to create a fairly substantial range of useful interactive figures, there are of course many limitations and gaps when compared to other, better-established established data visualization packages. These will be discussed in this section. 

First, `plotscaper` currently offers six plot types: scatterplots, barplots, histograms, fluctuation diagrams, histograms, two-dimensional histograms, and parallel coordinate plots. Additionally, normalized representations, such as spineplots and spinograms, are available for all aggregate plots. While these six plot types can already be used to create a wide range of useful interactive figures, there are numerous other plots which could be highly desirable in applied data analysis. Density plots, radar plots, mosaic plots, and maps, for example, may be compatible with the `plotscaper` model and could be potential additions in future versions of the package. Furthermore, a relatively simple addition which could be useful would be horizontal barplots. Finally, all plots are specified nominally, meaning that, for example, to add a scatterplot, we call the `add_scatterplot` function. While a system for specifying plots declaratively in R would be appealing, implementing such a system has presented challenges, and the issue will be discussed in more depth in Section \@ref(declarative-schemas). This somewhat limits the package's extensibility, however, users familiar with TypeScript can still create arbitrary new plot types using `plotscape` code.

Second, as described in Section \@ref(system), `plotscaper` currently uses a simple transient-persistent product model for linked selection. Users can transiently select cases or assign them to permanent groups via click or click-and-drag interactions. The combination of transient and persistent selection results in $2 \times 4 = 8$ possible selection states per case.  This model facilitates simple conditional queries, such as "how many cases assigned to group X are also transiently selected?" For example, users can assign a bar in a barplot to a permanent group and transiently select a point cluster in a scatterplot to easily identify cases belonging to both. However, this model is, of course, fairly limited: it does not allow to combine selections with logical operators such as negation or exclusive difference. Implementing more comprehensive selection operators [see e.g. @urbanek2003; @urbanek2011; @theus2002; @wilhelm2008] would enable more sophisticated analytical workflows. Finally, selection geometry is currently restricted to point-clicks or rectangular regions defined by clicking-and-dragging. While these are fairly simple and intuitive, alternative selection strategies, such as a movable selection brush, a circular region expanding around a point, or arbitrary lasso/polygon, could prove beneficial in specific contexts [see e.g. @wills2008].

Third, there are several other interactive features which could also be further polished, particularly. For instance, while it is currently possible to sort barplot bars by height, expanding sorting capabilities to other plot types and implementing alternative sorting schemes, such as sorting by the heights of highlighted segments, could be beneficial. Similarly, manual axis reordering in parallel coordinate plots could prove valuable. For continuous scales, the ability to interactively switch to alternative transformations, such as a logarithmic scale, would also be useful. Regarding querying, while the system currently supports querying of custom statistics aggregated via `Reducer`s, expanding the scope to non-monoidal statistics warrants consideration, as the requirements for statistics displayed in the query table are less stringent than those mapped to visual aesthetics. More generic model for parametric interaction (which will be touched on in Section \@ref(declarative-schemas)) would also be useful. Finally, there is wide a range of other, more ambitious features such as animation (time aesthetic) or semantic zooming, which could provide great utility in certain use-cases but which also present a substantial implementation overhead. 

Third, plot and figure customization in `plotscaper` is currently fairly limited. While users can adjust attributes such as the figure layout and plot scales, surface-level graphical attributes like colors, margins, and axis styling are not yet exposed. Although the support for adding these customization options does exist, their implementation was not prioritized. They may be added in future package versions, when the API stabilizes more.

Fourth, as will be discussed in more depth in Section \@ref(performance), while `plotscaper` provides decent performance even on moderately sized data sets (thousands or tens of thousands of data points), a particularly valuable feature for performance-sensitive applications would be the ability to register custom rendering backends. For example, GPU-based rendering could enable visualization of much larger datasets [to get some ideas of the scope, see e.g. @highschartsboost2022; @lekschas2024; @unwin2006]. Conversely, having the ability to switch to an SVG-based rendering backend could also be beneficial in certain scenarios. Currently, the rendering logic in `plotscaper` is fairly tightly integrated with the rest of the system, however, decoupling this logic would definitely be a priority for a potential major rewrite.  

Fifth and finally, there are also many opportunities for additional features in the client-server communication. For instance, the ability to render arbitrary graphical elements (e.g. computed regression lines) within existing `plotscaper` plots or figures would be highly beneficial. Similarly, features such as dynamically switching `Reducer`s or sending the underlying summary statistics to the server (similar to querying) warrant consideration. Furthermore, more fine-grained selection controls could also prove useful. mportantly, many of these features may be implementable relatively quickly, as the underlying infrastructure is already in place.

### Declarative schemas and extensibility {#declarative-schemas}

As discussed in Section \@ref(system), one limitation of the delivered system is the absence of a simple declarative schema-based approach for specifying plots. While declarative schemas, popularized by the Grammar of Graphics [GoG, @wilkinson2012], have become a highly popular in modern data visualization libraries [see e.g. @wickham2016; @satyanarayan2016; @plotly2023], my system deviates from this paradigm. Although certain core components within `plotscape` are modeled declaratively (as shown in Section \@ref(summaries)), the overall plot definitions remain largely procedural, and the user-facing API (`plotscaper`) relies on nominal plot types^[Currently there are six different types of plots implemented.]. While users familiar with TypeScript can still create new plot types using the underlying `plotscape` code, this approach does limit the extensibility of the software for R users.  

However, as I argued in Section \@ref(variables-encodings), the issue underlying this lack of a full declarative plot specification is not unique to my system. Instead, I contend that many currently available declarative data visualization libraries offer partial or incomplete solutions, and frequently conceal key implementation details [see also @wu2024]. For example, operations like grouping, binning, stacking, and normalization are frequently handled implicitly, with limited user control. In essence, there are gaps in the GoG model which, as far as I am aware, none of the currently available declarative data visualization libraries address. To quote @wu2024:

> Despite these broad-ranging benefits, we observe that this [GoG-based] semantic delineation obscures the process of visual mapping in a way that makes it difficult to reason about visualizations and their relationships to user tasks.
> 
> We observe that the core reason for this dissonance between visualization specifications and functions is that the Visual Mapping step represents two distinct substeps. The first involves additional transformations over the data tables in order to compute, e.g., desired statistics and spatially place marks that are specific to the visualization design. We term these Design-specific Transformations. The second is the visual encoding that maps each row in the transformed table to a mark and data attributes to mark attributes using simple scaling functions.

During the course of my project, I have independently reached several conclusions similar to those of @wu2024^[I became aware of Eugene Wu's work only after he contacted me concerning the publication of a paper I co-authored during this project's development: @bartonicek2024]. Most importantly, I have also come to the conclusion that separating the aggregation (Design-specific Transformation) and scaling (visual encoding) stages is highly desirable. By mapping data variables directly onto aesthetics, most declarative data visualization libraries entangle these two concepts together, hindering efficient and generic implementation of interactive features like representation switching.

However, specifying declarative schemas for separate aggregation and visual encoding stages presents challenges. While working on `plotscaper`, I attempted to devise a method for specifying these schemas, but despite some partial successes (see Section \@ref(aggregation-summaries)), a truly comprehensive solution ultimately eluded me. I opted to make this implementation gap explicit in `plotscaper` by specifying plots nominally. Nevertheless, I believe that a useful outcome of this effort is that I have been able to identify three key factors which contribute to this difficulty of developing declarative schemas for interactive data visualization, and these are detailed below. 

The first factor which complicates declarative schemas in interactive graphics is the already mentioned weak correspondence between data variables and visual encodings. Specifically, as discussed in Section \@ref(variables-encodings), often, what we plot are *not* variables found in the original data, but instead variables which have been computed or derived in some way. This is exists even in static graphics; however, it is further amplified in interactive graphics, where dynamic remapping of variables to aesthetics is often desirable (such as when transforming a barplot to a spineplot). Thus, directly mapping data variables to aesthetics seems to be an inadequate approach. However, to map derived variables to aesthetics, we have to explicitly specify *what* we compute and *how*. For example, to fully specify a histogram, we have to describe the fact that we want to bin cases based on some continuous variable, compute the counts within bins, stack these counts, also within bins, and finally map the bin borders to the x-axis variable and the counts to the y-axis variable. While I have been able to get reasonably far with providing a mechanism for specifying declarative schema for this process (see Section \@ref(variables-encodings)), ultimately, I was not able to integrate it with the rest of the system without some amount of procedural code.

The second factor which presents a challenge to declarative schemas in interactive data visualization is the hierarchical nature of graphics (see Section \@ref(hierarchy)). Specifically, while in static graphics, we can often act as if the data underlying our plots is "flat", interactivity necessitates hierarchical data structures. Interactive features like linked selection and parametric manipulation trigger updates to summary statistics, requiring hierarchical organization for efficient updates, especially when features like stacking and normalization are present (see Sections \@ref(transforming-summaries) and \@ref(aggregation-summaries)). Furthermore, hierarchical data can also be leveraged to provide more efficient scale and axis updates (see Section \@ref(plot-scales)), and display multiple levels of information during querying (e.g. displaying summary statistics for both objects and segments). However, specifying declarative schemas with hierarchical data is inherently more challenging than with flat data, due to both increased surface area (multiple data sets) and hierarchical dependencies. Currently, in `plotscape`, these hierarchical dependencies are represented by aggregated variables having references to parent-level variables, factors over which they have been computed, and the reducers which had been used to compute them. These references can then be leveraged by specialized operators such as `Reduced.stack` and `Reduced.normalize`, see Section \@ref(aggregation-summaries). An alternative, unexplored approach could be to partially "flatten" this hierarchical structure via SQL-like table joins (i.e. storing a secondary key to parent data and left-joining). However, the issue of each aggregated variable having to "remember" additional metadata, such as the factor cardinality and aggregation strategy (see Sections \@ref(aggregation) and \@ref(aggregation-summaries)), still remains.

The third and final factor which makes declarative schemas challenging is reactivity. Interactive visualizations need to, by definition, respond to user input; however, what complicates the issue is that the input may affect different stages of the data visualization pipeline, and changes need to be propagated accordingly. For instance, while shrinking or growing the size of points in a scatterplot is a purely graphical computation, shrinking or growing the width of histogram bins requires recomputation of the underlying data, which has downstream effects on the rest of the visualization. Therefore, any declarative schema which wants to incorporate reactivity needs to be able to handle reactive parameters with hierarchical dependencies. While signals (see Section \@ref(signals)) may offer one possible general solution, as discussed in Section \@ref(reactivity-solution), while developing `plotscape` I found the developer ergonomics of signals lacking. Furthermore, I eventually reached the conclusion that most of desired reactivity could be effectively integrated at specific, discrete points within the four stages of the data visualization pipeline. Together with the rest of the pipeline, these points would form form a linear dependency chain, greatly simplifying the reactive graph. Although I did not manage to develop a general declarative mechanism for specifying reactive parameters in this way in time, the following list identifies these potential entry points of reactivity:

- Pre-partitioning: Data streaming, partitioning parameter updates (e.g. histogram bin width, selection)
- Pre-aggregation: Aggregation parameter updates (e.g. regression line regularization)
- Pre-scaling: Scale parameter updates (e.g. sorting, size/alpha adjustments)
- Pre-rendering: Surface-level changes (e.g. layout modifications, DOM resize events)

To summarize, modeling interactive data visualizations declaratively presents significant challenges. Specifically, the weak correspondence between data variables and aesthetics, the inherent hierarchical nature of graphics, and the presence of reactive parameters make the seemingly appealing model of "assign variable $x$ to aesthetic $y$" inadequate for implementing many popular interactive graphics efficiently. Importantly, I contend that these issues are not unique to `plotscaper` and extend to other data visualization systems as well. While a truly general solution may require a significant amount of work, I hope that, by identifying and classifying these issues, this thesis provides insights that may be capitalized on by future research. 

### Performance {#performance}

A key aspect to discuss is the performance of the delivered software. As discussed in Section \@ref(background), responsiveness is crucial for interactive data visualization. Slow systems frustrate users, negating any potential benefits of sophisticated features. Given today's expectation of highly responsive GUIs and the growing size of data sets, performance is an important concern.

Despite not being the sole focus, I am happy to report that `plotscaper` achieves solid performance even on moderately sized data sets. Specifically, I was able to achieve highly responsive point-wise linked selection on data sets with tens of thousands of data points (such as the `diamonds` data set; see the [Performance vignette](https://bartonicek.github.io/plotscaper/articles/performance.html) on `plotscaper`'s package website). This performance is, of course, largely attributable to the highly optimized nature of JavaScript engines such as V8 rather than any targeted optimization. Nevertheless, I did try to be generally mindful of performance while developing the package, by adhering to data oriented practices such as relying on the Structure of Arrays (SoA) data layout.  

Profiling of the package revealed that the primary performance bottleneck was rendering. This manifests quite clearly even on the macro level: figures composed entirely of aggregate plots tend remain responsive even with fairly large data sets, whereas figures with multiple bijective plots (such as scatterplots or parallel coordinate plots) can start to become sluggish even with moderately-sized data sets (thousands or tens-of-thousands of data points). Therefore, alternative rendering strategies may offer significant performance gains. Currently, `plotscaper` relies on the default HTML5 `canvas` rendering context, which provides a simple interface for rendering 2D graphics. GPU-based rendering, particularly WebGL [@webgl2025], could lead to significant performance improvements.

Beyond rendering, there are other parts of the system that may provide opportunities for easy performance wins. For instance, currently, as mentioned in Section \@ref(plot-mousemove), querying and selection both rely on a naive collision-detection loop, such that all objects are looped through until matches are found. An approach using spatial data structures, for example quadtrees [@samet1988], could significantly accelerate these searches. Another area which may be open to performance improvements is reactivity. While reactivity is implemented using the - fairly performant - observer pattern, and event callbacks throttled on hot paths (such as mousemove events), further optimization of the reactive system may be possible. Finally, though less performance critical, client-server communication could also be made more performant. Currently, it is implemented with WebSockets [@cheng2024; @mdn2024g] and JSON-based payloads, which incurs serialization/deserialization overhead on each message. An alternative protocol like TCP could mitigate this.

# Discussion

This thesis explored the role of interaction in data visualization pipelines. More specifically, I investigated how interaction affects the four stages of data visualization pipelines - partitioning, aggregation, scaling, and rendering - and explored the inherent problems and challenges. The main thrust of my argument was that the popular model implied by the Grammar of Graphics [@wilkinson2012], which treats statistics and geometric objects as independent entities, is insufficient for describing the complex relationships between the components of interactive figures [see also @wu2024]. As an alternative, I proposed a simple category-theoretic model, conceptualizing the data visualization pipeline as a functor. 

The essence of the proposed model is the idea that, initially, all visualizations begin as a collection of data subsets. In almost all cases, this collection is not arbitrary, but instead has a special kind of structure: it is a hierarchy of data partitions ordered by set union. To maintain consistency during interactions like linked selection, the subsequent steps of the data visualization pipeline should preserve this structure. In plain words, the geometric objects in our plots and the underlying summary statistics should *behave like set union*. Formally, this means that the mappings from data subsets to summary statistics and from summary statistics to geometric objects should be functors. More specifically, using the properties of set union, we can identify the underlying algebraic structures as either groups or monoids: the operations in our plots should be associative and unital, and also potentially invertible, monotonic, and commutative. When these algebraic constraints are satisfied, the geometric objects in our plots will compose well under selection, meaning that their parts add up to a meaningful whole.     

To validate the proposed model, I developed `plotscaper`, an interactive data visualization R package. In fact, this implementation served as a crucial feedback loop, as many of the theoretical concepts emerged from practical challenges that I encountered during the design of the system. By translating theory into code, I was able to empirically test and refine assumptions about the structure and behavior of interactive data visualizations. 

However, `plotscaper` was also developed to provide a practical tool for data exploration, not just theory testing. As outlined in Section \@ref(background), within the R community, there is currently no shortage of interactive data visualization packages and frameworks; however, many of these offer only fairly limited, shallow kinds of interactivity. Implementing more complex kinds of interaction, such as linked selection, representation switching, and parametric interaction (see section \@ref(common-features)) often requires substantial programming expertise and time-investment, creating a barrier to entry for casual users and solo data analysts [see e.g. @batch2017]. Thus, one of the goals of the project was to try to address this perceived lack of simple and practical tools for interactive data exploration. This hypothesis seems to have been largely proven correct by the package's moderate success - despite its relatively experimental status in comparison to other, far larger and better-established interactive data visualization frameworks, `plotscaper` has been downloaded over `r cranlogs::cran_downloads("plotscaper", from = "2024-10-01", to = Sys.Date())$count |> sum()` times^[The number only includes downloads from the RStudio CRAN mirror.], in the `r as.numeric(Sys.Date() - as.Date("2024-10-19"))` days since its initial release.

However, despite all of these relative successes, both the theoretical model and its practical implementation in `plotscaper` have certain important limitations. These will be the subject of the next few sections.
 
## Limitations of the theoretical model

It is first important discuss the limitations of the theoretical model presented in this thesis. This model, described in Section \@ref(problems), conceptualizes the data visualization pipeline as a structure-preserving mapping, also known as a functor. Specifically, a key initial assumption is that, when visualizing data, we start with a hierarchy of data subsets. These subsets are disjoint (share no overlapping data) and are ordered by set inclusion/union. To produce meaningful graphics, the subsequent steps of the data visualization pipeline should preserve this inherent structure. In plain words, the transformations of data into summary statistics and of statistics into aesthetics should also *behave like set union*. In category theoretic terms, this means that the summarizing function and the rendering process have to be functors. Based on the properties of set union, this naturally identifies algebraic structures known as groups and monoids. 

The proposed model of the data visualization pipeline naturally raises several questions and potential criticisms, some of which have been already discussed in Section \@ref(problems). These will be further reviewed and expanded upon in the following subsections. My objective is to clearly articulate the model's advantages, while also acknowledging its limitations and suggesting opportunities for future work.

### Why the model is necessary

First, let's briefly reiterate why the model is even necessary. As discussed in Section \@ref(partitioning), when visualizing data, we render geometric objects representing parts of our data set. We establish these parts by conditioning on discrete variable, such as a categorical variable in a barplot or a binned continuous variable in a histogram. Often, we also want to further subdivide these parts by conditioning on another variable or variables. In particular, in interactive data visualization, this nested conditioning is a pre-requisite of linked selection, which naturally partitions the data according to the selection status.

After being subdivided into parts, the data is summarized via summary statistics and rendered as geometric objects. Importantly, the rendered objects should reflect the hierarchical part-whole structure created by the partitioning. This is the case with techniques like stacking, which represent the subdivided data subsets as highlighted parts of whole objects. While alternative techniques such as dodging or layering do exist, they provide a subpar solution for interactive graphics (see Section \@ref(stacking-part-whole)). 

However, a key insight is that the graphical representation is *not* independent of the way we summarize our data. Specifically, as discussed in Section \@ref(stackable-or-not), regarding stacking, past data visualization researchers have noted the fact that while some summary statistics such as sums or counts can be effectively stacked, others like averages or quantiles cannot [see e.g. @wilke2019; @wills2011; @wu2022]. For instance, while the sum of sums or maximum of maximums represent valid overall statistics, the average of averages is different from grand mean.

Thus, when implementing data visualization features like linked selection, it is really useful to know which summary statistics will play well with the part-whole (stacking) model. As demonstrated throughout Section \@ref(problem), this idea can be formalized by identifying the fundamental underlying operation as set union - the data representing parts of objects is combined together to form a whole, and so should the derived summary statistics. This naturally leads to some algebraic ideas that will be explored in the following subsections.

### Disjointness

A point which is important to discuss early is that the partition hierarchy model does not describe *all* types of plots in use today. For instance, it precludes visualizations where geometric objects within the same graphical layer represent overlapping data subsets, as seen, for example, in certain visualizations of set-typed data [see e.g. @alsallakh2014] or two-dimensional kernel density plots (see Section \@ref(comparison-disjointness)). However, these visualizations represent a fraction of available plot types. In the vast majority of "standard" plots - including barplots, histograms, scatterplots, density plots, heatmaps, violin plots, boxplots, lineplots, and parallel coordinate plots - each geometric object represents a distinct subset of cases^[As a short exercise, I tried going through `ggplot2`'s list of [`geoms`](https://ggplot2.tidyverse.org/reference/) and identifying all non-disjoint data representations. The only examples I have been able to find have been find were `geom`s based on two-dimensional kernel density estimates, i.e. `geom_density_2d` and `geom_density_2d_filled` (each line or polygon represents densities computed across all datapoints).].

Further, in Section \@ref(comparison-disjointness), I argued that this tendency towards disjoint data representations is not a mere convention or accident, but instead stems from a fundamental naturality of disjointness. Disjointness underlies many fundamental concepts in statistics, programming, and mathematics, and, in general, seems to align better with our cognitive processes (see Section \@ref(comparison-disjointness)). Put simply, reasoning about objects which are disjoint is easier than about objects which are entangled [see also @hickey2011].

Thus, while not the exclusive model, I argue that disjointness provides a good general model for data visualization, just as it does in the other fields mentioned above. Furthermore, disjointness seems to be particularly well-suited to interactive data visualization, and, conversely, with non-disjoint data representations, many interactive features may prove difficult or even impossible to implement. For instance, how would one implement non-trivial linked selection (or querying) with a two-dimensional kernel density plot, without referring to the underlying (disjoint) data points? Like manipulating objects in the real world, I content that it is far easier to interact with geometric objects that represent distinct data entities.  

###

Another potential point of contention are the limitations that the model imposes. Requiring that all graphics and the underlying summary statistics are (monotonic, commutative) monoids or groups excludes a non-trivial fraction of popular plot types. For instance, boxplots are entirely made out of non-monoidal summary statistics, nevertheless, they have been successfully implemented alongside linked selection in some interactive data visualization packages [see e.g. @theus2002; @urbanek2011]. However, the point of the model is not to say that certain kinds of visualizations can never be reconciled with linked selection, instead, it is to easily identify certain "natural" types of visualization where linked selection is inherently compatible. We are free to violate this naturality when we see fit, however, we should be aware of the fact that the reconciliation of these non-compliant visualizations may require some ad-hoc solutions. 

## Limitations of the software

During this project, I had time to think through and test out different implementations of the various features of the interactive data visualization system. While I succeeded in some areas, there were also areas which could still use some improvement. 

As discussed in Section \@ref(system), one shortcoming of my system is the absence of a simple declarative schema for specifying interactive graphics. While some parts of the internal codebase such as [REFERENCE WRANGLER] showed some promise, ultimately, plot were defined in a largely procedural way, and the package's user-facing API (`plotscaper`) used nominal plot-type functions. As I argued earlier, I believe this challenge is not unique to my system; many other, overtly declarative data visualization systems offer only partial solutions [see also @wu2024]. There are important reasons for why coming up with declarative schemas for interactive graphics is not easy, such as the lack of a direct correspondence between data variables and encodings (see Section [REFERENCE]) or the hierarchical nature of graphics itself (discussed e.g. in Section \@ref(hierarchy)), 

Further, 




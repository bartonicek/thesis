# Discussion

This thesis explored the role of interaction in data visualization pipelines. More specifically, I investigated how interaction affects the four stages of data visualization pipelines - partitioning, aggregation, scaling, and rendering - and explored the inherent problems and challenges. The core argument was that the popular model implied by the Grammar of Graphics [@wilkinson2012], which treats statistics and geometric objects as independent entities, is insufficient for describing the complex relationships between the components of interactive figures [see also @wu2024]. As an alternative, I proposed a simple category-theoretic model, conceptualizing the data visualization pipeline as a functor. 

The essence of the proposed model is as follows. Ultimately, when we visualize, we want to represent our data with a set of geometric objects. To do this, we need to break our data into a collection of subsets. This collection of subsets is not arbitrary, but, most of the time, has a very specific kind of structure: a hierarchy of data partitions ordered by set union. To maintain consistency during interactions like linked selection, the subsequent steps of the data visualization pipeline should preserve this structure. In plain words, the geometric objects in our plots and the underlying summary statistics should *behave like set union*. In category theoretic terms, this means that the mapping from data subsets to summary statistics should be a functor, and so should be the mapping from summary statistics to geometric objects. Further, using the properties of set union, we can identify specific algebraic structures that our statistics and objects should conform to: groups and monoids. Specifically, to behave consistently, the operations underlying our plots should be associative and unital, and potentially also invertible, monotonic, and commutative. When these algebraic constraints are satisfied, the geometric objects in our plots will compose well, meaning that their parts will add up to a meaningful whole, and this makes linked selection consistent.     

To test the proposed model, I developed `plotscaper`, an interactive data visualization R package. In fact, this implementation served as a crucial feedback loop for the development of the model, as many of the theoretical concepts emerged from practical challenges that I encountered during the design of the system. By translating theory into code, I was able to empirically test and refine assumptions about the structure and behavior of interactive data visualizations. 

However, `plotscaper` was also developed to provide a practical tool for data exploration, not just theory testing. As outlined in Section \@ref(background), within the R ecosystem, there is currently no shortage of interactive data visualization packages and frameworks; however, many offer only fairly basic interactive features out of the box. Implementing complex features such as linked selection, representation switching, or parametric interaction (see section \@ref(common-features)) often requires substantial programming expertise and time-investment, creating a barrier to entry for casual users [see e.g. @batch2017]. Thus, a secondary goal of this project was to try to address this perceived gap, and this goal seems to have been met with moderate success. Despite its experimental status and the fact that it is competing with a number of far larger and better-established interactive data visualization frameworks, `plotscaper` has been downloaded over `r cranlogs::cran_downloads("plotscaper", from = "2024-10-01", to = Sys.Date())$count |> sum()` times^[The number only includes downloads from the RStudio CRAN mirror.], in the `r as.numeric(Sys.Date() - as.Date("2024-10-19"))` days since its initial release.

Despite all of these relative successes, both the theoretical model and its practical implementation in `plotscaper` have, of course, their limitations. These will be the subject of the next few sections.
 
## Limitations of the theoretical model

First, it is important discuss the limitations of the theoretical model presented in this thesis. This model, described in Section \@ref(problems), conceptualizes the data visualization pipeline as a structure-preserving mapping, also known as a functor. Specifically, a key initial assumption is that, when visualizing data, we start with a hierarchy of data subsets. These subsets are disjoint (share no overlapping data) and are ordered by set inclusion/union. To produce meaningful graphics, the subsequent steps of the data visualization pipeline should preserve this inherent structure. In plain words, the transformations of data into summary statistics and of statistics into aesthetics should also *behave like set union*. In category theoretic terms, this means that the summarizing function and the rendering process have to be functors. Based on the properties of set union, this naturally identifies algebraic structures as groups and monoids. 

This proposed model of the data visualization pipeline naturally raises several questions and potential criticisms, some of which have been already discussed in Section \@ref(problems). These will be further reviewed and expanded upon in the following subsections. My objective is to clearly articulate the model's advantages, while also acknowledging its limitations and suggesting opportunities for future work.

### Why the model is necessary

First, let's briefly reiterate why the model is even necessary. As discussed in Section \@ref(partitioning), when visualizing data, we render geometric objects representing parts of our data set. We establish these parts by conditioning on discrete variable, such as a categorical variable in a barplot or a binned continuous variable in a histogram. Often, we also want to further subdivide these parts by conditioning on another variable or variables. In particular, in interactive data visualization, this nested conditioning is a pre-requisite of linked selection, which naturally partitions the data according to the selection status.

After being subdivided into parts, the data is summarized via summary statistics and rendered as geometric objects. Importantly, the rendered objects should reflect the hierarchical part-whole structure created by the partitioning. This is the case with techniques like stacking, which represent the subdivided data subsets as highlighted parts of whole objects. While alternative techniques such as dodging or layering do exist, they provide a subpar solution for interactive graphics (see Section \@ref(stacking-part-whole)). 

However, a key insight is that the graphical representation is *not* independent of the way we summarize our data. Specifically, as discussed in Section \@ref(stackable-or-not), regarding stacking, past data visualization researchers have noted the fact that while some summary statistics such as sums or counts can be effectively stacked, others like averages or quantiles cannot [see e.g. @wilke2019; @wills2011; @wu2022]. For instance, while the sum of sums or maximum of maximums represent valid overall statistics, the average of averages is different from grand mean. To formally describe this property of "stackability", we can leverage some fundamental ideas from category theory/abstract algebra.

By grounding our reasoning in this algebraic model, we can quickly identify which combinations of statistics and geometric objects will behave well under linked selection. For instance, since we know that the average operator is not monoidal, we know that it will not be stackable and will pose certain challenges if we try to implement it alongside linked selection. Further, the model also allows us to identify what *kinds* of linked selection will work. For instance, while the maximum and the convex hull operators are monoids, they both lack inverse (they are not groups, in the algebraic sense), and thus we know that they will be able to display the selection of one highlighted group, however, they will not work for comparisons of multiple selection groups (see Section \@ref(groups-inverses)). 

### Disjointness

A point which is important to discuss is that the partition hierarchy model does not describe *all* types of plots in use today. For instance, it precludes visualizations where geometric objects within the same graphical layer represent overlapping data subsets, as seen, for example, in certain visualizations of set-typed data [see e.g. @alsallakh2014] or two-dimensional kernel density plots (see Section \@ref(comparison-disjointness)). However, these visualizations represent a fraction of available plot types. In the vast majority of "standard" plots - including barplots, histograms, scatterplots, density plots, heatmaps, violin plots, boxplots, lineplots, and parallel coordinate plots - each geometric object represents a distinct subset of cases^[As a short exercise, I tried going through `ggplot2`'s list of [`geoms`](https://ggplot2.tidyverse.org/reference/) and identifying all non-disjoint data representations. The only examples I have been able to find have been find were `geom`s based on two-dimensional kernel density estimates, i.e. `geom_density_2d` and `geom_density_2d_filled` (each line or polygon represents densities computed across all datapoints).].

Further, in Section \@ref(comparison-disjointness), I argued that this tendency towards disjoint data representations is not a mere convention or accident, but instead stems from a fundamental naturality of disjointness. Disjointness underlies many fundamental concepts in statistics, programming, and mathematics, and, in general, seems to align better with our cognitive processes (see Section \@ref(comparison-disjointness)). Put simply, reasoning about objects which are distinct is easier than about objects which are entangled [see also @hickey2011].

Thus, while not the only option, I argue that disjointness provides a good default model for data visualization, just as it does in the other fields mentioned above. Furthermore, disjointness seems to be particularly well-suited to interactive data visualization, and, conversely, with non-disjoint data representations, many interactive features may prove difficult or even impossible to implement. For instance, how would one implement non-trivial linked selection with a two-dimensional kernel density plot, without referring to the underlying (disjoint) data points? To summarize, in general, objects which represent distinct data entities are far easier to reason about and manipulate interactively.

### Associativity and unitality

The core idea of the model is preservation of the properties of set union. Among these, the two most important ones are associativity, allowing for arbitrary ordering of operations, and unitality, guaranteeing the existence of a neutral element. Together, these two primary properties identify monoids as the fundamental algebraic structure.

This connection between set union and monoids is not novel. For instance, monoids have long been known to have desirable properties for parallel computing [see e.g. @parent2018; @fegaras2017; @lin2013]. More broadly, monoids are also popular functional programming [see e.g. @milewski2018], and form the basis of certain algorithms in generic programming [@stepanov2009; @stepanov2013]. Finally, monoids and other concepts from category theory have also been used in certain areas of relational database theory [see e.g. @fong2019; @gibbons2018]

However, as far as I am aware, I am the first to make the connection between monoids and many popular visualization types. As discussed in Section \@ref(visualization-category-theory), the model described in the present thesis differs significantly from past applications of category theory to data visualization, being on one hand more applied than some applications (it relates to concrete statistics, geometric objects, and interactive features) and more theoretical than others (not a functional programming library). 

Interestingly, the challenges we face we face when visualizing data are in some ways remarkably similar to the parallel computing: often, we want to break the data into parts, compute some summaries, and the combine these summaries back together in a consistent way.

### Model constraints and utility

A potential point of contention is that the constraints which the model provides are too rigid. Specifically, requiring all plots to be groups or monoids excludes a significant fraction of popular visualizations. Further, many non-monoidal plot types, such as boxplots, have been successfully implemented alongside linked selection in the past [see e.g. @theus2002; @urbanek2011]. Thus, a valid question one might have is whether the limitations imposed by the model are justified by its practical utility.

However, I believe this is not an issue, since the model is not meant to be prescriptive. Instead, it simply provides a framework for identifying visualizations which are "naturally" compatible with features like linked selection, and we are free to violate this naturality whenever we see fit. However, we should be aware of the fact that reconciling non-monoidal visualizations with linked selection may require some ad hoc design decisions. For instance, since a boxplot box is not a monoid, there is no such thing as highlighting a "part of a boxplot box". We may solve this issue by e.g. plotting the highlighted cases as a second box next to the original one, however, then we lose the nice interactive properties of part-whole relations provided by monoidal plots (see Section \@ref(stacking-part-whole)). Thus, the model helps us to identify inherent trade-offs in the design and implementation of interactive graphics.    

## Limitations of the software

During this project, I had time to think through and test out different implementations of the various features of the interactive data visualization system. While I succeeded in some areas, there were also areas which could still use further improvement. 

### Scope and features

Currently, `plotscaper` offers a number of features for creating and manipulating interactive figures, many of which have been discussed either in Section \@ref(system) or in Section \@ref(applied-example). While these features can be used to create a fairly substantial range of useful interactive figures, there are of course many limitations and gaps when compared to other, better-established established data visualization packages. These will be discussed in this section. 

First, `plotscaper` currently offers six plot types: scatterplots, barplots, histograms, fluctuation diagrams, histograms, two-dimensional histograms, and parallel coordinate plots. Additionally, normalized representations, such as spineplots and spinograms, are available for all aggregate plots. While these six plot types can already be used to create a wide range of useful interactive figures, there are numerous other plots which could be highly desirable in applied data analysis. Density plots, radar plots, mosaic plots, and maps, for example, may be compatible with the `plotscaper` model and could be potential additions in future versions of the package. Finally, all plots are specified in a nominal way, meaning that, for example, to add a scatterplot, we call the `add_scatterplot` function. While a system for specifying plots declaratively would be appealing, implementing such a system has presented challenges, and the issue will be discussed in more depth in Section \@ref(declarative-schemas).

Second, as described in Section \@ref(system), `plotscaper` currently uses a simple transient-persistent product model for linked selection. Users can transiently select cases or assign them to permanent groups via click or click-and-drag interactions. The combination of transient and persistent selection results in $2 \times 4 = 8$ possible selection states per case.  This model facilitates simple conditional queries, such as "how many cases assigned to group X are also transiently selected?" For example, users can assign a bar in a barplot to a permanent group and transiently select a point cluster in a scatterplot to easily identify cases belonging to both. However, this model is, of course, fairly limited: it does not allow to combine selections with logical operators such as negation or exclusive difference. Implementing more comprehensive selection operators [see e.g. @urbanek2003; @urbanek2011; @theus2022; @wilhelm2008] would enable more sophisticated analytical workflows.       
Third, plot and figure customization in `plotscaper` is currently fairly limited. While users can adjust attributes such as the figure layout and plot scales, surface-level graphical attributes like colors, margins, and axis styling are not yet exposed. Although the support for adding these customization options does exist, their implementation was not prioritized. They may be added in future package versions, when the API stabilizes more.

### Declarative schemas {#declarative-schemas}

As discussed in Section \@ref(system), one limitation of the delivered system is the absence of a simple declarative schema-based approach for specifying plots. While declarative schemas, popularized by the Grammar of Graphics [GoG, @wilkinson2012], have become a highly popular in modern data visualization libraries [see e.g. @wickham2016; @satyanarayan2016; @plotly2023], my system deviates from this paradigm. Although certain core components within `plotscape` are modeled declaratively (as shown in Section \@ref(summaries)), the overall plot definitions remain largely procedural, and the user-facing API (`plotscaper`) relies on nominal plot types^[Currently there are six different types of plots implemented.]. 

However, as I argued in Section \@ref(variables-encodings), this lack of a full declarative plot specification is not unique to my system. Instead, I contend that many currently available declarative data visualization libraries provide partial or incomplete solutions, and frequently conceal or obscure key implementation details [see also @wu2024]. For instance, operations such as grouping, binning, stacking, and normalization are often passed implicitly. Put simply, there are gaps in the GoG-based model. To quote @wu2024:

> Despite these broad-ranging benefits, we observe that this [GoG-based] semantic delineation obscures the process of visual mapping in a way that makes it difficult to reason about visualizations and their relationships to user tasks.
> 
> We observe that the core reason for this dissonance between visualization specifications and functions is that the Visual Mapping step represents two distinct substeps. The first involves additional transformations over the data tables in order to compute, e.g., desired statistics and spatially place marks that are specific to the visualization design. We term these Design-specific Transformations. The second is the visual encoding that maps each row in the transformed table to a mark and data attributes to mark attributes using simple
scaling functions.

During the course of my project, I have independently reached several conclusions which similar to those of @wu2024^[I became aware of Eugene Wu's work only after he contacted me concerning the publication of a paper I co-authored during this project's development: @bartonicek2024]. I did try to come up with a way of specifying declarative schemas for interactive graphics, however, despite some partial successes (see Section \@ref(aggregation-summaries)), ultimately, I was not able to come up with a comprehensive model. I chose to make this implementation gap explicit in `plotscaper`. However, I believe I have also been able to identify three key factors which contribute to the difficulty of developing declarative schemas for interactive data visualization, and these are detailed below. 

The first factor which complicates declarative schemas in interactive graphics is the weak correspondence between data variables and visual encodings. Specifically, as discussed in Section \@ref(variables-encodings), often, what we plot are *not* variables found in the original data, but instead variables which have been computed or derived in some way. This is exists even in static graphics; however, it is further amplified in interactive graphics, where dynamic remapping of variables to aesthetics is often desirable (such as when transforming a barplot to a spineplot). Thus, directly mapping data variables to aesthetics seems to be an inadequate approach. However, to map derived variables to aesthetics, we have to explicitly specify *what* we compute and *how*. For example, to fully specify a histogram, we have to describe the fact that we want to bin cases based on some continuous variable, compute the counts within bins, stack these counts, also within bins, and finally map the bin borders to the x-axis variable and the counts to the y-axis variable. While I have been able to get reasonably far with providing a mechanism for specifying declarative schema for this process (see Section \@ref(variables-encodings)), ultimately, I was not able to integrate it with the rest of the system without some amount of procedural code.

The second factor which presents a challenge to declarative schemas in interactive data visualization is the hierarchical nature of graphics itself (see Section \@ref(hierarchy)). Specifically, while in static graphics, we can often act as if the data underlying our plots is "flat", interactivity necessitates hierarchical data structures. Interactive features like linked selection and parametric manipulation trigger updates to summary statistics, requiring hierarchical organization for efficient updates, especially with features like stacking and normalization (see Sections \@ref(transforming-summaries) and \@ref(aggregation-summaries)). Furthermore, hierarchical data can also be leveraged to provide more efficient scale and axis updates (see Section \@ref(plot-scales)), and display more information during querying (e.g., by displaying summary statistics for both objects and segments). However, specifying declarative schemas with hierarchical data is inherently more complex than with flat data, due to both increased surface area (multiple data sets) and hierarchical dependencies. In `plotscape`, these hierarchical dependencies are represented by aggregated variables with references to parent variables (and factors/reducers), which can be leveraged by specialized operators such as `Reduced.stack` and `Reduced.normalize`, see Section \@ref(aggregation-summaries). An alternative, unexplored approach could be to partially "flatten" the hierarchical structure via SQL-like table joins (i.e. adding parent data as new columns with repeated values). However, with multiple levels of partitioning, this approach may become unwieldy^[I.e., the data would have to contain columns such `value`, `parent_value`, `parent_parent_value`, etc...], and there are other considerations, such as the necessity for variables to "remember" how they have been aggregated (see Sections \@ref(aggregation) and \@ref(aggregation-summaries)), and the reactive propagation of updates, which make completely flat data tables inadequate. 

The third and final factor which makes declarative schemas challenging is reactivity. Interactive visualizations need to, by definition, respond to user input. However, this input can impact various stages of the data visualization pipeline differently. For instance, while shrinking or growing the size of points in a scatterplot is a purely graphical computation, shrinking or growing the width of histogram bins requires recomputation of the underlying data. Consequently, the declarative schema should be able to handle reactive parameters which may have hierarchical dependencies. Signals (see Section \@ref(signals)) may offer a hypothetical general solution; however, as discussed in Section \@ref(reactivity-solution), my experience with developing `plotscape` revealed that reactivity typically tends to enter the visualization process at discrete points, corresponding to the four stages of the data visualization pipeline. While I did not manage to develop a general declarative mechanism for this, the following list outlines the identified hypothetical points of reactivity:

- Pre-partitioning: Data streaming, partitioning parameter updates (e.g. histogram bin width, selection)
- Pre-aggregation: Aggregation parameter updates (e.g. regression line regularization)
- Pre-scaling: Scale parameter updates (e.g. sorting, size/alpha adjustments)
- Pre-rendering: Surface-level changes (e.g. layout modifications, DOM resize events)

To summarize, modeling interactive data visualizations declaratively presents significant challenges. Specifically, the weak correspondence between data variables and aesthetics, the inherent hierarchical nature of graphics, and the necessity for reactive updates make the seemingly appealing model of "assign variable $x$ to aesthetic $y$" inadequate for capturing a wide range of popular interactive graphics. These issues are not unique to `plotscaper`

### Performance

A key aspect to discuss is the performance of the delivered software. As discussed in Section \@ref(background), responsiveness is crucial for interactive data visualization. Slow systems frustrate users, negating any potential benefits of sophisticated features. Given today's expectation of highly responsive GUIs and the growing size of data sets, performance is an important concern.

Despite not being the sole focus, I am happy to report that `plotscaper` achieves solid performance even on moderately sized data sets. Specifically, I was able to achieve highly responsive point-wise linked selection on data sets with tens of thousands of data points (such as the `diamonds` data set; see the [Performance vignette](https://bartonicek.github.io/plotscaper/articles/performance.html) on `plotscaper`'s package website). This performance is, of course, largely attributable to the highly optimized nature of JavaScript engines such as V8 rather than any targeted optimization. Nevertheless, I did try to be generally mindful of performance while developing the package, by adhering to data oriented practices such as relying on the Structure of Arrays (SoA) data layout.  

Profiling of the package revealed that the primary performance bottleneck was rendering. This manifests quite clearly even on the macro level: figures composed entirely of aggregate plots tend remain responsive even with fairly large data sets, whereas figures with multiple bijective plots (such as scatterplots or parallel coordinate plots) can start to become sluggish even with moderately-sized data sets (thousands or tens-of-thousands of data points). Therefore, alternative rendering strategies may offer significant performance gains. Currently, `plotscaper` relies on the default HTML5 `canvas` rendering context, which provides a simple interface for rendering 2D graphics. GPU-based rendering, particularly WebGL [@webgl2025], could lead to significant performance improvements.

Beyond rendering, there are other parts of the system that may provide opportunities for easy performance wins. For instance, currently, as mentioned in Section \@ref(plot-mousemove), querying and selection both rely on a naive collision-detection loop, such that all objects are looped through until matches are found. An approach using spatial data structures, for example quadtrees [@samet1988], could significantly accelerate these searches. Another area which may be open to performance improvements is reactivity. While reactivity is implemented using the - fairly performant - observer pattern, and event callbacks throttled on hot paths (such as mousemove events), further optimization of the reactive system may be possible. Finally, though less performance critical, client-server communication could also be made more performant. Currently, it is implemented with WebSockets [@cheng2024; @mdn2024g] and JSON-based payloads, which incurs serialization/deserialization overhead on each message. An alternative protocol like TCP could mitigate this.
